---
title: "MLE - Data 3"
description: "Professor: Andy Philips"
author:
  - name: Stone Neilon
    url: https://stoneneilon.github.io/
    orcid: 0009-0006-6026-4384
    affiliation: PhD student of political science @ The University of Colorado Boulder
    affiliation-url: https://www.colorado.edu/polisci/people/graduate-students/stone-neilon
date: 08-26-2024
categories: [2024, Methods, Fall] # self-defined categories
citation: 
  url: https://stoneneilon.github.io/notes/Comparative_Behavior/
image: jamiexx.webp
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
---

# [Syllabus](https://www.dropbox.com/scl/fi/32ji3ucw54taatfon4ja4/MLE-syllabus.pdf?rlkey=jhqzrrb2ymqxg70s7wuwzeeyr&st=taxpe036&dl=0)

# Topic 1 - Introduction to Probability Models

## Required Reading:

### King Chapters 1 & 2

#### Chapter 1:

-   Introduction of the book. Political science methodology is disjointed and not as completely coherent as it should (and needs) to be. King seeks to organize and centralize the political science methodology.

-   Statistic Model: a formal representation of the *process* by which a social system produces output.

    -   Since no interesting social systems generate outcomes deterministically, statistical models are assumed to have both systematic and stochastic components.

-   **Inference:** the general process by which one uses observed data to learn about the social system and its outputs.

-   **Estimation:** the specific procedure by which one obtains estimates of features (usually parameters) of the statistical model.

-   **The important question for political science research:**

    -   is the underlying process that gives rise to the observed data

        -   What are the characteristics of the social system that produced these data?

        -   What changes in known features of the social system might have produced data with different characteristics?

        -   What is the specific stochastic process driving one's results?

    -   By posing these questions, statistical modelling will be more theoretically relevant and empirically fruitful.

#### Chapter 2:

-   Conditional probabilities: describing the uncertainty of an observed or hypothetical event given a set of assumptions about the world.

-   Likelihood: a measure of relative uncertainty

### Ward and Ahlquist Chapter 1:

## Lecture:

[Week 1 Slides](https://www.dropbox.com/scl/fi/n8anv7gle9cit9sp8v329/MLE-week1-Handout.pdf?rlkey=38st1zfju2yrsyh0ybgw51zw5&st=8rnk9udt&dl=0)

-   Summary: Run models that find parameters that are most likely to have generated the observed data.

-   These models are hard to interpret.

-   Goal: Familiarize you with a variety of MLE models used in the social sciences.

-   Probability has to sum to 1.

    -   We want to find the best estimate $\theta$

-   Probabilities are:

    -   Bounded between 1 and 0.

    -   Sum of probabilities equal 1.

    -   Trials -\> $\infty$

    -   Mutually exclusive outcomes (Independent).

-   Theta is the only parameter we need to estimate.

-   We are still specifying the distribution of the outcome variable. Is it a poisson, bernoulli, normal, etc?

    -   this will help us specify which model to use.

-   L stands for "likelihood function"

-   Our goal is to select some $\theta$\* -\> $\hat{\theta}$ as to maximize the likelihood of these data being generated. Ways to do this:

    -   plug in candidate $\theta$\* values

    -   look at the graph

    -   optimize function (solve for $\theta$\*)

-   No priors! (that would be Bayesian)

    -   for our coin flip example, we know .5 is the probability but we only have a set of {H,H,T}

    -   Without anymore knowledge, the best estimate of $\theta$ is 2/3 or .66.

-   We use ML anytime our dependent variable has a distribution that was not generated by a Gaussian (normal) process.

    -   see slide 23 for examples.

    -   We can estimate all of these using OLS but we may hit a few snags and violation assumptions.

-   **Working through an ML problem is as follows:**

    -   Build a parametric statistical model

    -   Define the probability density for $Y_i$ (uncertainty component)

    -   Define the systematic component ($\theta$)

    -   Maximize the likelihood function, given the data.

    -   Interpret

-   We will pretty much always use log-likelihood.

    -   why?

    -   logarithms turn multiplication problems into addition problems.

    -   likelihood starts to breakdown around N=1000. Log-likelihood does not. Why?

        -   Our optimization function is multiplying probabilities

            -   what happens when we multiply a bunch of probabilities?

                -   Multiplying thousands of probabilities together is simply not a viable approach without infinite precision.

### Example:

```{r}
# compare what happens when we increase n. 
p <- seq(0,1,by=0.01)
l <- (p^2)*(1-p)^3
plot(l ~ p, type="l")
p[which(l == max(l))]

p <- seq(0,1,by=0.01)
l <- (p^40)*(1-p)^20
plot(l ~ p, type="l")
p[which(l == max(l))]

```

## Homework 1:

-   "Lab 1.R"

# Topic 2: Estimation: Looking Under the Hood

-   $(\theta|y_i)=Pr(y|\theta)$

-   you have to pick which distribution generated y.

    -   assume a probability model.

-   Remember:

    -   Traditional probability is a measure of absolute uncertainty. It comes from three axioms:

        -   See slide 5.

    -   However, the likelihood is only a relative measure of uncertainty.

    -   Likelihood model is never absolutely true. It is assumed. We always have to assume a probability model.

    -   Therefore, we assume that information about $\theta$ comes from

        -   the data

        -   assumption about the DGP (assumed probability distribution)

-   Important to assume outcomes are independent.

-   Pick a theta and figure out the probability/distribution of outcomes.

    -   higher theta better.

-   Whath happens when we multiply a bunch of probabilities together?

    -   they get really small

        -   so we use logs.

            -   what happens when we take natural logs of probabilities?

                -   we get negative numbers - and they will become more negative with more observations.

                    -   take value closes to zero

                        -   maximizing a negative number. (making it less negative)

## Under the Hood of ML:

-   We are calculating the derivative of the highest point of the joint distribution.

-   Types of optimization methods:

    -   Numerical

        -   grid search: Give me a bunch of plausible values of theta and evaluate.

            -   we will find a global maximum.

            -   very slow

            -   computationally becomes crazy very quickly.

    -   Iterative

        -   this is the "default" one - everyone does this.

            -   these are optimization methods of "steepest ascent" or "gradient Ascent" since the algorithm 'crawls' up the surface of the likelihood.

    -   Others...

## Measures of Uncertainty:

-   We have discussed how to obtain the MLE, $\hat{\theta}$. Yet it is an estimate.

-   uncertainty is kinda measured by the curvature.

-   standard errors are derived from the negative of the inverse of the second derivative.

    -   standard errors can't be negative

    -   bigger values imply smaller variance.

    -   bigger negative = more curvature. see equation 8/9 on slides.

    -   we take take the inverse since larger (more negative) values indicate a sharper curvature, which indicates more certainty in our estimate.

-   We use the Hessian for standard errors in MLE.

    -   Variance = $-[\textbf{H}^-1]$

    -   SE: $\sqrt{-[\textbf{H}^-1]}$

## Properties of ML:

-   Sufficiency: there exists a single $\theta$

-   Consistency

    -   $\hat{\theta} ->\theta as \textit{n} -> \infty$

-   Asymptotic normality

-   Invariance: ML estimate is invariant to functional transformations.

-   Efficiency: MLE has the smallest variance (asymptotically), as given by the Cramer-Rao Lower Bound

## Disadvantages of ML:

-   small sample issues. Since ML is asymptotically normal, use Z- rather than t-statistics.

-   We know the VCV is biased in small samples

-   (not a disadvantage) but most MLE models use z rather than t-stat.

-   Have to make distributional assumptions. We must characterize the nature of the statistical experiment.

-   some regularity conditions must be met.

## Information Criteria:

-   Provides goodness-of-fit with penalization for model complexity

    -   basically R\^2

-   Used for feature (i.e., covariate) selection

-   Relative, not absolute.

-   Data-dependent (sample-dependent, just like log likelihood): numerical values of Y must be identical.

-   No hypothesis test

    -   no p-value.

-   Akaike Information criterion (AIC)

    -   $AIC=2k-2ln(L)$

    -   Lower AIC is preferred model.

-   Schwartz Bayesian information criterion (SBIC)

    -   $SBIC=ln(n)k-2ln(L)$

    -   Lower SBIC is preferred model

    -   stronger penalty for over fitting than AIC. Penalty derived from "prior" information.

-   Restricted Mode: Less parameters

-   unrestricted model: all parameters.
