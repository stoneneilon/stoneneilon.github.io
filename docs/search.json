[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Listing Example",
    "section": "",
    "text": "About\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Institutions\n\n\n\n\n\nNotes from American Institutions class with Chinnu\n\n\n\n\n\nDec 15, 2024\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nClass Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparative Behavior\n\n\n\n\n\nComparative Behavior with Jennifer Fitzgerald\n\n\n\n\n\nMay 15, 2024\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nData 1 & 2\n\n\n\n\n\nCombined notes from Data 1 & 2 with Anand Sokhey and Andy Phillips\n\n\n\n\n\nMay 15, 2024\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nICPSR - Introduction to Python\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nICPSR - Math For Social Science (workshop)\n\n\n\n\n\n\n\n\n\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning: Applications in Social Science Research (ICPSR)\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nMy Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy first blog!\n\n\n\n\n\nTest blog to make sure everything is working.\n\n\n\n\n\nOct 24, 2022\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule of Law\n\n\n\n\n\nRule of Law with Josh Strayhorn\n\n\n\n\n\nMay 15, 2024\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Data/index.html",
    "href": "notes/Data/index.html",
    "title": "Data 1 & 2",
    "section": "",
    "text": "These notes were compiled in my first year of graduate school. These two classes cover simple to complex linear regression. Additionally, some other methods are discussed such as Logit/Probit, Causal Inference, and time-series. These other methods were only discussed in brief and require their own separate set of notes.\nReminder: These are notes and do not encompass every idea or detail associated with the concepts. They do not (and cannot) replace classes or reading the material."
  },
  {
    "objectID": "notes/Data/index.html#mean-average",
    "href": "notes/Data/index.html#mean-average",
    "title": "Data 1 & 2",
    "section": "Mean (Average)",
    "text": "Mean (Average)\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\nAbove is the formula for mean (average)."
  },
  {
    "objectID": "notes/Data/index.html#median",
    "href": "notes/Data/index.html#median",
    "title": "Data 1 & 2",
    "section": "Median",
    "text": "Median\n\\[\n\\text{Median} = \\begin{cases}       x_{\\frac{n+1}{2}} & \\text{if } n \\text{ is odd} \\\\      \\frac{1}{2}(x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1}) & \\text{if } n \\text{ is even}    \\end{cases}\n\\]\nDon’t worry about knowing this formula. Arrange everything in order. Select the middle number."
  },
  {
    "objectID": "notes/Data/index.html#mode",
    "href": "notes/Data/index.html#mode",
    "title": "Data 1 & 2",
    "section": "Mode",
    "text": "Mode\n\nAnother measure of central tendency\n\nMode is simply what number appears the most in our dataset.\n\n{4,7,3,7,8,1,7,8,9,4,7}\n\nOur mode would be 7\n\nIt appears the most.\n\n\n\n\nWe don’t use mode that much as a measure of central tendency but it still provides some information about the distribution.\nDon’t worry about the formula"
  },
  {
    "objectID": "notes/Data/index.html#variance",
    "href": "notes/Data/index.html#variance",
    "title": "Data 1 & 2",
    "section": "Variance",
    "text": "Variance\n\\[\n\\text{Variance} (s^2) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\\]\n\nThe average of the squared differences from the Mean. \nVariance is a measure of SPREAD.\nLet’s walk through the formula step by step.\n\nThe \\(\\Sigma\\) means to sum all the values together.\n\\((x_i - \\bar{x})\\)\n\nin this part we are taking each observation and subtracting it by the mean (average).\nNow lets add the square term. \\((x_i - \\bar{x})^2\\)\n\nWhy do we square this?\n\nImagine a number line from 0 to 100. We have some dataset where the mean is 50. Now let’s say one of our observations is 38. 38-50 = -12. See what happens!? We have a negative number. All observations to the left of our mean are negative while all observations to the right of our mean are positive.\n\nWhen we add these all up without the square term, we get ZERO!\n\nThus we square to accommodate for these canceling out.\n\nThere are other reasons we square but they aren’t relevant here and this is the main reason.\n\n\n\nNow the \\(n-1\\)\n\nN represents the number of observations.\n\nWhy are we subtracting it by 1?\n\nIf we were calculating the population variance, then we wouldn’t subtract by 1. However, we are pretty much never working with the population. We are always using some samples. \nThis part is not super intuitive. BUT, we are using the sample mean, NOT the population mean to calculate the variance.\n\nWe don’t know what the “true” population mean is. We have an estimate of it using our sample. Thus, there is some uncertainty around the sample mean (we don’t know if the sample mean is = to the population mean). To account for this uncertainty we add a -1 to our denominator. \n\nBy subtracting 1 from the denominator this makes the spread a little larger to account for that uncertainty. Think about what happens when we make our denominator smaller compared to if we don’t. Example:\n\n\\(\\frac{16}{4-1}\\) vs. \\(\\frac{16}{4}\\)\n\nthe one with the \\(4-1\\) denominator will have a larger output and thus account for the uncertainty in our measurement."
  },
  {
    "objectID": "notes/Data/index.html#standard-deviation",
    "href": "notes/Data/index.html#standard-deviation",
    "title": "Data 1 & 2",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\\[\n\\text{Sample Standard Deviation} (s) = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\n\\]\n\nStandard deviation is denoted by \\(s\\) or \\(\\sigma\\) (lower case sigma).\nStandard deviation represents how far the numbers are from each other.\n\nLook how similar this equation is compared to the variance equation.\n\nThe standard deviation is the square root of the variance!\n\n\nI won’t explain the whole formula again.\n\nI will explain why we square root the equation\n\nWe take the square root to put the output back into its original units. Our output is in the same units as the mean.\n\n\nHave a good understanding of standard deviation THEN understand standard error."
  },
  {
    "objectID": "notes/Data/index.html#standard-error",
    "href": "notes/Data/index.html#standard-error",
    "title": "Data 1 & 2",
    "section": "Standard Error",
    "text": "Standard Error\n\\[\n\\text{Standard Error} (\\text{SE}) = \\frac{s}{\\sqrt{n}}\n\\]\n\nThe numerator (s) is standard deviation!\nWhat is standard error?\n\nIt is the standard deviation of the means!\n\nOK, so what is the difference between standard deviation and standard error?\n\nStandard deviation quantifies the variation within a set of measurements. (singular)\nStandard error quantifies the variation in the means from multiple sets of measurements. (multiple)\n\nWhat is confusing is that we can get standard error from one single measurement, even though it describes the means from multiple sets. Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error.\n\nJust watch the damn video.\n\nhttps://www.youtube.com/watch?v=A82brFpdr9g\n\n\n\n\nWhy do we take the square root of observations in the denominator? \n\nBy dividing by the square root of the sample size, we’re essentially adjusting for the fact that the standard deviation of the sampling distribution of the mean tends to decrease as the sample size increases. This is due to the Central Limit Theorem, which states that the sampling distribution of the sample mean becomes approximately normal as the sample size increases, with a mean equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size. (chat gpt gave me this and it is a kick ass explanation)\n\nIt is because of this that standard error gets smaller when we have more observations!"
  },
  {
    "objectID": "notes/Data/index.html#skewness",
    "href": "notes/Data/index.html#skewness",
    "title": "Data 1 & 2",
    "section": "Skewness",
    "text": "Skewness\nYou do not need to know the formula. You just need to know what skewness looks like and how to properly identify when your data is skewed.\nWhen our distribution has no skew, the mean, median, and mode are all the same value.\n\n\n\nSkewness Visualized\n\n\nPositive skewness is also called “right skew”. Notice where the mean/median/mode are.\nNegative skewness is also called “left skew”. Notice where the mean/median/mode are."
  },
  {
    "objectID": "notes/Data/index.html#covariance",
    "href": "notes/Data/index.html#covariance",
    "title": "Data 1 & 2",
    "section": "Covariance",
    "text": "Covariance\n\\[\n\\text{Covariance} (\\text{cov}(X, Y)) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\]\n\nA measure of how much two random variables vary together\n\nSimilar to variance BUT covariance deals with two variables.\n\nLet’s walk through the formula.\n\nThe numerator tells us to take the sum of each observation minus its mean for both variables (x and y). Then multiply. \n\nRemember we divide by N-1 because we are using the sample means and not the population means. Thus we have a bit of uncertainty. By subtracting 1, it makes our denominator smaller and subsequently the output larger, representing the greater uncertainty. \n\n\nThe output of covariance is a number that tells us the direction of how these two variables vary together. \n\nIt does not tell us the strength of the relationship between the two variables. \n\nLook how similar this formula is compared to the variance formula! \nCovariance is influenced by the scale of the variables. \n\nMeaning its hard to read/understand by itself."
  },
  {
    "objectID": "notes/Data/index.html#correlation",
    "href": "notes/Data/index.html#correlation",
    "title": "Data 1 & 2",
    "section": "Correlation",
    "text": "Correlation\n\\[\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n\\]\n\nThis formula looks scary! It actually is super simple!\n\nDoes the numerator look familiar? Look back at the covariance formula! The top IS covariance!\n\nThe denominator looks familiar too! It is the standard deviation for x and y.\n\nCorrelation is similar to covariance but it is more useful.\n\nWe interpret correlation from -1 to 1.\n\n-1 represents a perfectly negative correlation\n\nThis will almost never happen\n\n1 represents a perfectly positive correlation\n\nthis will almost never happen.\n\n\nCorrelation tell us the direction and strength of a linear relationship between two variables.\n\n\nCorrelation is covariance divided by the product of the two variables standard deviation. So they measure the same thing but one belongs to [-1,1] and the other (covariance) take any value."
  },
  {
    "objectID": "notes/Data/index.html#t-test",
    "href": "notes/Data/index.html#t-test",
    "title": "Data 1 & 2",
    "section": "t-Test",
    "text": "t-Test\n\nProbability Distribution:\n\n\nPurpose:\n\n\nUses:"
  },
  {
    "objectID": "notes/Data/index.html#chi2-test",
    "href": "notes/Data/index.html#chi2-test",
    "title": "Data 1 & 2",
    "section": "\\(\\chi^2\\) Test",
    "text": "\\(\\chi^2\\) Test\n\nPurpose:\n\n\nUses:"
  },
  {
    "objectID": "notes/Data/index.html#analysis-of-variance-anova-test",
    "href": "notes/Data/index.html#analysis-of-variance-anova-test",
    "title": "Data 1 & 2",
    "section": "Analysis of Variance (ANOVA) Test",
    "text": "Analysis of Variance (ANOVA) Test\n\nPurpose:\n\n\nUses:"
  },
  {
    "objectID": "notes/Data/index.html#f-test",
    "href": "notes/Data/index.html#f-test",
    "title": "Data 1 & 2",
    "section": "F-Test",
    "text": "F-Test\n\nPurpose:\n\n\nUses:"
  },
  {
    "objectID": "notes/Data/index.html#p-value",
    "href": "notes/Data/index.html#p-value",
    "title": "Data 1 & 2",
    "section": "p-value",
    "text": "p-value\nRead very closely! So many people misinterpret this concept!\n\nDefinition:\nA p-value is the probability of observing a test statistic value equal to or more extreme than the value you computed if the null were true.\nAlternatively, the p-value is the probability of making a type I/II error.\n\n\nPurpose:\nIf we assume the null hypothesis is true, then we could draw the sampling distribution centered around zero. By specifing the null hypothesis we can invoke the central limit theorem.\nP-values help us determine how statistically significant our relationship is. Remember that we are using data we have to tell us about data we do not have.\n\n\n\nUses:\nThe p-value is decided by the researcher. Convention typically sets the p-value at .10 and below. However, .10 is still not ideal, the lower the better.\n\n\nImportant Notes:\nthe p-value does not tell us anything substantive. It simply tells us"
  },
  {
    "objectID": "notes/Data/index.html#assumptions-scalar-notation",
    "href": "notes/Data/index.html#assumptions-scalar-notation",
    "title": "Data 1 & 2",
    "section": "Assumptions (Scalar Notation)",
    "text": "Assumptions (Scalar Notation)\n\nAssumption 1: \\(\\epsilon_i\\)is normally distributed \n\nThis assumption is for the purposes of constructing a hypothesis test. \nWe don’t need this to estimate our beta. \nBut by assuming the errors are normally distributed then we can run a hypothesis test (t-test) to see if we accept or reject the null hypothesis of beta = 0. \n\n\n\nAssumption 2: \\(E[\\epsilon_i]=0\\)\n\nThe distance between the observed and fitted line is zero. (the residual is zero) \n\nThis rarely happens BUT we estimate our \\(\\hat{\\beta}\\)’s so that the error is as close to zero as possible. \n\nThe goal is to have a line of best fit that does this for all observations. \n\n\n\n\n\nAssumption 3: \\(Var(\\epsilon_i)=\\sigma^2\\)\n\nThis is homoscedasticity (or no heteroskedasticity). \n\nWe want constant error variance. \nHomoscedasticity visualized:\n\n\n\nHomoscedasticity visualized (From Gujarati & Porter)\n\n\n\n\n\n\nAssumption 4: \\(Cov(\\epsilon_i,\\epsilon_j)=0 \\:\\forall\\:i \\neq j\\)\n\nThis represents no autocorrelation \nThe disturbances of i and j are NOT correlated\n\\(\\forall\\) means “for all”\n\n\n\nAssumption 5: \\(x_i\\) is fixed in repeated sampling\n\nX values are independent fo the error term\n\n\n\nAssumption 6: Sample regression model correctly specified \n\nOur sample regression equation correctly identifies the variables in the theoretical population regression model. \nWe include all relevant confounding variables. \nNo omitted variable bias. \n\n\n\nAssumption 7: \\(Cov(\\epsilon_i,x_i)=0\\)\n\ncovariance between residuals and parameters is equal to zero\n\n\n\nAssumption 8: Parametric Linearity\n\nLinear in the parameters. \nWe do NOT raise the betas to a power. \n\nWe can raise the variables (x’s) to a power and it remains linear. \nNote: Logit models add a “link function”. This line is not linear but it is still a linear relationship.\n\n\n\n\nAssumption 9: \\(x_i\\) must vary\n\nDuh. You can’t do anything if your X variable doesn’t vary.\n\n\n\nAssumption 10: n &gt; k\n\nThis relates to degrees of freedom.\nWe need more operations than we have parameters or else we do not have enough information to test a relationship.\n\n\n\nAssumption 11: No perfect multicollinearity\n\nWe wouldn’t include a column for male and a column for female because that would be perfect multicollinearity. \nMulticollinearity is not a big issue (it is natural there will be some level of collinearity between our variables). \n\nBUT perfect multicollinearity is bad and we do not want it. \n\nMulticollinearity can (in some cases) disappear as we increase the number of observations.\nThis is easy to see in matrix algebra or an excel sheet."
  },
  {
    "objectID": "notes/Data/index.html#assumptions-matrix-notation",
    "href": "notes/Data/index.html#assumptions-matrix-notation",
    "title": "Data 1 & 2",
    "section": "Assumptions (Matrix Notation)",
    "text": "Assumptions (Matrix Notation)\nNote: These assumptions are the EXACT same assumptions listed above. The difference is in notation. Why do we do this? I answer this later, but basically its a different way to write math that is more concise and easier to understand. We use matrix algebra/notation because as our model gets bigger, scalar notation gets more complicated to read/keep track of.\nWhy do we bold letters? Bold letters represent matrices.\n\nAssumption 1: Linearity in the parameters\n\\(y_i=x_{i1}\\beta_1+x_{i2}\\beta_2+...+X_{iK}\\beta_K+\\epsilon_i\\)\n\nThe model specifies a linear relationship between y and X\nDo not raise the \\(\\beta\\) to a power.\n\n\n\nAssumption 2: Full rank\n\nX is an n x K matrix with rank K\n\nThere is no exact linear relationship among variables \nSometimes called the “identification condition”\n\nWhat is “rank”?\n\nIt is the number of linearly independent columns \n\nIf the number of independent columns is equal to the total number of columns then the matrix is full rank.\n\n\nThis assumption relates to the scalar assumption of no perfect multicollinearity.\n\n\n\nAssumption 3: Exogeneity of the independent variables\n\nThe independent variables contain NO predictive information about\nThe expected value of is not a function of the independent variables at any observation (including i):\n\n\\(E[\\epsilon_i|x_{j1},x_{j2},...,x_{jK}=0\\)\n\n\\(E[\\epsilon_i|\\textbf{X}]=0\\)\n\n\nWhat does this mean?\n\nThe independent variables are not influenced by the error term or any other unobserved factors in the model.\nThe X variable does not depend on the Y variable (reverse causality). We can’t have the Y variable influencing our regressors (that would be endogeneity)\n\n\n\n\nAssumption 4: Spherical disturbances\n\nNo autocorrelation\n\n\\(E[\\text{Cov}(i,j|\\mathbf{X})] = 0\\: \\forall \\:i=j\\)\n\nAssumed homoscedasticity\n\n\\(E[\\text{Var}(i|\\mathbf{X})] = 2\\: \\forall\\: i=1,2,\\ldots,n\\)\nWhy assumed?\n\nSome level of heteroscedasticity is not fatal\nWe can fix it. But homoscedasticity is always preferable.\n\n\nThese two assumptions can be written mathematically into one single assumption using matrix algebra:\n\n\nThe off-diagonal (the zeros) represent autocorrelation\n\nif these are not zero (or at least very close) we have autocorrelation\n\nThe main-diagonal (the variance) represents our homoscedasticity assumption\n\nIf these values along the main diagonal are not the same or at least very close, then we have heteroscedasticity.\n\n\n\n\n\nAssumption 5: Data generation\n\nThe data generating process of X and \\(\\epsilon\\) are independent.\nX is generated by a non-stochastic process\nthis assumption allows us to say “conditional on X”\nThis assumption is a bit confusing to me.\n\nFrom my understanding, we want our X values to be fixed. We then take samples to see how our y values vary based on the fixed values of X. \nLet’s say you want to predict annual income based on years of experience. Your manager gave you three lists of employees with their annual income. Each list corresponds to a particular experience level — let’s say 3 years, 6 years, and 10 years of experience respectively. Each list contains data on 50 employees. As you can see, the x-values are fixed(3, 6, 10), but have repeated samples (50 data points per sample). This is what is known as Non-stochastic regressors\n\n\n\n\nAssumption 6: \\(\\epsilon\\) is normally distributed\n\nThis is useful for constructing our hypothesis tests and test statistics \nTechnically, we don’t need this for estimating our beta, just uncertainty surrounding it."
  },
  {
    "objectID": "notes/Data/index.html#formula-for-deriving-beta",
    "href": "notes/Data/index.html#formula-for-deriving-beta",
    "title": "Data 1 & 2",
    "section": "Formula for deriving \\(\\beta\\)",
    "text": "Formula for deriving \\(\\beta\\)\n\nProblem: We have two missing terms, 𝜷 and 𝛆. Knowing one of these will tell us the line. But since we don’t know either of these terms, how do we find it out? \nWe have to solve for beta. Solving for beta in Ordinary Least Squares (OLS) requires us to find a line of best fit that minimizes the unexplained difference (the error). \nTo do this we take the sum of the squared residuals  \n\nIt may help to understand this through the formula of the residual. \n\nFirst the residual is the amount our actual observed value differs from the predicted value (This is in matrix notation). \n\n\\((y-\\textbf{X}\\beta_0)\\)\n\ny is our observed value and the \\(\\textbf{X}\\beta_0\\) is our predicted value (the line of best fit). \n\nWE SQUARE THIS! SO NOW: \n\n\\((y-\\textbf{X}\\beta_0)'(y-\\textbf{X}\\beta_0)\\)\n\nThe (’) means transpose. It is matrix notation that allows us to multiply these two matrices (vectors). \nWhy do we square? \n\nWe square the residuals for a bunch of reasons. Mainly: if we don’t, the residuals (Both positive and negative) cancel out. \n\n\nMultiplying this through, we get:\n\\(y'y-y'\\textbf{X}\\hat{\\beta_0}'\\textbf{X}'y+\\hat{\\beta_0}'\\textbf{X}'\\textbf{X}\\)\nYou collect the terms and simply. \nhttps://www.youtube.com/watch?v=K_EH2abOp00 see for more\n\n\n\n\nSO NOW: we want to find a line \\(\\hat{\\beta_0}\\) such that the derivative (the tangent) is set to 0 aka the minimum, hence LEAST squares. Remember, we do not know the Beta. \n\nWe set to zero to find the critical point (the minimum) \nTaking the partial derivative with respect to beta, you’re essentially finding the point where the error function is not changing with respect to changes in beta. Where the slope of the error function with respect to beta is zero. \n\nFigure b is a visual representation of what this looks like when we set our minimum. We are finding the tangent line of the function that is equal to zero!\n\nThe formula is\n\n\\(\\hat{\\beta}=(\\textbf{X'X})^{-1}\\textbf{X}'y\\)\n\nthis gives us the line of best fit. This is the formula R uses to calculate the beta/line.\n\n\nControlling for other variables:\n\n\nCompare figure 3.3 to figure 3.2. They are the same thing. However in 3.3 we have added an additional dimension because of the additional variable. What we are doing remains the same however we now just have more dimensions and we are still trying to find the minimum of that parabola(?) plane(?)"
  },
  {
    "objectID": "notes/Data/index.html#omega-matrix",
    "href": "notes/Data/index.html#omega-matrix",
    "title": "Data 1 & 2",
    "section": "Omega Matrix",
    "text": "Omega Matrix\n\nWhat the hell is an omega matrix \\(\\Omega\\)?\n\nThe omega matrix is literally 𝛆𝛆’ \n\nThe error times its transpose.\nWe obviously can’t solve this without knowing what the errors are. \n\nThis produces the variance covariance matrix (VCV) AKA covariance matrix of the errors. \n\n\n\nWhy do we care about this matrix? \n\nWe need the residuals to get our standard errors. \nAdditionally, this matrix is used to test our assumptions about the model. Specifically whether our model has autocorrelation and heteroskedasticity. \n\nThis is basically what the omega matrix looks like. This photo however is what we want that omega matrix to look like (ours won’t always look like that). But we want the off diagonals to be zero (or effectively zero) and we want the main diagonal to be constant. \n\nIf off-diagonal values are &gt; 0 \n\nWe have autocorrelation\n\nIf main-diagonal values are not the same at each value\n\nWe have heteroskedasticity. \n\n\nNOTE: our omega matrix will NEVER be perfectly spherical.\n\n\n\nConversation with Andy about Omega Matrix:\nI emailed Andy about this and figured it might be beneficial to include it here.\nStone:\nI am looking back on your “Roll your own standard errors.r”. I see how the residual maker is part of the variance formula.\n# the formula for s^2 is e’e/(n-K)\ns.2 &lt;- (t(e)%*%e)/(length(y) - ncol(X))\n##I ran this code individually and it gave me a scalar. I assume this is the sum of the squared error (SSE)?\nAndy:\nYes, divided by degrees of freedom, so it’s a variance\nStone:\nSo, autocorrelation and heteroscedasticity manifest through the variance. Then: vcv &lt;- s.2*(solve(t(X)%*%X))\n\nThis is our VCV of the X’s and then we take the square root of the diagonal to get our SE.\n\nWe use the omega (and the assumptions of no spherical errors) to derive the equation for the SE (equation above). However, if we have spherical disturbances and use the same equation to derive our standard errors then our standard errors are wrong.\nAndy:\nYes,if there are non-spherical disturbances than our standard VCV above isn’t technically correct anymore b/c the equation doesn’t simplify to that.\nStone:\nThen the omega matrix (and its assumptions) is related to the population error. And thus, when we get a sample with spherical disturbances that does not match our expectations of the population error of no spherical disturbances, we then must fix it. Right?\nAndy:\nRight…we can’t know what the population Omega is, but we can get a good guess based off our sample Omega matrix\nStone:\nSo, if we switch the order, e%*%t(e) gives us the matrix of errors (WHICH IS NOT THE OMEGA MATRIX(?)). We want our matrix of errors to look like the omega matrix. It never will but we use the various tests to figure out the level of spherical errors that are present in this matrix.\nAndy:\nThe matrix of the errors IS the Omega matrix, which is the variance covariance matrix of the errors (note the other VCV for our X’s above). It’ll never be spherical perfectly but our assumptions are about expectations so it just needs to be consistently a problem (e.g., 2 errors can be correlated, but it’s only a problem if on average there’s a correlation between errors)\nStone:\nThen when we detect spherical disturbances, we purge it or do whatever (FGLS, Robust/cluster SE), which then fixes our variance and then fixes our SE? Do I have all this right? This all feels kind of magical.\nAndy:\nIf you’re running FGLS you’re using the info in the residual Omega to adjust both your SE’s and coefficients. If you’re correcting just your SEs you’re basically adjusting the standard SE formula to account for the pattern you want to correct for."
  },
  {
    "objectID": "notes/Data/index.html#standard-error-1",
    "href": "notes/Data/index.html#standard-error-1",
    "title": "Data 1 & 2",
    "section": "Standard Error",
    "text": "Standard Error\n\nStandard Errors are not intuitive to me…but they are important\nStandard error is the standard deviation of the means. \n\nThe standard error quantifies the variation in the means from multiple sets of measurements. \n\nWhat gets often confused is that the standard error can be estimated from a SINGLE set of measurements, even though it describes the means from multiple sets. Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error. \n\nIt is an estimate!\n\n\n\nIt is worth discussing standard deviation and its formula.\n\n\\(\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}}\\)\n\nAbove is the formula for standard deviation. \n\nNote: the similarity of this to variance. \n\n\n\nStandard error formula is:\n\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\nThe s is the standard deviation! So all that in the standard deviation formula above is IN the standard error formula. \n\n\nWhy are standard errors important? \n\nNeed for precision of our coefficients\n\nHow precise of a claim can we make? \n\n\nWe make assumptions about our standard errors.**TK  \n\nThey are normally distributed. \n\nNot a big deal. \n\nAssuming the error term is independent and identically distributed. \n\nEach individual error term follows the same distribution and is uncorrelated with each other.\n\nKnowing an error term does not tell you anything about another error term.\n\n\n\nAutocorrelation/heteroskedasticity do not bias our coefficient. \nPresence of autocorrelation leads to an underestimation of the true standard errors. \n\nIncreases the possibility of making a type 1 error. \n\nStandard errors are useful for creating confidence intervals."
  },
  {
    "objectID": "notes/Data/index.html#heteroscedasticity-spherical-disturbances",
    "href": "notes/Data/index.html#heteroscedasticity-spherical-disturbances",
    "title": "Data 1 & 2",
    "section": "Heteroscedasticity (spherical disturbances)",
    "text": "Heteroscedasticity (spherical disturbances)\n\nWhat is heteroskedasticity? \n\nNon-constant error variance. \n\nSee the picture at the beginning of the document of what homoscedasticity looks like. Heteroscedasticity is the opposite of that. \n\nThink of our errors having a pattern or they “fan out”\nUsing the omega matrix again, it is when each value along the main diagonal is different. \n\n\n\nTHIS AFFECTS OUR STANDARD ERROR! \n\nHow? \n\nWhat does it do to our estimate? \n\nOur coefficient is unchanged.\nHowever the efficiency of our model is influenced."
  },
  {
    "objectID": "notes/Data/index.html#autocorrelation-spherical-disturbances",
    "href": "notes/Data/index.html#autocorrelation-spherical-disturbances",
    "title": "Data 1 & 2",
    "section": "Autocorrelation (spherical disturbances)",
    "text": "Autocorrelation (spherical disturbances)\n\nWhat is autocorrelation? \nTHIS AFFECTS OUR STANDARD ERROR! \n\nHow?"
  },
  {
    "objectID": "notes/Data/index.html#interactions",
    "href": "notes/Data/index.html#interactions",
    "title": "Data 1 & 2",
    "section": "Interactions:",
    "text": "Interactions:\nInteractions are used when we believe the relationship is conditional. For example, X causes Y, only if Z is active. The effect of X on Y depends on the level of Z. In other words, the effect of one independent variable on the dependent variable is conditioned by another variable.\nTo accommodate a relationship such as this one, we multiply the two variables together rather than adding.\n\nInteractions increase multicollinearity\nTHIS IS OKAY.\n\n\nInclude all constitutive terms\nIt is essential that you include the constitutive terms and the interaction in the model.\nWrong: Turnout = Age + Age*Race\nCorrect: Turnout = Age + Race + Age*Race\n\n\nInterpretation\nWhen interactions are dichotomous or categorical, interpretation is relatively easy. When the interaction includes a continuous variable, interpretation from the table becomes difficult."
  },
  {
    "objectID": "notes/Data/index.html#what-are-moments",
    "href": "notes/Data/index.html#what-are-moments",
    "title": "Data 1 & 2",
    "section": "What are moments?",
    "text": "What are moments?\nMoments describe the probability distribution. Think of the shape of the density plot. Technically, two unique distributions could have the same mean or median. However, we need moments to help us better understand the distribution shape.\n\nMean\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\n\n\nMedian\nasdf\n\n\nMode\nfdsaf\n\n\nKurtosis\nsadf"
  },
  {
    "objectID": "notes/Data/index.html#why-do-we-use-matrix-algebra-scalar-notation-seems-fine",
    "href": "notes/Data/index.html#why-do-we-use-matrix-algebra-scalar-notation-seems-fine",
    "title": "Data 1 & 2",
    "section": "Why do we use matrix algebra? Scalar notation seems fine…",
    "text": "Why do we use matrix algebra? Scalar notation seems fine…\nThere are a lot of reasons. In relation to regression, matrix algebra becomes essential because doing this in scalar notation turns into hell. It is simply too hard to do all of that once you get more and more variables. \nSecondly, it is how R and other coding languages calculate the coefficient. Why? Long story short, it is less taxing on your computer to do these calculations. Besides more computer sciencey explanations, your computer is doing matrix algebra all the time. \nFinally, matrix algebra will be used in further methods classes. This is especially important in machine learning. You are working with an array now but in machine learning, those arrays gain more dimensions. Imagine a matrix stacked upon another matrix and another. These are called tensors. Don’t worry, you don’t have to deal with these, ever…unless you want to. TK"
  },
  {
    "objectID": "notes/Data/index.html#what-is-the-difference-between-covariance-and-correlation",
    "href": "notes/Data/index.html#what-is-the-difference-between-covariance-and-correlation",
    "title": "Data 1 & 2",
    "section": "What is the difference between covariance and correlation?",
    "text": "What is the difference between covariance and correlation?\nCorrelation is covariance divided by the product of the two variables standard deviation. So they measure the same thing but correlation gives an output bounded to [-1,1] and the covariance takes on the same value as the constitutive terms.\nCorrelation is a normalization of covariance. Covariance is hard to interpret because the scale depends on the variances of two inputs. If you see a covariance of 11,350 or 2,489, you don’t know what those mean or even which set of variables have a high correlation. Correlation divides variance out and rescales to the interval [-1, 1], so now you can make those comparisons. Correlation is covariance but has greater readability and usefulness."
  },
  {
    "objectID": "notes/Data/index.html#what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept",
    "href": "notes/Data/index.html#what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept",
    "title": "Data 1 & 2",
    "section": "What do dummy variables do to the line? Why don’t they change the slope? How come they only shift the intercept?",
    "text": "What do dummy variables do to the line? Why don’t they change the slope? How come they only shift the intercept?\nA dummy variable is a variable coded in binary (0 or 1). Dummy variables can be a factor (0, 1, 2, 3, etc.)"
  },
  {
    "objectID": "notes/Data/index.html#what-is-the-difference-between-variance-and-standard-deviation",
    "href": "notes/Data/index.html#what-is-the-difference-between-variance-and-standard-deviation",
    "title": "Data 1 & 2",
    "section": "What is the difference between variance and standard deviation?",
    "text": "What is the difference between variance and standard deviation?"
  },
  {
    "objectID": "notes/Data/index.html#why-is-ordinary-least-squares-ols-so-powerful",
    "href": "notes/Data/index.html#why-is-ordinary-least-squares-ols-so-powerful",
    "title": "Data 1 & 2",
    "section": "Why is Ordinary Least Squares (OLS) so powerful?",
    "text": "Why is Ordinary Least Squares (OLS) so powerful?\nThe power of OLS becomes somewhat clearer as you learn about different methods. OLS is powerful because it is extremely easy to interpret. The interpretation of OLS is easy because we are specifying a linear relationship.\nOLS power comes from the popularly known Gauss-Markov assumptions. If these assumptions are met, OLS is BLUE - Best Unbiased Linear Estimator.\nDespite its power, OLS has shortfalls. However, it is still important to know OLS, as many methods serve as extensions of OLS and adapt it to better fit the data."
  },
  {
    "objectID": "notes/Data/index.html#when-is-ols-not-good-why-use-other-ones",
    "href": "notes/Data/index.html#when-is-ols-not-good-why-use-other-ones",
    "title": "Data 1 & 2",
    "section": "When is OLS not good? Why use other ones?",
    "text": "When is OLS not good? Why use other ones?\nOLS has numerous advantages. However, OLS has shortfalls that other methods can fix/correct.\n\nOLS is not good with a categorical dependent variable."
  },
  {
    "objectID": "notes/Data/index.html#how-is-standard-error-difference-from-standard-deviation",
    "href": "notes/Data/index.html#how-is-standard-error-difference-from-standard-deviation",
    "title": "Data 1 & 2",
    "section": "How is standard error difference from standard deviation?",
    "text": "How is standard error difference from standard deviation?\n\nhttps://www.youtube.com/watch?v=A82brFpdr9g\n\nWatch the video.\n\nStandard deviation quantifies the variation within a set of measurements. Standard error quantifies the variation in the MEANS from multiple sets of measurements.\nThis gets confusing because we can estimate standard error off of one measurement. \nWatch the video. Seriously, just watch the damn video."
  },
  {
    "objectID": "notes/Data/index.html#are-the-assumptions-about-regression-related-to-the-sample-or-population",
    "href": "notes/Data/index.html#are-the-assumptions-about-regression-related-to-the-sample-or-population",
    "title": "Data 1 & 2",
    "section": "Are the assumptions about regression related to the sample or population?",
    "text": "Are the assumptions about regression related to the sample or population?\n\nThis was originally a question on Andy’s midterm. I got it wrong. :(\nThe assumptions relate to the population.\n\nWe test these assumptions using our sample.\nWe use samples to tell us what we think the true (population) relationship is.\n\nUsing data we have to tell us about data we do not have."
  },
  {
    "objectID": "notes/Data/index.html#why-is-it-called-ordinary-least-squares-ols",
    "href": "notes/Data/index.html#why-is-it-called-ordinary-least-squares-ols",
    "title": "Data 1 & 2",
    "section": "Why is it called Ordinary Least Squares (OLS)?",
    "text": "Why is it called Ordinary Least Squares (OLS)?"
  },
  {
    "objectID": "notes/Data/index.html#what-is-variance-and-why-is-it-important",
    "href": "notes/Data/index.html#what-is-variance-and-why-is-it-important",
    "title": "Data 1 & 2",
    "section": "What is variance and why is it important?",
    "text": "What is variance and why is it important?\n\\[\n\\text{Variance} (s^2) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\\]\nVariance is a measure of spread. Variance helps us understand the dispersion or variability in a data set. Variance estimates how far a set of numbers are spread out from the mean value. It can be difficult to interpret based on the output alone. This is because these values are squared, so we can’t really tell based on the number alone whether the value is relatively high or low.\nUnderstanding variance is critical in statistics. Variance is integral to the efficiency of our estimators. That is, how accurate our model is."
  },
  {
    "objectID": "notes/Data/index.html#why-do-we-care-so-much-about-standard-errors",
    "href": "notes/Data/index.html#why-do-we-care-so-much-about-standard-errors",
    "title": "Data 1 & 2",
    "section": "Why do we care so much about standard errors?",
    "text": "Why do we care so much about standard errors?"
  },
  {
    "objectID": "notes/Data/index.html#i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do",
    "href": "notes/Data/index.html#i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do",
    "title": "Data 1 & 2",
    "section": "I am having trouble visualizing OLS with many variables. What do I do?",
    "text": "I am having trouble visualizing OLS with many variables. What do I do?\nNot much. We are pretty limited to understanding things in three dimensions. Imagine you have 8 variables in your OLS model. Try to draw an 8 dimensional model that shows the relationship. It is impossible."
  },
  {
    "objectID": "notes/Data/index.html#instrumental-variables-what-are-they-will-i-use-them-should-i-use-them",
    "href": "notes/Data/index.html#instrumental-variables-what-are-they-will-i-use-them-should-i-use-them",
    "title": "Data 1 & 2",
    "section": "Instrumental variables, what are they? Will I use them? Should I use them?",
    "text": "Instrumental variables, what are they? Will I use them? Should I use them?\nInstrumental variables are somewhat rare and frowned upon (?) in political science. To be a good instrumental variable, instrumental variables must satisfy two conditions:\n\nThe instrumental variable is theoretically relevant to x.\nThe instrumental variable must satisfy the exclusion restriction.\n\nThe first point requires that our instrument (z) must be endogenous to our independent variable (x)\nThe exclusion restriction is typically where instrumental variables get attacked. The instrumental variable (z in this case) must only affect X. Z-&gt; X -&gt; Y. The difficulty to this condition is that there is no statistical test. The exclusion restriction must be defended by theory.\nIt is very difficult to find an instrument that is both related to X and does not affect Y. An example of a good instrument is provided below (thank you ChrisP from StackExchange):\n“For example, suppose we want to estimate the effect of police (𝑥) on crime (𝑦) in a cross-section of cities. One issue is that places with lots of crime will hire more police. We therefore seek an instrument 𝑧𝑧 that is correlated with the size of the police force, but unrelated to crime.\nOne possible 𝑧 is number of firefighters. The assumptions are that cities with lots of firefighters also have large police forces (relevance) and that firefighters do not affect crime (exclusion). Relevance can be checked with the reduced form regressions, but whether firefighters also affect crime is something to be argued for. Theoretically, they do not and are therefore a valid instrument.”\n\nWhy should we use an instrumental variable?\nThe need for an instrumental variable arises when we are concerned for confounding variables or measurement error."
  },
  {
    "objectID": "notes/Data/index.html#should-we-care-about-r2",
    "href": "notes/Data/index.html#should-we-care-about-r2",
    "title": "Data 1 & 2",
    "section": "Should we care about \\(R^2\\)?",
    "text": "Should we care about \\(R^2\\)?\nIt depends on your question. Chances are you want to find some variable (X) that causes another variable (Y). In this instance, your \\(R^2\\) is mostly irrelevant. You want to see whether that X variable is statistically having an effect on your Y variable. For example, my data 1 project was on the relationship between walkability and voter turnout. I wanted to see if the walkability of an area had an impact on voter turnout in 2016, 2018, and 2020 general elections. Once I accounted for confounding variables, all I cared about was the significance of my variable of interest (walkability). \\(R^2\\) told me nothing that helped me answer this question. \nHowever, R^2 is very important for questions surrounding prediction. TK\nAlso we should focus on adjusted R^2"
  },
  {
    "objectID": "notes/Data/index.html#everyone-talks-about-endogeneity.-what-is-it",
    "href": "notes/Data/index.html#everyone-talks-about-endogeneity.-what-is-it",
    "title": "Data 1 & 2",
    "section": "Everyone talks about endogeneity. What is it?!",
    "text": "Everyone talks about endogeneity. What is it?!\nEndogeneity is when the error term is correlated with the X. Remember that the error term contains everything not in our model (everything that determines Y but is NOT X, will be in our error term). If any of those things not in our model (the error) are related to our X and affect Y, then we have endogeneity. Endogeneity relates to confounders.\nEndogeneity leads to bias in our coefficient."
  },
  {
    "objectID": "notes/Data/index.html#what-is-orthogonal",
    "href": "notes/Data/index.html#what-is-orthogonal",
    "title": "Data 1 & 2",
    "section": "What is orthogonal?",
    "text": "What is orthogonal?\nThis concept was always a bit confusing as it can have different meaning in different contexts.\nORTHOGONAL MEANS INDEPENDENT\nSimply put, orthogonality means “uncorrelated”. An orthogonal model means that all independent variables in that model are uncorrelated. If one or more independent variables are correlated, then that model is non-orthogonal (statisticshowto.com)"
  },
  {
    "objectID": "notes/Data/index.html#is-ols-a-causal-inference-model",
    "href": "notes/Data/index.html#is-ols-a-causal-inference-model",
    "title": "Data 1 & 2",
    "section": "Is OLS a causal Inference model?",
    "text": "Is OLS a causal Inference model?"
  },
  {
    "objectID": "notes/Data/index.html#why-do-we-use-the-normal-distribution",
    "href": "notes/Data/index.html#why-do-we-use-the-normal-distribution",
    "title": "Data 1 & 2",
    "section": "Why do we use the normal distribution?",
    "text": "Why do we use the normal distribution?"
  },
  {
    "objectID": "notes/American_Institutions/index.html",
    "href": "notes/American_Institutions/index.html",
    "title": "American Institutions",
    "section": "",
    "text": "In progress.\n\n\n\nCitationBibTeX citation:@online{neilon2024,\n  author = {Neilon, Stone},\n  title = {American {Institutions}},\n  date = {2024-12-15},\n  url = {https://stoneneilon.github.io/notes/American_Institutions/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNeilon, Stone. 2024. “American Institutions.” December 15,\n2024. https://stoneneilon.github.io/notes/American_Institutions/."
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#carothers-thomas.-1998.-the-rule-of-law-revival-foreign-affairs",
    "href": "notes/Rule_of_Law/index.html#carothers-thomas.-1998.-the-rule-of-law-revival-foreign-affairs",
    "title": "Rule of Law",
    "section": "Carothers, Thomas. 1998. “The Rule of Law Revival”, Foreign Affairs",
    "text": "Carothers, Thomas. 1998. “The Rule of Law Revival”, Foreign Affairs"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#waldron-jeremy.-2011.-the-rule-of-law-and-the-importance-of-procedure-nomos",
    "href": "notes/Rule_of_Law/index.html#waldron-jeremy.-2011.-the-rule-of-law-and-the-importance-of-procedure-nomos",
    "title": "Rule of Law",
    "section": "Waldron, Jeremy. 2011. “The Rule of Law and the Importance of Procedure,” Nomos",
    "text": "Waldron, Jeremy. 2011. “The Rule of Law and the Importance of Procedure,” Nomos"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#weingast-barry.-1997.-the-political-foundations-of-democracy-and-the-rule-of-law-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#weingast-barry.-1997.-the-political-foundations-of-democracy-and-the-rule-of-law-american-political-science-review",
    "title": "Rule of Law",
    "section": "Weingast, Barry. 1997. “The Political Foundations of Democracy and the Rule of Law”, American Political Science Review",
    "text": "Weingast, Barry. 1997. “The Political Foundations of Democracy and the Rule of Law”, American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#shaprio-martin.-1981.-courts-a-comparative-and-political-analysis-ch.-1",
    "href": "notes/Rule_of_Law/index.html#shaprio-martin.-1981.-courts-a-comparative-and-political-analysis-ch.-1",
    "title": "Rule of Law",
    "section": "Shaprio, Martin. 1981. Courts: A Comparative and Political Analysis, Ch. 1",
    "text": "Shaprio, Martin. 1981. Courts: A Comparative and Political Analysis, Ch. 1"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#north-douglass-and-barry-weingast.-1989.-constitutions-and-commitment-the-evolution-of-institutions-governing-public-choice-in-seventeenth-century-england-journal-of-economic-history",
    "href": "notes/Rule_of_Law/index.html#north-douglass-and-barry-weingast.-1989.-constitutions-and-commitment-the-evolution-of-institutions-governing-public-choice-in-seventeenth-century-england-journal-of-economic-history",
    "title": "Rule of Law",
    "section": "North, Douglass, and Barry Weingast. 1989. “Constitutions and Commitment: The Evolution of Institutions Governing Public Choice in Seventeenth-Century England”, Journal of Economic History",
    "text": "North, Douglass, and Barry Weingast. 1989. “Constitutions and Commitment: The Evolution of Institutions Governing Public Choice in Seventeenth-Century England”, Journal of Economic History"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#ferejohn-john.-1999.-independent-judges-dependent-judiciary-explaining-judicial-independence-southern-california-law-review",
    "href": "notes/Rule_of_Law/index.html#ferejohn-john.-1999.-independent-judges-dependent-judiciary-explaining-judicial-independence-southern-california-law-review",
    "title": "Rule of Law",
    "section": "Ferejohn, John. 1999. “Independent Judges, Dependent Judiciary: Explaining Judicial Independence”, Southern California Law Review",
    "text": "Ferejohn, John. 1999. “Independent Judges, Dependent Judiciary: Explaining Judicial Independence”, Southern California Law Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#feld-lars-and-stefan-voigt.-2003.-economic-growth-and-judicial-independence-cross-country-evidence-using-a-new-set-of-indicators-cesifo-working-paper",
    "href": "notes/Rule_of_Law/index.html#feld-lars-and-stefan-voigt.-2003.-economic-growth-and-judicial-independence-cross-country-evidence-using-a-new-set-of-indicators-cesifo-working-paper",
    "title": "Rule of Law",
    "section": "Feld, Lars and Stefan Voigt. 2003. “Economic Growth and Judicial Independence: Cross Country Evidence Using a New Set of Indicators”, CESifo Working Paper",
    "text": "Feld, Lars and Stefan Voigt. 2003. “Economic Growth and Judicial Independence: Cross Country Evidence Using a New Set of Indicators”, CESifo Working Paper"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#melton-james-and-tom-ginsburg.-2014.-does-de-jure-judicial-independence-really-matter-journal-of-law-and-courts",
    "href": "notes/Rule_of_Law/index.html#melton-james-and-tom-ginsburg.-2014.-does-de-jure-judicial-independence-really-matter-journal-of-law-and-courts",
    "title": "Rule of Law",
    "section": "Melton, James and Tom Ginsburg. 2014. “Does De Jure Judicial Independence Really Matter?” Journal of Law and Courts",
    "text": "Melton, James and Tom Ginsburg. 2014. “Does De Jure Judicial Independence Really Matter?” Journal of Law and Courts"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#gibler-doug-and-kirk-randazzo.-2011.-testing-the-effects-of-independent-judiciaries-on-the-likelihood-of-democratic-backsliding-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#gibler-doug-and-kirk-randazzo.-2011.-testing-the-effects-of-independent-judiciaries-on-the-likelihood-of-democratic-backsliding-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Gibler, Doug and Kirk Randazzo. 2011. “Testing the Effects of Independent Judiciaries on the Likelihood of Democratic Backsliding”, American Journal of Political Science",
    "text": "Gibler, Doug and Kirk Randazzo. 2011. “Testing the Effects of Independent Judiciaries on the Likelihood of Democratic Backsliding”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#dahl-robert.-1957.-decision-making-in-a-democracy-the-supreme-court-as-national-policy-maker-journal-of-public-law",
    "href": "notes/Rule_of_Law/index.html#dahl-robert.-1957.-decision-making-in-a-democracy-the-supreme-court-as-national-policy-maker-journal-of-public-law",
    "title": "Rule of Law",
    "section": "Dahl, Robert. 1957. “Decision-Making in a Democracy: The Supreme Court as National Policy Maker”, Journal of Public Law",
    "text": "Dahl, Robert. 1957. “Decision-Making in a Democracy: The Supreme Court as National Policy Maker”, Journal of Public Law"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#vanberg-georg.-2005.-the-politics-of-constituional-review-in-germany-chs.-2-4",
    "href": "notes/Rule_of_Law/index.html#vanberg-georg.-2005.-the-politics-of-constituional-review-in-germany-chs.-2-4",
    "title": "Rule of Law",
    "section": "Vanberg, Georg. 2005. The Politics of Constituional Review in Germany, Chs. 2, 4",
    "text": "Vanberg, Georg. 2005. The Politics of Constituional Review in Germany, Chs. 2, 4"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#clark-tom.-2009.-the-separation-of-powers-court-curbing-and-judicial-legitimacy-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#clark-tom.-2009.-the-separation-of-powers-court-curbing-and-judicial-legitimacy-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Clark, Tom. 2009. “The Separation of Powers, Court Curbing, and Judicial Legitimacy”, American Journal of Political Science",
    "text": "Clark, Tom. 2009. “The Separation of Powers, Court Curbing, and Judicial Legitimacy”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#whittington-keith.-2005.-interpose-your-friendly-hand-political-supports-for-the-exercise-of-judicial-review-by-the-united-states-supreme-court-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#whittington-keith.-2005.-interpose-your-friendly-hand-political-supports-for-the-exercise-of-judicial-review-by-the-united-states-supreme-court-american-political-science-review",
    "title": "Rule of Law",
    "section": "Whittington, Keith. 2005. “Interpose Your Friendly Hand: Political Supports for the Exercise of Judicial Review by the United States Supreme Court”, American Political Science Review",
    "text": "Whittington, Keith. 2005. “Interpose Your Friendly Hand: Political Supports for the Exercise of Judicial Review by the United States Supreme Court”, American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#hall-matthew-and-joseph-ura.-2015.-judicial-majoritarianism-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#hall-matthew-and-joseph-ura.-2015.-judicial-majoritarianism-journal-of-politics",
    "title": "Rule of Law",
    "section": "Hall, Matthew and Joseph Ura. 2015. “Judicial Majoritarianism”, Journal of Politics",
    "text": "Hall, Matthew and Joseph Ura. 2015. “Judicial Majoritarianism”, Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#rohde-david-and-harold-spaeth.-1975.-supreme-court-decision-making-chs.-4-7",
    "href": "notes/Rule_of_Law/index.html#rohde-david-and-harold-spaeth.-1975.-supreme-court-decision-making-chs.-4-7",
    "title": "Rule of Law",
    "section": "Rohde, David and Harold Spaeth. 1975. Supreme Court Decision Making, Chs. 4, 7",
    "text": "Rohde, David and Harold Spaeth. 1975. Supreme Court Decision Making, Chs. 4, 7"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#segal-jeffrey-and-harold-spaeth.-2002.-the-supreme-court-and-the-attitudinal-model-revisited-chs.-3-7-8-selected-portions",
    "href": "notes/Rule_of_Law/index.html#segal-jeffrey-and-harold-spaeth.-2002.-the-supreme-court-and-the-attitudinal-model-revisited-chs.-3-7-8-selected-portions",
    "title": "Rule of Law",
    "section": "Segal, Jeffrey and Harold Spaeth. 2002. The Supreme Court and the Attitudinal Model Revisited, Chs. 3, 7, 8 (Selected portions)",
    "text": "Segal, Jeffrey and Harold Spaeth. 2002. The Supreme Court and the Attitudinal Model Revisited, Chs. 3, 7, 8 (Selected portions)"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#maltzman-forrest-jim-spriggs-and-paul-wahlbeck.-2000.-crafting-law-on-the-supreme-court-the-collegial-game-chs.-4-5",
    "href": "notes/Rule_of_Law/index.html#maltzman-forrest-jim-spriggs-and-paul-wahlbeck.-2000.-crafting-law-on-the-supreme-court-the-collegial-game-chs.-4-5",
    "title": "Rule of Law",
    "section": "Maltzman, Forrest, Jim Spriggs and Paul Wahlbeck. 2000. Crafting Law on the Supreme Court: The Collegial Game, Chs. 4-5",
    "text": "Maltzman, Forrest, Jim Spriggs and Paul Wahlbeck. 2000. Crafting Law on the Supreme Court: The Collegial Game, Chs. 4-5"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bailey-michael.-2013.-is-todays-court-the-most-conservative-in-sixty-years-challenges-and-opportunities-in-measuring-judicial-preferences-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#bailey-michael.-2013.-is-todays-court-the-most-conservative-in-sixty-years-challenges-and-opportunities-in-measuring-judicial-preferences-journal-of-politics",
    "title": "Rule of Law",
    "section": "Bailey, Michael. 2013. “Is Todays Court the Most Conservative in Sixty Years? Challenges and Opportunities in Measuring Judicial Preferences”, Journal of Politics",
    "text": "Bailey, Michael. 2013. “Is Todays Court the Most Conservative in Sixty Years? Challenges and Opportunities in Measuring Judicial Preferences”, Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#caldeira-gregory-and-jack-wright.-1988.-organized-interests-and-agenda-setting-in-the-us-supreme-court-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#caldeira-gregory-and-jack-wright.-1988.-organized-interests-and-agenda-setting-in-the-us-supreme-court-american-political-science-review",
    "title": "Rule of Law",
    "section": "Caldeira, Gregory and Jack Wright. 1988. “Organized Interests and Agenda Setting in the US Supreme Court”, American Political Science Review",
    "text": "Caldeira, Gregory and Jack Wright. 1988. “Organized Interests and Agenda Setting in the US Supreme Court”, American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#perry-h.w.-1991.-deciding-to-decide-ch.-8.",
    "href": "notes/Rule_of_Law/index.html#perry-h.w.-1991.-deciding-to-decide-ch.-8.",
    "title": "Rule of Law",
    "section": "Perry, H.W. 1991. Deciding to Decide, Ch. 8.",
    "text": "Perry, H.W. 1991. Deciding to Decide, Ch. 8."
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#rosenberg-gerald.-1991.-the-hollow-hope-chs.-1-2.-4",
    "href": "notes/Rule_of_Law/index.html#rosenberg-gerald.-1991.-the-hollow-hope-chs.-1-2.-4",
    "title": "Rule of Law",
    "section": "Rosenberg, Gerald. 1991. The Hollow Hope, Chs. 1-2. 4",
    "text": "Rosenberg, Gerald. 1991. The Hollow Hope, Chs. 1-2. 4"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#zemans-frances-kahn.-1983.-legal-mobilization-the-neglected-role-of-the-law-in-the-political-system-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#zemans-frances-kahn.-1983.-legal-mobilization-the-neglected-role-of-the-law-in-the-political-system-american-political-science-review",
    "title": "Rule of Law",
    "section": "Zemans, Frances Kahn. 1983. “Legal Mobilization: The Neglected Role of the Law in the Political System,” American Political Science Review",
    "text": "Zemans, Frances Kahn. 1983. “Legal Mobilization: The Neglected Role of the Law in the Political System,” American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#mccann-michael.-1994.-rights-at-work.-chs.-13",
    "href": "notes/Rule_of_Law/index.html#mccann-michael.-1994.-rights-at-work.-chs.-13",
    "title": "Rule of Law",
    "section": "McCann, Michael. 1994. Rights at Work. Chs. 1,3",
    "text": "McCann, Michael. 1994. Rights at Work. Chs. 1,3"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#epp-charles.-1998.-the-rights-revolution-lawyers-activists-and-supreme-courts-in-comparative-perspective-chs-1-2-6.",
    "href": "notes/Rule_of_Law/index.html#epp-charles.-1998.-the-rights-revolution-lawyers-activists-and-supreme-courts-in-comparative-perspective-chs-1-2-6.",
    "title": "Rule of Law",
    "section": "Epp, Charles. 1998. The Rights Revolution: Lawyers, Activists, and Supreme Courts in Comparative Perspective, Chs 1-2, 6.",
    "text": "Epp, Charles. 1998. The Rights Revolution: Lawyers, Activists, and Supreme Courts in Comparative Perspective, Chs 1-2, 6."
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#hall-matthew.-2010.-the-nature-of-supreme-court-power-chs.-3-5-7-portions",
    "href": "notes/Rule_of_Law/index.html#hall-matthew.-2010.-the-nature-of-supreme-court-power-chs.-3-5-7-portions",
    "title": "Rule of Law",
    "section": "Hall, Matthew. 2010. The Nature of Supreme Court Power, Chs. 3, 5, 7 (portions)",
    "text": "Hall, Matthew. 2010. The Nature of Supreme Court Power, Chs. 3, 5, 7 (portions)"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#posner-richard.-1990.-the-problems-of-jurisprudence-ch.-1",
    "href": "notes/Rule_of_Law/index.html#posner-richard.-1990.-the-problems-of-jurisprudence-ch.-1",
    "title": "Rule of Law",
    "section": "Posner, Richard. 1990. “The Problems of Jurisprudence”, Ch. 1",
    "text": "Posner, Richard. 1990. “The Problems of Jurisprudence”, Ch. 1"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#edwards-harry.-2003.-the-effects-of-collegiality-on-judicial-decisionmaking-university-of-pennsylvania-law-review",
    "href": "notes/Rule_of_Law/index.html#edwards-harry.-2003.-the-effects-of-collegiality-on-judicial-decisionmaking-university-of-pennsylvania-law-review",
    "title": "Rule of Law",
    "section": "Edwards, Harry. 2003. “The Effects of Collegiality on Judicial Decisionmaking”, University of Pennsylvania Law Review",
    "text": "Edwards, Harry. 2003. “The Effects of Collegiality on Judicial Decisionmaking”, University of Pennsylvania Law Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bailey-michael-and-forrest-maltzman.-2008.-does-legal-doctrine-matter-unpacking-law-and-policy-preferences-on-the-u.s.-supreme-court-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#bailey-michael-and-forrest-maltzman.-2008.-does-legal-doctrine-matter-unpacking-law-and-policy-preferences-on-the-u.s.-supreme-court-american-political-science-review",
    "title": "Rule of Law",
    "section": "Bailey, Michael and Forrest Maltzman. 2008. “Does Legal Doctrine Matter? Unpacking Law and Policy Preferences on the U.S. Supreme Court” American Political Science Review",
    "text": "Bailey, Michael and Forrest Maltzman. 2008. “Does Legal Doctrine Matter? Unpacking Law and Policy Preferences on the U.S. Supreme Court” American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bartels-brandon.-2009.-the-constraining-capacity-of-legal-doctrine-on-the-u.s.-supreme-court-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#bartels-brandon.-2009.-the-constraining-capacity-of-legal-doctrine-on-the-u.s.-supreme-court-american-political-science-review",
    "title": "Rule of Law",
    "section": "Bartels, Brandon. 2009. “The Constraining Capacity of Legal Doctrine on the U.S. Supreme Court”, American Political Science Review",
    "text": "Bartels, Brandon. 2009. “The Constraining Capacity of Legal Doctrine on the U.S. Supreme Court”, American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#richards-mark-and-herbert-kritzer.-2002.-jurisprudential-regimes-in-supreme-court-decision-making-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#richards-mark-and-herbert-kritzer.-2002.-jurisprudential-regimes-in-supreme-court-decision-making-american-political-science-review",
    "title": "Rule of Law",
    "section": "Richards, Mark and Herbert Kritzer. 2002. “Jurisprudential Regimes in Supreme Court Decision Making”, American Political Science Review",
    "text": "Richards, Mark and Herbert Kritzer. 2002. “Jurisprudential Regimes in Supreme Court Decision Making”, American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bartels-brandon-and-andrew-ogeen.-2015.-the-nature-of-legal-change-on-the-u.s.-supreme-court-jurisprudential-regimes-theory-and-its-alternatives-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#bartels-brandon-and-andrew-ogeen.-2015.-the-nature-of-legal-change-on-the-u.s.-supreme-court-jurisprudential-regimes-theory-and-its-alternatives-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Bartels, Brandon and Andrew O’Geen. 2015. “The Nature of Legal Change on the U.S. Supreme Court: Jurisprudential Regimes Theory and Its Alternatives”, American Journal of Political Science",
    "text": "Bartels, Brandon and Andrew O’Geen. 2015. “The Nature of Legal Change on the U.S. Supreme Court: Jurisprudential Regimes Theory and Its Alternatives”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#gibson-jim-greg-caldeira-and-lester-kenyatta-spence.-2003.-measuring-attitudes-toward-the-u.s.-supreme-court-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#gibson-jim-greg-caldeira-and-lester-kenyatta-spence.-2003.-measuring-attitudes-toward-the-u.s.-supreme-court-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Gibson, Jim, Greg Caldeira, and Lester Kenyatta Spence. 2003. “Measuring Attitudes Toward the U.S. Supreme Court,” American Journal of Political Science",
    "text": "Gibson, Jim, Greg Caldeira, and Lester Kenyatta Spence. 2003. “Measuring Attitudes Toward the U.S. Supreme Court,” American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#baird-vanessa.-2001-building-institutional-legitimacy-the-role-of-procedural-justice-political-research-quarterly",
    "href": "notes/Rule_of_Law/index.html#baird-vanessa.-2001-building-institutional-legitimacy-the-role-of-procedural-justice-political-research-quarterly",
    "title": "Rule of Law",
    "section": "Baird, Vanessa. 2001 “Building Institutional Legitimacy: The Role of Procedural Justice”, Political Research Quarterly",
    "text": "Baird, Vanessa. 2001 “Building Institutional Legitimacy: The Role of Procedural Justice”, Political Research Quarterly"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#carrubba-cliff.-2009.-a-model-of-the-endogenous-development-of-judicial-institutions-in-federal-and-international-systems-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#carrubba-cliff.-2009.-a-model-of-the-endogenous-development-of-judicial-institutions-in-federal-and-international-systems-journal-of-politics",
    "title": "Rule of Law",
    "section": "Carrubba, Cliff. 2009. “A Model of the Endogenous Development of Judicial Institutions in Federal and International Systems”, Journal of Politics",
    "text": "Carrubba, Cliff. 2009. “A Model of the Endogenous Development of Judicial Institutions in Federal and International Systems”, Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#staton-jeff.-2006.-constitutional-review-and-the-selective-promotion-of-case-results-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#staton-jeff.-2006.-constitutional-review-and-the-selective-promotion-of-case-results-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Staton, Jeff. 2006. “Constitutional Review and the Selective Promotion of Case Results”, American Journal of Political Science",
    "text": "Staton, Jeff. 2006. “Constitutional Review and the Selective Promotion of Case Results”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bartels-brandon-and-eric-kramon.-2020.-does-public-support-for-judicial-power-depend-on-who-is-in-political-power-testing-a-theory-of-partisan-alignment-in-africa-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#bartels-brandon-and-eric-kramon.-2020.-does-public-support-for-judicial-power-depend-on-who-is-in-political-power-testing-a-theory-of-partisan-alignment-in-africa-american-political-science-review",
    "title": "Rule of Law",
    "section": "Bartels, Brandon and Eric Kramon. 2020. “Does Public Support for Judicial Power Depend on Who is in Political Power? Testing a Theory of Partisan Alignment in Africa,” American Political Science Review",
    "text": "Bartels, Brandon and Eric Kramon. 2020. “Does Public Support for Judicial Power Depend on Who is in Political Power? Testing a Theory of Partisan Alignment in Africa,” American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#giles-michael-bethany-blackstone-and-rich-vining.-2008.-the-supreme-court-in-american-democracy-unraveling-the-linkages-between-public-opinion-and-judicial-decision-making-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#giles-michael-bethany-blackstone-and-rich-vining.-2008.-the-supreme-court-in-american-democracy-unraveling-the-linkages-between-public-opinion-and-judicial-decision-making-journal-of-politics",
    "title": "Rule of Law",
    "section": "Giles, Michael, Bethany Blackstone, and Rich Vining. 2008. “The Supreme Court in American Democracy: Unraveling the Linkages between Public Opinion and Judicial Decision Making”, Journal of Politics",
    "text": "Giles, Michael, Bethany Blackstone, and Rich Vining. 2008. “The Supreme Court in American Democracy: Unraveling the Linkages between Public Opinion and Judicial Decision Making”, Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#gibson-jim-greg-caldeira-and-lester-kenyatta-spence.-2003.-the-supreme-court-and-the-us-presidential-election-of-2000-wounds-self-inflicted-or-otherwise-british-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#gibson-jim-greg-caldeira-and-lester-kenyatta-spence.-2003.-the-supreme-court-and-the-us-presidential-election-of-2000-wounds-self-inflicted-or-otherwise-british-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Gibson, Jim, Greg Caldeira, and Lester Kenyatta Spence. 2003. “The Supreme Court and the US Presidential Election of 2000: Wounds, Self-Inflicted or Otherwise?” British Journal of Political Science",
    "text": "Gibson, Jim, Greg Caldeira, and Lester Kenyatta Spence. 2003. “The Supreme Court and the US Presidential Election of 2000: Wounds, Self-Inflicted or Otherwise?” British Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bartels-brandon-and-christopher-johnson.-2013.-on-the-ideological-foundations-of-supreme-court-legitimacy-in-the-american-public-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#bartels-brandon-and-christopher-johnson.-2013.-on-the-ideological-foundations-of-supreme-court-legitimacy-in-the-american-public-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Bartels, Brandon and Christopher Johnson. 2013. “On the Ideological Foundations of Supreme Court Legitimacy in the American Public”, American Journal of Political Science",
    "text": "Bartels, Brandon and Christopher Johnson. 2013. “On the Ideological Foundations of Supreme Court Legitimacy in the American Public”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#gibson-jim-and-michael-nelson.-2015.-is-the-u.s.-supreme-courts-legitimacy-grounded-in-performance-satisfaction-and-ideology-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#gibson-jim-and-michael-nelson.-2015.-is-the-u.s.-supreme-courts-legitimacy-grounded-in-performance-satisfaction-and-ideology-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Gibson, Jim and Michael Nelson. 2015. “Is the U.S. Supreme Court’s Legitimacy Grounded in Performance Satisfaction and Ideology?” American Journal of Political Science",
    "text": "Gibson, Jim and Michael Nelson. 2015. “Is the U.S. Supreme Court’s Legitimacy Grounded in Performance Satisfaction and Ideology?” American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bartels-brandon.-2020.-curbing-the-court-why-the-public-constrains-judicial-independence-ch.-3",
    "href": "notes/Rule_of_Law/index.html#bartels-brandon.-2020.-curbing-the-court-why-the-public-constrains-judicial-independence-ch.-3",
    "title": "Rule of Law",
    "section": "Bartels, Brandon. 2020. Curbing the Court: Why the Public Constrains Judicial Independence, Ch. 3",
    "text": "Bartels, Brandon. 2020. Curbing the Court: Why the Public Constrains Judicial Independence, Ch. 3"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#christenson-dino-and-david-glick.-2019.-reassessing-the-supreme-court-how-decisions-and-negativity-bias-affecting-legitimacy-political-research-quarterly",
    "href": "notes/Rule_of_Law/index.html#christenson-dino-and-david-glick.-2019.-reassessing-the-supreme-court-how-decisions-and-negativity-bias-affecting-legitimacy-political-research-quarterly",
    "title": "Rule of Law",
    "section": "Christenson, Dino and David Glick. 2019. “Reassessing the Supreme Court: How Decisions and Negativity Bias Affecting Legitimacy”, Political Research Quarterly",
    "text": "Christenson, Dino and David Glick. 2019. “Reassessing the Supreme Court: How Decisions and Negativity Bias Affecting Legitimacy”, Political Research Quarterly"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#bartels-brandon.-2020.-curbing-the-court-why-the-public-constrains-judicial-independence-chs.-6-7",
    "href": "notes/Rule_of_Law/index.html#bartels-brandon.-2020.-curbing-the-court-why-the-public-constrains-judicial-independence-chs.-6-7",
    "title": "Rule of Law",
    "section": "Bartels, Brandon. 2020. Curbing the Court: Why the Public Constrains Judicial Independence, Chs. 6-7",
    "text": "Bartels, Brandon. 2020. Curbing the Court: Why the Public Constrains Judicial Independence, Chs. 6-7"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#driscoll-amanda-and-michael-nelson.-2023.-the-cost-of-court-curbing-evidence-from-the-united-states-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#driscoll-amanda-and-michael-nelson.-2023.-the-cost-of-court-curbing-evidence-from-the-united-states-journal-of-politics",
    "title": "Rule of Law",
    "section": "Driscoll, Amanda and Michael Nelson. 2023. “The Cost of Court Curbing: Evidence from the United States,” Journal of Politics",
    "text": "Driscoll, Amanda and Michael Nelson. 2023. “The Cost of Court Curbing: Evidence from the United States,” Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#gibson-jim.-n.d.-losing-legitimacy-the-challenges-of-the-dobbs-ruling-to-conventional-legitimacy-theory-working-paper",
    "href": "notes/Rule_of_Law/index.html#gibson-jim.-n.d.-losing-legitimacy-the-challenges-of-the-dobbs-ruling-to-conventional-legitimacy-theory-working-paper",
    "title": "Rule of Law",
    "section": "Gibson, Jim. n.d. “Losing Legitimacy: The Challenges of the Dobbs Ruling to Conventional Legitimacy Theory,” Working Paper",
    "text": "Gibson, Jim. n.d. “Losing Legitimacy: The Challenges of the Dobbs Ruling to Conventional Legitimacy Theory,” Working Paper"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#strayhorn-josh.-n.d.-judicial-legitimacy-and-the-dynamics-of-belief-formation-working-paper.",
    "href": "notes/Rule_of_Law/index.html#strayhorn-josh.-n.d.-judicial-legitimacy-and-the-dynamics-of-belief-formation-working-paper.",
    "title": "Rule of Law",
    "section": "Strayhorn, Josh. n.d. “Judicial Legitimacy and the Dynamics of Belief Formation,” Working Paper.",
    "text": "Strayhorn, Josh. n.d. “Judicial Legitimacy and the Dynamics of Belief Formation,” Working Paper."
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#ramseyer-j.-mark-and-eric-rasmusen.-2001.-why-are-japanese-judges-so-conservative-in-politically-charged-cases-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#ramseyer-j.-mark-and-eric-rasmusen.-2001.-why-are-japanese-judges-so-conservative-in-politically-charged-cases-american-political-science-review",
    "title": "Rule of Law",
    "section": "Ramseyer, J. Mark and Eric Rasmusen. 2001. “Why are Japanese Judges So Conservative in Politically Charged Cases?” American Political Science Review",
    "text": "Ramseyer, J. Mark and Eric Rasmusen. 2001. “Why are Japanese Judges So Conservative in Politically Charged Cases?” American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#harvey-anna-and-barry-friedman.-2006.-pulling-punches-congressional-constraints-on-the-supreme-courts-constitutional-ruling-1987-2000-legislative-studies-quarterly",
    "href": "notes/Rule_of_Law/index.html#harvey-anna-and-barry-friedman.-2006.-pulling-punches-congressional-constraints-on-the-supreme-courts-constitutional-ruling-1987-2000-legislative-studies-quarterly",
    "title": "Rule of Law",
    "section": "Harvey, Anna and Barry Friedman. 2006. “Pulling Punches: Congressional Constraints on the Supreme Court’s Constitutional Ruling, 1987-2000”, Legislative Studies Quarterly",
    "text": "Harvey, Anna and Barry Friedman. 2006. “Pulling Punches: Congressional Constraints on the Supreme Court’s Constitutional Ruling, 1987-2000”, Legislative Studies Quarterly"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#owens-ryan.-2010.-the-separation-of-powers-and-supreme-court-agenda-setting-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#owens-ryan.-2010.-the-separation-of-powers-and-supreme-court-agenda-setting-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Owens, Ryan. 2010. “The Separation of Powers and Supreme Court Agenda Setting”, American Journal of Political Science",
    "text": "Owens, Ryan. 2010. “The Separation of Powers and Supreme Court Agenda Setting”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#segal-jeff-chad-westerland-and-stef-lindquist.-2011.-congress-the-supreme-court-and-judicial-review-testing-a-constitutional-separation-of-powers-model-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#segal-jeff-chad-westerland-and-stef-lindquist.-2011.-congress-the-supreme-court-and-judicial-review-testing-a-constitutional-separation-of-powers-model-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Segal, Jeff, Chad Westerland and Stef Lindquist. 2011. “Congress, the Supreme Court and Judicial Review: Testing a Constitutional Separation of Powers Model”, American Journal of Political Science",
    "text": "Segal, Jeff, Chad Westerland and Stef Lindquist. 2011. “Congress, the Supreme Court and Judicial Review: Testing a Constitutional Separation of Powers Model”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#hall-matthew.-2014.-the-semiconstrained-court-public-opinion-the-separation-of-powers-and-the-u.s.-supreme-courts-fear-of-nonimplementation-american-journal-of-political-science",
    "href": "notes/Rule_of_Law/index.html#hall-matthew.-2014.-the-semiconstrained-court-public-opinion-the-separation-of-powers-and-the-u.s.-supreme-courts-fear-of-nonimplementation-american-journal-of-political-science",
    "title": "Rule of Law",
    "section": "Hall, Matthew. 2014. “The Semiconstrained Court: Public Opinion, the Separation of Powers, and the U.S. Supreme Court’s Fear of Nonimplementation”, American Journal of Political Science",
    "text": "Hall, Matthew. 2014. “The Semiconstrained Court: Public Opinion, the Separation of Powers, and the U.S. Supreme Court’s Fear of Nonimplementation”, American Journal of Political Science"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#cameron-chuck-jeff-segal-and-donald-songer.-2000.-strategic-auditing-in-a-political-hierarchy-an-informational-model-of-the-supreme-courts-certiorari-decisions-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#cameron-chuck-jeff-segal-and-donald-songer.-2000.-strategic-auditing-in-a-political-hierarchy-an-informational-model-of-the-supreme-courts-certiorari-decisions-american-political-science-review",
    "title": "Rule of Law",
    "section": "Cameron, Chuck, Jeff Segal and Donald Songer. 2000. “Strategic Auditing in a Political Hierarchy: An Informational Model of the Supreme Court’s Certiorari Decisions”, American Political Science Review",
    "text": "Cameron, Chuck, Jeff Segal and Donald Songer. 2000. “Strategic Auditing in a Political Hierarchy: An Informational Model of the Supreme Court’s Certiorari Decisions”, American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#stefanie-lindquist-susan-haire-and-donald-songer.-2007.-supreme-court-auditing-of-the-us-courts-of-appeals-an-organizational-perspective-journal-of-public-administrative-research-and-theory",
    "href": "notes/Rule_of_Law/index.html#stefanie-lindquist-susan-haire-and-donald-songer.-2007.-supreme-court-auditing-of-the-us-courts-of-appeals-an-organizational-perspective-journal-of-public-administrative-research-and-theory",
    "title": "Rule of Law",
    "section": "Stefanie Lindquist, Susan Haire, and Donald Songer. 2007. “Supreme Court Auditing of the US Courts of Appeals: An Organizational Perspective”, Journal of Public Administrative Research and Theory",
    "text": "Stefanie Lindquist, Susan Haire, and Donald Songer. 2007. “Supreme Court Auditing of the US Courts of Appeals: An Organizational Perspective”, Journal of Public Administrative Research and Theory"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#kastellec-jon.-2011.-hierarchical-and-collegial-politics-on-the-u.s.-courts-of-appeals-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#kastellec-jon.-2011.-hierarchical-and-collegial-politics-on-the-u.s.-courts-of-appeals-journal-of-politics",
    "title": "Rule of Law",
    "section": "Kastellec, Jon. 2011. “Hierarchical and Collegial Politics on the U.S. Courts of Appeals”, Journal of Politics",
    "text": "Kastellec, Jon. 2011. “Hierarchical and Collegial Politics on the U.S. Courts of Appeals”, Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#beim-deborah.-2017.-learning-in-the-judicial-hierarchy-journal-of-politics",
    "href": "notes/Rule_of_Law/index.html#beim-deborah.-2017.-learning-in-the-judicial-hierarchy-journal-of-politics",
    "title": "Rule of Law",
    "section": "Beim, Deborah. 2017. “Learning in the Judicial Hierarchy”, Journal of Politics",
    "text": "Beim, Deborah. 2017. “Learning in the Judicial Hierarchy”, Journal of Politics"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#strayhorn-josh.-2023.-lower-courts-in-interbranch-conflict-journal-of-law-and-courts",
    "href": "notes/Rule_of_Law/index.html#strayhorn-josh.-2023.-lower-courts-in-interbranch-conflict-journal-of-law-and-courts",
    "title": "Rule of Law",
    "section": "Strayhorn, Josh. 2023. “Lower Courts in Interbranch Conflict”, Journal of Law and Courts",
    "text": "Strayhorn, Josh. 2023. “Lower Courts in Interbranch Conflict”, Journal of Law and Courts"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#cameron-chuck-and-jon-kastellec.-2016.-are-supreme-court-nominations-a-move-the-median-game-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#cameron-chuck-and-jon-kastellec.-2016.-are-supreme-court-nominations-a-move-the-median-game-american-political-science-review",
    "title": "Rule of Law",
    "section": "Cameron, Chuck and Jon Kastellec. 2016. “Are Supreme Court Nominations a Move-the-Median Game?” American Political Science Review",
    "text": "Cameron, Chuck and Jon Kastellec. 2016. “Are Supreme Court Nominations a Move-the-Median Game?” American Political Science Review"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#cameron-chuck-and-jon-kastellec.-n.d.-simulating-the-future-ideological-composition-of-the-supreme-court-working-paper",
    "href": "notes/Rule_of_Law/index.html#cameron-chuck-and-jon-kastellec.-n.d.-simulating-the-future-ideological-composition-of-the-supreme-court-working-paper",
    "title": "Rule of Law",
    "section": "Cameron, Chuck and Jon Kastellec. n.d. “Simulating the Future Ideological Composition of the Supreme Court” Working Paper",
    "text": "Cameron, Chuck and Jon Kastellec. n.d. “Simulating the Future Ideological Composition of the Supreme Court” Working Paper"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#geyh-charles.-2003.-why-judicial-elections-stink-ohio-state-law-journal",
    "href": "notes/Rule_of_Law/index.html#geyh-charles.-2003.-why-judicial-elections-stink-ohio-state-law-journal",
    "title": "Rule of Law",
    "section": "Geyh, Charles. 2003. “Why Judicial Elections Stink”, Ohio State Law Journal",
    "text": "Geyh, Charles. 2003. “Why Judicial Elections Stink”, Ohio State Law Journal"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#canes-wrone-brandice-tom-clark-and-amy-semet.-2018.-judicial-elections-public-opinion-and-decisions-on-lower-salience-issues-journal-of-empirical-legal-studies",
    "href": "notes/Rule_of_Law/index.html#canes-wrone-brandice-tom-clark-and-amy-semet.-2018.-judicial-elections-public-opinion-and-decisions-on-lower-salience-issues-journal-of-empirical-legal-studies",
    "title": "Rule of Law",
    "section": "Canes-Wrone, Brandice, Tom Clark, and Amy Semet. 2018. “Judicial Elections, Public Opinion, and Decisions on Lower-Salience Issues”, Journal of Empirical Legal Studies",
    "text": "Canes-Wrone, Brandice, Tom Clark, and Amy Semet. 2018. “Judicial Elections, Public Opinion, and Decisions on Lower-Salience Issues”, Journal of Empirical Legal Studies"
  },
  {
    "objectID": "notes/Rule_of_Law/index.html#arrington-nancy-et-al.-2021.-constitutional-reform-and-the-gender-diversification-of-peak-courts-american-political-science-review",
    "href": "notes/Rule_of_Law/index.html#arrington-nancy-et-al.-2021.-constitutional-reform-and-the-gender-diversification-of-peak-courts-american-political-science-review",
    "title": "Rule of Law",
    "section": "Arrington, Nancy, et al. 2021. “Constitutional Reform and the Gender Diversification of Peak Courts,” American Political Science Review",
    "text": "Arrington, Nancy, et al. 2021. “Constitutional Reform and the Gender Diversification of Peak Courts,” American Political Science Review"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html",
    "href": "notes/ICPSR_Math_Review/index.html",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "",
    "text": "These are notes from the ICPSR workshop, “Math for Social Science” with Prof. Sara Tomek. The purpose of this workshop was to review mathematical concepts already taught. Much of the information in here is taken from the lectures and handouts provided."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#matrix-addition-and-subtraction",
    "href": "notes/ICPSR_Math_Review/index.html#matrix-addition-and-subtraction",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Matrix Addition and Subtraction",
    "text": "Matrix Addition and Subtraction\nMatrices must have the same dimensions to be added or subtracted.\n\nThis means they must have the same rows x columns."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#matrix-multiplication",
    "href": "notes/ICPSR_Math_Review/index.html#matrix-multiplication",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nTK (Just add a cheat sheet for how to do these)"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#determinant",
    "href": "notes/ICPSR_Math_Review/index.html#determinant",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Determinant",
    "text": "Determinant\nThe determinant is a scalar number that comprises information on the matrix. The information it comprises tells us whether we can take the inverse of the matrix. If the determinant of a matrix is zero, then we cannot take the inverse of that matrix. In a sense, the determinant “summaries” the information in the matrix. A specific type of information from a matrix. It is easier to talk about a single number than the whole data.\n\nDeterminants measure the factor of how much the area of a given region increases or decreases in space.\n\nhow much does the transformation stretch or squish “things”.\n\nthe determinant tells us the factor of how much a given area stretches or squishes.\n\nin 3 dimensions this tells us the factor of change in volume\n\n\n\nIf the determinant is zero, it is squishes all of space onto a line or single point. Since then, the area of any region would be zero.\nDeterminants are non-zero if the matrix has full rank\nonly a square matrix can have a determinant.\n\n\n\n\n\n\n\nIf an inverse is badly scaled or ill-conditioned, then the inverse is unstable. This means a tiny change in just one of the elements of the original matrix can result in HUGE changes in many (or all) of the elements of its inverse.\nKnowing how it is calculated isn’t super important. You can just google this if needed. Our computers can do this all as it gets much more tedious when we increase the # of dimensions.\nYou can compute a negative determinant (pre absolute value). This would tell you orientation and that the space has been inverted. If you still take the absolute value of the determinant, that still gives you the factor in which the space changed."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#identity-matrix",
    "href": "notes/ICPSR_Math_Review/index.html#identity-matrix",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Identity Matrix",
    "text": "Identity Matrix\nA square matrix, I, with ones on the main diagonal and zeros everywhere else.\n\nIf the size of I is not specified, then it is assumed to be conformable and as big as necessary.\nThe identity matrix is analogous to the number 1.\nIf you multiple any matrix with a conformable identity matrix, the result will be the same matrix (or vector)."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#inverse-matrices",
    "href": "notes/ICPSR_Math_Review/index.html#inverse-matrices",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Inverse Matrices",
    "text": "Inverse Matrices\n\nThere is no way to divide a matrix.\nwe instead take the inverse of a matrix and multiply it by itself.\n\\(\\textbf{A}^{-1}\\) raising to the power of negative 1 indicates we are taking the inverse of a matrix.\nWe need to make sure the matrix can be inverted.\n\nit is invertible if it is non-singluar.\nThe matrix times its inverse will return an identity matrix.\n\nDeterminate has to be non-zero.\n\nNeed to have full rank for this."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#vectors",
    "href": "notes/ICPSR_Math_Review/index.html#vectors",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Vectors",
    "text": "Vectors\n\nVectors are matrices with only one row or columns.\n\\(x'x = \\sum x^2_i\\)\n\nThis is the sum of squares.\n\nVectors are one variable\nmatrices are multiple variables.\n\n\nDot Product\n\nFor example, if A = [5, -2, 1] and B = [3, 0, 4] then A • B = (5)(3) + (-2)(0) + (1)(4) = 15 + 0 + 4 = 19 .\nSo the Dot Product of A and B is 19.\nIf the dot product of two vectors is equal to zero then those two vectors are Orthogonal, which implies that the\nangle between them is 90° (i.e., they are perpendicular) and they are independent of each other.\n\northogonal means independent.\n\nThe dot product tells us how closely two vectors align."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#idempotent",
    "href": "notes/ICPSR_Math_Review/index.html#idempotent",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Idempotent",
    "text": "Idempotent\n\nA square matrix, P is idempotent if when multiplied by itself, yields itself. PP=P\nThe trace of an idempotent matrix is equal to the rank.\n1 X 1 = 1 - this is idempotent"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#order-of-operations",
    "href": "notes/ICPSR_Math_Review/index.html#order-of-operations",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Order of Operations",
    "text": "Order of Operations\n\nMatrix multiplication is non-communicative. \\(AB \\neq BA\\)\nMatrix multiplication is associative. As long as the order stays the same. \\((AB)C = A(BC)\\)"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#rank-of-matrix",
    "href": "notes/ICPSR_Math_Review/index.html#rank-of-matrix",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Rank of Matrix",
    "text": "Rank of Matrix\n\nThank rank of a matrix is the maximal number of linearly independent rows or columns.\nrank = number dimensions in the output.\nThe columns in a matrix should be independent of each other.\nHow much information can we actually get out of a matrix that is independent of each other.\nMax rank will be equal to the number of columns in the matrix.\nNot full rank means some variables are linearly dependent on each other.\n\nclassic example is male and female. Perhaps your matrix includes a dummy variable for male (0 = female, 1 = male). If you include another variable for female where (0 = male, 1 = female) - this is a dummy variable trap. The female variable is perfectly dependent on the male variable. They give the exact same information. The female variable is determined by the male variable.\n\nKind of similar to degrees of freedom.\nIf a square matrix is of full rank then it is nonsingular (i.e., it does have an inverse).\nIf a square matrix is not of full rank then it is singular (i.e., it does not have an inverse)."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#trace",
    "href": "notes/ICPSR_Math_Review/index.html#trace",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Trace",
    "text": "Trace\n\nsum of the diagonal elements.\nLet’s say we have a VCV matrix.\n\nthe variance of each element along the main diagonal represents variance for that variable.\n\nIf you were to take the trace of the main diagonal in a VCV, you would get the variance across all variables together (non-weighted).\n\nVCV’s are always square matrices."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#eigenvalues-and-eigenvectors",
    "href": "notes/ICPSR_Math_Review/index.html#eigenvalues-and-eigenvectors",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nLet’s say we have a matrix, A, that is square and nxn.\n\nEigenvectors have a special relationship with this matrix\n\nsuch that when you multiply \\(\\textbf{A}\\overrightarrow{x}\\) you get \\(\\lambda \\overrightarrow{x}\\)\n\nthe \\(\\overrightarrow{x}\\) is the eigenvector.\nA is a matrix. The \\(\\lambda\\) is an EIGENVALUE and is a SCALAR.\n\nWhen you multiply A times the eigenvector x, you get back that same vector, multiplied by a scalar, lambda. These scalars are called eigenvalues.\n\n\nFrom reddit:\n\n“I’m not aware of how it’s used, if at all, in statistics. It comes from linear algebra.\nSay you have a shape. You can apply a transformation to it - rotate it, stretch bits, squash bits, etc. If you paint an arrow on the shape, after the transformation the arrow will most likely be pointing a different direction.\nBut sometimes the transformation you apply conspires to make so that the arrow doesn’t change direction. Maybe it gets stretched or squished, but it still points in the same direction. From the arrow’s perspective, all you did was scale it up or down in size by some amount.\nIf that happens, the arrow is called an eigenvector of the transformation, and the amount it gets scaled by is its associated eigenvalue. They are properties of the transformation itself, not the shape you apply it to.”\n\nA matrix can have multiple eigenvalues BUT no more than its number or rows/columns.\nEach eigenvalue is associate with a specific eigenvector.\nYou can get negative eigenvalues but they are not good for us in statistics.\n\n\nDefinite\n\neigenvalues are closely related to definiteness.\nWhy do we care about definiteness?\n\nIt is useful for establishing if a (multivariate) function has a maximum, minimum or neither at a critical point.\n\nthis is important for regression (OLS). We are trying to find the line that minimizes the squared difference.\n\n\nWe want positive definiteness.\n\nTo have positive definiteness we need our matrix to satisfy the following:\n\nsymmetric\nall eigenvalues are positive\nall the subdeterminants are positive\nyou could also just calculate the quadratic form and check its positiveness\n\nif the quadratic form is &gt; 0 then it is positive definiteness.\nwe want a positive definite matrix because it is good for interpretation."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#variance-inflation-factor-vif",
    "href": "notes/ICPSR_Math_Review/index.html#variance-inflation-factor-vif",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Variance Inflation Factor (VIF)",
    "text": "Variance Inflation Factor (VIF)\n\nmetric that measures how much overlap we have between our independent variables.\nIf we created a correlation matrix we can find the VIF easily.\n\nEND DAY 2!"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#differentiation",
    "href": "notes/ICPSR_Math_Review/index.html#differentiation",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Differentiation",
    "text": "Differentiation\n\nDerivative\nThe instantaneous rate of change. Finding the slope of a single point. The tangent line of the curve.\n\n\nHow do we write a derivative?\nDerivatives are represented by either:\n\nf’(x)\n\\(\\frac{\\textit{d}}{\\textit{dx}}f(x)\\)\n\nWe are saying to take the derivative of x\n\n\nDifferentiation formulas\n\nthe derivative of a constant is 0\n\n\\(\\frac{\\textit{d}}{\\textit{du}}c=0\\)\n\nexample: \\(\\frac{\\textit{d}}{\\textit{du}}7\\)\n\nif we take the derivative of just 7, we get zero.\n\n\n\nThe derivative of a sum is the sum of the derivatives.\n\n\\(\\frac{\\textit{d}}{\\textit{dt}}(t+4) = \\frac{\\textit{d}}{\\textit{dt}}(t)+\\frac{\\textit{d}}{\\textit{dt}}(4)=1+0=1\\)\n\nThe derivative of u to a constant power (use this one a lot)\n\n\\(\\frac{\\textit{d}}{\\textit{du}}u^n=n*u^{n-1}du\\)\n\\(\\frac{\\textit{d}}{\\textit{dx}}3x^3=3*3x^{2}=9x^2\\)\n\nThe derivative of log:\n\n\\(\\frac{\\textit{d}}{\\textit{du}}log(u)=\\frac{1}{u}du\\)\n\n\\(\\frac{\\textit{d}}{\\textit{dy}}3log(x)=3*\\frac{1}{x}*\\frac{\\textit{d}}{\\textit{dx}}x=\\frac{3}{x}\\)\n\n\nThe derivative of e:\n\n\\(\\frac{\\textit{d}}{\\textit{du}}e^u=e^udu\\)\n\n\\(\\frac{\\textit{d}}{\\textit{dy}}e^{4y}=e^{4y}*\\frac{\\textit{d}}{\\textit{dy}}4y=e^{4y}*4=4^{4y}\\)\n\n\nThere is also the Product and Quotient rules\n\nput those here TK.\n\n\n\nThe Chain Rule\n\nThe chain rule allows you to combine any of the differentiation rules we have already covered\nfirst do the derivative of the outside and then do the derivative of the inside.\n\n\\(\\frac{\\textit{d}}{\\textit{du}}f(g(u))=f'(g(u))*g'(u)*du\\)\n\n\n\n\nPartial derivatives\nPartial derivatives are a way to derive functions that have more than one independent variable.\nPartial derivatives use a notation that is intentionally similar to that of regular derivatives. Their overall format is the same but the shorthand symbols, such as “dx,” are replaced by their stylized version of the letter d: \\(\\partial\\). Partial derivatives should be labeled as \\(\\frac{\\partial}{\\partial x}\\) or \\(\\frac{\\partial}{\\partial y}\\), depending on the variable being derived.\nThe two derivatives should be read as “The partial derivative, with respect to x,” and “The partial derivative, with respect to y.”\n\nWhen taking a partial derivative with respect to a particular variable, treat all other variables as though they are constants.\n\nWhen deriving with respect to x, treat y as a constant, and do not derive the y.\n\n\n\nExample:\n\\(f(x,y)=x^2y+2x^3\\)\n\\(f(x,y)=xy^2+x^3\\)\n\n\\(\\frac{\\partial}{\\partial x}=\\frac{\\partial}{\\partial x}(x)y^2+\\frac{\\partial}{\\partial x}(x^3)=y^2 +3x^2\\)\n\ntake the partial derivative, with respect to x.\n\nhold y constant.\nthe \\(y^2+3x^2\\) is the rate of change for the function.\n\n\n\\(\\frac{\\partial}{\\partial y}=\\frac{\\partial}{\\partial y}x(y^2)+\\frac{\\partial}{\\partial y}x^3=2xy+0=2xy\\)\n\ntake the partial derivative, with respect to y.\n\nhold x constant. `\nthe \\(2xy\\) is the rate of change for the function."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#integration",
    "href": "notes/ICPSR_Math_Review/index.html#integration",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Integration",
    "text": "Integration\n\nThe integral provides us with information about the area under the function.\nThe indefinite integral of a function is a function, but the definite integral may be a number.\n\nExample:\n\nthe integral of the rate of function is the distance function\nintegrating over a specified time tells us how far we’ve gone.\n\n\nTwo forms of integration:\n\nindefinite (anti-derivative)\n\n\\(\\int f(x)dx\\)\n\nintegrate the whole thing.\n\nWhen doing an indefinite integral, we must add a constant to our function\n\ndefinite\n\n\\(\\int_{a}^{b}f(x)dx\\)\n\nnotice the integral sign (the a and b). These serve as limits and tell us what values to integrate over.\nthe integration is going over a lower end to a positive end.\n\n\n\n\nTK TK TK TK MORE ON INTEGRALS HERE + FORMULAS"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#calculus-in-statistics",
    "href": "notes/ICPSR_Math_Review/index.html#calculus-in-statistics",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Calculus in Statistics",
    "text": "Calculus in Statistics\nWe use calculus in many ways, some include:\n\ncontinuous density functions\nfinding the expected value (mean) of a distribution\nfinding the variance (standard deviation) of a distribution\nfinding the median of a distribution\n\n\nProbability Density Function (pdf)\n\nThe probability density function of a variable tells us the probability of a certain event when a continuum of events is possible.\nThe pdf of x is usually noted by the lowercase, f, i.e. f(x).\nThe probability of events at a particular point.\nThe area under a probability density function is 1.\n\nthe integral of the pdf over all events is 1.\n\nthink about what this looks like.\n\nif you were to shade in the pdf, you would color the whole thing!\n\n\n\n\n\n\nCumulative Density Function (cdf)\n\nThe cdf measures the area under the pdf.\n\ncdf is the integral of the pdf.\n\ndenoted with a capital F, i.e. F(x)\n\nEND DAY 3!"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#continuous-probability-distributions",
    "href": "notes/ICPSR_Math_Review/index.html#continuous-probability-distributions",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Continuous Probability Distributions",
    "text": "Continuous Probability Distributions\n\nContinuous random variables can take on an infinite number of possible values, corresponding to every value in an interval.\nvalues where the curve is high = higher probability of occurring.\nwe model a continuous random variable with a curve f(x), called a probability density function (pdf).\n\nf(x) represents the height of the curve at point x\nfor continuous random variables probabilities are areas under the curve.\n\nThe probability that a random value of x is exactly equal to one specific value is ZERO\n\nthis is because the variable is continuous.\nthe value can be an infinitely small value.\n\nso we use ranges. (Pa&lt;X&lt;b)\n\nneed to find area between an interval.\n\n\n\nTotal area under the curve is always 1.\nWhat are continuous probability distributions?\n\nthey describe the probabilities of a continuous random variable’s possible values.\nThere a couple distributions we should know:\n\nUniform, Normal (Gaussian), Standardized normal, Gamma,\n\n\nProbabilities and percentiles are found by integrating the probability density function.\n\nDeriving the mean and variance also requires integration."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#uniform-distribution",
    "href": "notes/ICPSR_Math_Review/index.html#uniform-distribution",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\noften called the rectangular distribution.\nFunction of x; A, B\n\nwhere X is a random variable\nwhere A and B are the known values of the lower bound and upper bound\n\nRepresented mathematically as:\n\n\\(\\frac{1}{B-A}\\)\n\nthe pdf will be the function above. Elsewhere it will be 0.\n\n\nHow the Uniform Distribution looks:\n\n\n\n\nUniform Distribution (visual)"
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#normal-distribution-gaussian",
    "href": "notes/ICPSR_Math_Review/index.html#normal-distribution-gaussian",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Normal Distribution (Gaussian)",
    "text": "Normal Distribution (Gaussian)\n\nMost important continuous probability distribution in the entire field of statistics\nApproximately describes many phenomena that occur in nature, industry, and research. For example, physical measurements in areas such as meteorological experiments, rainfall studies, and measurements of manufactured parts.\nThis distribution is our assumption in regression\n\nwe assume normal distribution.\n\nBell-shaped.\nThe highest probability of events to occur will happen around the mean.\n\nfarther from the mean = less probability of event occurring.\n\nMean = median = mode\nThe beauty of the normal curve:\n\nNo matter what \\(\\mu\\) and \\(\\sigma\\) are, the area between \\(\\mu-\\sigma\\) and \\(\\mu+\\sigma\\) is about 68%; the area between \\(\\mu-2\\sigma\\) and \\(u+2\\sigma\\) is about 95%; and the area between \\(\\mu-3\\sigma\\) and \\(\\mu+3\\sigma\\) is about 99.7%. Almost all values fall within 3 standard deviations.\n\nchanging the variance simply moves away how far away that 99.7% is. Think: how spread affects the shape of the distribution and subsequently the area under the curve.\n\n\n\n\nStandard Normal (Z):\n\nsimilar to the normal distribution\n\nwe are just standardizing.\n\nnormal distribution can have any mean and standard deviation\nthe standard normal distribution has a mean of 0 and a standard deviation to 1.\n\nmakes it easier to interpret and solve\nStandard normal distribution simply translates the normal distribution using z-scores.\n\nwill give us the same area under the curve.\n\n\n\n\nz-scores represent the area under the curve from a value of negative infinity to the z-score.\n\nif z-score is 0, then we get .5 = which means = .5 area under the curve.\n\nBelow is an image that shows the difference between the normal distribution and the standard normal.\n\nnotice the mean and standard deviation values."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#gamma-distribution",
    "href": "notes/ICPSR_Math_Review/index.html#gamma-distribution",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\n\nThe gamma function is only for positive values.\n\nwe are using it for only values that can be positive.\n\nuse for something that cannot be negative (time).\n\n\nBasically can get us a bunch of other distributions.\n\n\nExponential Distribution\n\nPoisson distribution is used to compute the probability of specific numbers of events during a particular period of time or span of space.\n\nlooking across a time period or space.\ncount data.\n\njust need to know lambda to define shape of distribution."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#chi-squared-distribution",
    "href": "notes/ICPSR_Math_Review/index.html#chi-squared-distribution",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Chi-squared Distribution",
    "text": "Chi-squared Distribution\n\nA special case of the gamma distribution.\nall we need to know is the degrees of freedom.\n\nEND DAY 4."
  },
  {
    "objectID": "notes/ICPSR_Math_Review/index.html#day-5",
    "href": "notes/ICPSR_Math_Review/index.html#day-5",
    "title": "ICPSR - Math For Social Science (workshop)",
    "section": "Day 5",
    "text": "Day 5\n\nDiscrete Probability Distributions\n\nNot continuous (duh).\nhave to be a whole number!\n\nwe treat these different.\ncount data!\n\nWe don’t have to integrate\ninstead of looking at a PDF, we look at a probability mass function. (PMF)\nWe can get a probability for an exact value!\nsummation instead of integration\nREMEMBER: X IS NOW COUNTABLE - MUST BE A WHOLE NUMBER TO USE THESE DISTRIBUTIONS!\nDiscrete random variable:\n\nvalues consitute a finite or countably infinite set\n\n\n\n\nBernoulli Random Variable\n\nany random variable whose only possible values are 0 and 1 is called a Bernoulli random variable.\nbinary, yes or no, right or wrong, True or False.\nEach trial is independent.\nWhat we count is called a success\n\neverything else is called a failure\n\nP(success) = p\n\nP(failure) = 1 - p\n\nLet X = 1 if a success occurs, and X = 0 if a failure occurs.\n\nThen X has a Bernoulli distribution\n\n\\(P(X=x)=p^x(1-p)^{1-x}\\)\n\nthis is referred to the mass function of the Bernoulli distribution.\n\n\n\nWhy is the Bernoulli important?\n\nsome other common discrete probability distributions are built on the assumption of independent Bernoulli trials.\n\nBinomial, geometric, negative binomial\n\n\n\n\n\nGeometric Random Variable\n\nDistribution of the number of trials to get the first success in n Bernoulli trials.\nSimilar in the sense that it is also a specific value BUT we are interested in the count of how long it takes us to achieve a success.\n\n\n\nBinomial Distribution\n\nDistribution of the number of success in n independent Bernoulli Trials.\n\n\n\n\nNegative Binomial\n\nThe distribution of the number of trials to get the rth success in independent Bernoulli Trials.\n\n\n\nPoisson Distribution\n\nPoisson can either be continuous or discrete.\nthis section focuses on discrete version of poisson distribution."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am a political science PhD student at the University of Colorado Boulder. I study american politics and methodology. I am primarily interested in the political implications of the built environment. When not studying, you can find me at a music festival, building legos, or attempting to stay fit."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome!",
    "section": "",
    "text": "I am a political science PhD student at the University of Colorado Boulder. I study american politics and methodology. I am primarily interested in the political implications of the built environment. When not studying, you can find me at a music festival, building legos, or attempting to stay fit."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Welcome!",
    "section": "Education",
    "text": "Education\nUniversity of Colorado Boulder | Boulder, CO | PhD Student in Political Science | August 2023 - Present\nUniversity of California, Los Angeles | Los Angeles, CA | B.A in Political Science | Graduated December 2020\nCollege of the Desert | Palm Desert, CA | AA-T in Political Science | Graduated May 2018"
  },
  {
    "objectID": "index.html#fellowship",
    "href": "index.html#fellowship",
    "title": "Welcome!",
    "section": "Fellowship",
    "text": "Fellowship\nInstitute of Behavioral Science | 2024 - Present\nRace, Ethnicity, and Politics (REP) Lab | 2024 - Present\nAmerican Political Research Lab (APRL) | 2023 - Present"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Text as Data\n\n\n\n2024\n\n\nFall\n\n\nMethods\n\n\n\nText as Data with Alex Siegel\n\n\n\nStone Neilon\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule of Law\n\n\n\nSpring\n\n\n2024\n\n\nAmerican\n\n\n\nRule of Law with Josh Strayhorn\n\n\n\nStone Neilon\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning: Applications in Social Science Research (ICPSR)\n\n\n\nSummer\n\n\nICPSR\n\n\n2024\n\n\nMethods\n\n\n\n\n\n\n\nStone Neilon\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nICPSR - Math For Social Science (workshop)\n\n\n\n\n\n\nStone Neilon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICPSR - Introduction to Python\n\n\n\nSummer\n\n\nICPSR\n\n\n2024\n\n\nMethods\n\n\n\n\n\n\n\nStone Neilon\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData 1 & 2\n\n\n\nSpring\n\n\nFall\n\n\n2024\n\n\nMethods\n\n\n2023\n\n\n\nCombined notes from Data 1 & 2 with Anand Sokhey and Andy Phillips\n\n\n\nStone Neilon\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparative Behavior\n\n\n\nSpring\n\n\n2024\n\n\nComparative\n\n\n\nComparative Behavior with Jennifer Fitzgerald\n\n\n\nStone Neilon\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Politics Core\n\n\n\n2024\n\n\nFall\n\n\nAmerican\n\n\n\nNotes from American core with Josh Strayhorn\n\n\n\nStone Neilon\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Institutions\n\n\n\n2023\n\n\nAmerican\n\n\nFall\n\n\n\nNotes from American Institutions class with Chinnu\n\n\n\nStone Neilon\n\n\nDec 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "In Progress."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "In Progress"
  },
  {
    "objectID": "posts/2024-05-28-first-blog-test/index.html",
    "href": "posts/2024-05-28-first-blog-test/index.html",
    "title": "My first blog!",
    "section": "",
    "text": "My First Blog!\nAs a recovering twitter addict, the need to voice my opinion has grown in my time away. Now, no longer restricted to 180 characters, I have the power to annoy in a longer format! No one is safe, Dobby is free.\n\n\n\n\nCitationBibTeX citation:@online{neilon2024,\n  author = {Neilon, Stone},\n  title = {My First Blog!},\n  date = {2024-05-28},\n  url = {https://stoneneilon.github.io/posts/2024-05-28-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNeilon, Stone. 2024. “My First Blog!” May 28, 2024. https://stoneneilon.github.io/posts/2024-05-28-my-blog-post/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a PhD student at the University of Colorado Boulder. I fields are american politics and methodology. My primary research interest regards the political implications of the built environment. I am an Instituite of Behavioral Science (IBS) affiliate in the population department; a member of the American Political Research Lab (APRL); and a member of the Race, Ethnicity, and Politics (REP) lab. When not studying, you can find me at a music festival, building legos, or attempting to stay fit."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I am a PhD student at the University of Colorado Boulder. I fields are american politics and methodology. My primary research interest regards the political implications of the built environment. I am an Instituite of Behavioral Science (IBS) affiliate in the population department; a member of the American Political Research Lab (APRL); and a member of the Race, Ethnicity, and Politics (REP) lab. When not studying, you can find me at a music festival, building legos, or attempting to stay fit."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of Colorado Boulder | Boulder, CO | PhD Student in Political Science | Aug 2023 - Present\nUniversity of California, Los Angeles | Los Angeles, CA | B.A in Political Science | Graduated December 2020\nCollege of the Desert | Palm Desert, CA | AA-T in Political Science | Graduated May 2018"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html",
    "href": "notes/Comparative_Behavior/index.html",
    "title": "Comparative Behavior",
    "section": "",
    "text": "This week was primarily introductions from other classmates and the professor. Some discussion included expectations and what the class would look like."
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#anderson-christopher-j.-2009.-interaction-of-structures-and-voter-behavior.-ohpb-ch.-31-589-609.",
    "href": "notes/Comparative_Behavior/index.html#anderson-christopher-j.-2009.-interaction-of-structures-and-voter-behavior.-ohpb-ch.-31-589-609.",
    "title": "Comparative Behavior",
    "section": "Anderson, Christopher J. 2009. “Interaction of Structures and Voter Behavior.” OHPB CH. 31: 589-609.",
    "text": "Anderson, Christopher J. 2009. “Interaction of Structures and Voter Behavior.” OHPB CH. 31: 589-609.\nBumper Sticker: Rules & context influence and are influenced by behaviour! (British spelling)\nAbstract: This article discusses and reviews the growing literature on the nexus of macro-level structures and individual behaviour that some studies are a part of. It looks at the effects that macro-level institutions and contexts have on citizen behaviour, along with how political institutions and the environment where citizens form opinions and act, help in moderating the effects of individual-level factors on citizen behaviour. The modelling structures and behaviour, effects of structures on voter behaviour, and the interactions of structures and behaviour in research on economic voting are some of the topics covered in the article.\nOutline:\n\nDoes x cause behaviour y? It depends!\n\nExample: Does institutional performance affect people’s sense of whether their political system is legitimate?\n\nAnswer: The impact of corruption on system support is conditional on whether citizens are supporters of the incumbent government.\n\nStudies are getting better at using institutions and context to predict the effects of citizen behaviour.\n\nComparative study of structures and behaviour = citizens in context \nContext and behaviour intimately connected by:\n\nFormal and informal rules; people’s preferences, attitudes, and behaviour affect the establishment and functioning of rules\nCitizens are exposed to variable social, political, and economic environments that they are supposed to understand, interpret, and sometimes shape\n\nAdvances in surveys, replication, and computing have allowed for cross-national and multi-level research into behaviour\n1980s and 1990s renewed focus on institutional questions across polysci that could be tested, as well as interest in developing contextual theories of political behaviour\nComparative study of behaviour politics has investigated macro-level contexts/structures:\n\nInstitutions\nStructural conditions\n\nInteraction between structures and behaviour presumes several things:\n\nPolitics is about the interaction of people’s values and the rules and conditions that govern the implementation of those values\nThe rules and realities in which citizens make choices are themselves a function of people’s values\n\n“Put another way: contexts are critical for understanding the decisions people make because they affect different people differently, and people’s decisions, in turn, shape the nature, shape, and stability of these contexts” (678).\n\n\nCommon approach assumes that context shapes behaviour; assumes behaviour to be DV and that it is exogenous and stable\n\nDo institutions have direct effects on behaviour?\n\nVoting example: structures can affect voters directly, indirectly, and interactively (or contingently)\n\nDirect: rational choice theory; individuals weigh costs and benefits of voting and act accordingly\nIndirect: behaviours of elites within electoral rules empower or constrain citizen choices; structures have consequences but these consequences have secondary (or indirect) effects on behaviour\nContingent: effect of some structural feature strengthened or weakened, depending on presence of some 3rd variable; the turnout gap between individuals with many and few resources is particularly pronounced in countries where the cost of voting is high\n\n\n\n\n{INSERT PHOTO HERE}\n\nRecent studies have looked at how the nature of a country’s representative structures interacts with the willingness of voters to punish governments for bad economic performance\n\nBad economy impact hinges on ability of voters to hold gov’t responsible\nInstitutions can hamper this ability of voters to reward or punish gov’ts\nClarity of responsibility also varies over time within (and across) countries b/c of election outcomes that change bargaining power and reshape context\nAlso contingent upon credible alternatives for voters to vote into office\n\nInteractions of vote choice and structures in research on legitimacy\n\nWhat role do institutions play in moderating sense of loss or victory citizens feel?\n\nInstitutions shape the responses of winners and losers; citizen attitudes toward democratic institutions shaped by country’s political context\n\n\n“At the end of the day, what is particularly noteworthy about cross-level investigations of behavioural politics is that they hold the promise of producing a more nuanced and contextualized understanding of political life by connected hitherto unconnected streams of scholarship in the areas of institutions, political economy, policy, and behaviour and allowing us a better and more complex empirical and theoretical handle on the hows and whys of citizen politics” (690)."
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#chong-dennis.-2013.-degrees-of-rationality-in-politics.-ch.-4-in-leonie-huddy-david-o.-sears-and-jack-s.-levy-eds.-the-oxford-handbook-of-political-psychology-2nd-edition.-oxford-university-press",
    "href": "notes/Comparative_Behavior/index.html#chong-dennis.-2013.-degrees-of-rationality-in-politics.-ch.-4-in-leonie-huddy-david-o.-sears-and-jack-s.-levy-eds.-the-oxford-handbook-of-political-psychology-2nd-edition.-oxford-university-press",
    "title": "Comparative Behavior",
    "section": "Chong, Dennis. 2013. “Degrees of Rationality in Politics.” Ch. 4 in Leonie Huddy, David O. Sears and Jack S. Levy, Eds., The Oxford Handbook of Political Psychology, 2nd edition. Oxford University Press",
    "text": "Chong, Dennis. 2013. “Degrees of Rationality in Politics.” Ch. 4 in Leonie Huddy, David O. Sears and Jack S. Levy, Eds., The Oxford Handbook of Political Psychology, 2nd edition. Oxford University Press\nrieghfjiewaojri"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#lipset-seymour-martin.-1959.-democracy-and-working-class-authoritarianism.-american-sociological-review-244-482-501.",
    "href": "notes/Comparative_Behavior/index.html#lipset-seymour-martin.-1959.-democracy-and-working-class-authoritarianism.-american-sociological-review-244-482-501.",
    "title": "Comparative Behavior",
    "section": "Lipset, Seymour Martin. 1959. “Democracy and Working-Class Authoritarianism.” American Sociological Review 24(4): 482-501.",
    "text": "Lipset, Seymour Martin. 1959. “Democracy and Working-Class Authoritarianism.” American Sociological Review 24(4): 482-501.\n\nDependent Variable: Support for democracy \n\n\nIndependent Variable: Authoritarian predisposition of the lower strata \nANTECEDENT FACTORS OF IV: Lower strata lack “sophistication” and suffer economic and  psychological insecurities (intervening variables). \n\n\nKEY TERMS:\nauthoritarianism, democracy, strata, liberalism (political and economic),  conservatism, communism, socialism \n\n\nPUZZLE:\nThe intellectual left has a hard time reconciling the normative position of a  “proletarian” revolution as a necessary progressive force and the evidence of a “totalitarian degeneration of Communism” (quoting “The Choice of Comrades,”Encounter, 3 (December 1954), p. 25. Arnold A. Rogow, cited p.482).  \n\n\nKEY ARGUMENT:\nFollowing evidence and psychological research on personality traits of  different society’s strata, Lipset posits that the lower strata are predisposed to support  authoritarianism due to a lack of “sophistication” and facing economic and psychological  insecurity. The lower class will support economic liberalism but not political liberalism, while the  middle and upper class will support political liberalism. Hence, according to Lipset, there is a  natural link between the lower-class attitudes and their support of Communism. \n-Pre-1914: working-class supported both economic and political liberalism \n-Post-1914: “The intransigent, intolerant, and demonological aspects of Communist ideology attract members from the lower class of low-income, low-status occupations, and little education” (483).\n\n\nDEMOCRATIC VALUES AND STRATIFICATION:\n• “Leftism/economic liberalism associated with socio-economic status” (485). • “Psychologically oriented investigators have studied the social correlates of authoritarian  personality structures as measured by the now famous F scale”(Adorno, 1950, cited p.  486). \n• Even in conservatism, economic status predicts tolerance levels (see Eysenck, 1956 and  Stouffer, 1955, cited p.486)\n\n\nAUTHORITARIAN RELIGION AND STRATIFICATION:\n• Fundamentalist and chiliastic/millennial religious tradition as a predisposition for drastic  and rapid measures demanding authoritarian order. \n• The appeal of fundamentalism to the lower class acts as a pressure valve in a utopian world  where the upper class no longer dominates. \n• See Engels and his take on evangelism (488). \n\n\nTYPICAL SOCIAL SITUATION OF LOWER-CLASS PERSON\n\n“There is consistent evidence that degree of formal education, itself closely correlated with  social and economic status, is also highly correlated with undemocratic attitudes” (489). • Education predicts attitudes better than occupation (Morris Janowitz and Dwaine Marvick study, cited pp.489-490). \nPrincipal factors predisposing the lower class to authoritarian attitude are a lack of  “sophistication,” economic security, and psychological security -&gt; mental shortcuts and  need of immediacy = “surviving”  \n\n\n\nTHE PERSPECTIVES OF LOWER-CLASS GROUPS\n\nPrincipal factors predisposing the lower class to authoritarian attitude are a lack of  “sophistication,” economic security, and psychological security -&gt; mental shortcuts and  need of immediacy = “surviving”\nIncapacity of mental abstraction and fixed mental context\n\n\n\nEXTREMISM AS A COMPLEX ALTERNATIVE: A TEST OF HYPOTHESIS\n\nWhen the Communist Party is small and weak, it tends to uphold non-economic  liberalism/democratic values because of the presence of intellectual leadership. \nWhen the Communist Party (or Socialist in some cases) is a mass party, it tends to be more  authoritarian (or less committed to democratic values) because it has stripped its  intellectual leadership.\nUnderdeveloped countries/economies exacerbate the problems of the lower class and  increase the predisposition to authoritarianism on a like-for-like basis. \nThis argument is evaluated subnationally and supports the urban/rural predisposition to  uphold democratic values and political liberalism.\n\nHISTORICAL PATTERNS AND DEMOCRATIC ACTIONS\n\nHistorically, the left has been responsible for democratization; intellectual leadership still  supports democratic values, but the masses of the lower strata do not understand the  implications of the left beyond economic value to them. \nConservatism is vulnerable in democracy for demographic reasons (500). • Neither conservatism nor the authoritarian predisposition of the lower class should be seen  as a threat to democracy. \nEducational attainment is an essential “Social Requisites of Democracy” (Lipset, 1959)."
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#zuckerman-alan-s.-2005.-returning-to-the-social-logic-of-political-behavior.-ch.1-in-alan-s.-zuckerman-ed.-the-social-logic-of-politics-personal-networks-as-contexts-for-political-behavior.-temple-university-press.",
    "href": "notes/Comparative_Behavior/index.html#zuckerman-alan-s.-2005.-returning-to-the-social-logic-of-political-behavior.-ch.1-in-alan-s.-zuckerman-ed.-the-social-logic-of-politics-personal-networks-as-contexts-for-political-behavior.-temple-university-press.",
    "title": "Comparative Behavior",
    "section": "Zuckerman, Alan S. 2005. “Returning to the Social Logic of Political Behavior.” Ch.1 in Alan S. Zuckerman, Ed., The Social Logic of Politics: Personal Networks as Contexts for Political Behavior. Temple University Press.",
    "text": "Zuckerman, Alan S. 2005. “Returning to the Social Logic of Political Behavior.” Ch.1 in Alan S. Zuckerman, Ed., The Social Logic of Politics: Personal Networks as Contexts for Political Behavior. Temple University Press.\n\nChapter 1: Returning to the Social Logic of Political Behavior\n\n\nBumper Sticker:\n“Despite a period of deviation from theory, social logic is important for studying political choice.” \n\n\nSummary: \nThis chapter offers an intellectual history of the critical texts that defined the research orientation of political behavior. Is focuses on how traditional theories from sociology and psychology informed our understanding of political behaviors through interactions with primary groups. However, along the way, the political science discipline that studied behavior began undergoing changes to the way behavioral study was approached - emphasizing, instead, individual level survey research and a reconceptualization of social logic. Fortunately, social logic is back on the mend. \n\n\nResearch Question/Purpose: \nThe purpose of this chapter is to outline the reasons for which those who study political choice and behavior should return to the social logic of the original behavioral revolution in political science. \n\n\nData/Methods: \nThere is no data used. Passages, statements, and long quotations stand in for the data. Draws on authors such as Campbell, Converse, Miller, Stokes, Verba Findings: \nWe need to return to our roots an incorporate social logic back into behavioral political science \nInterpersonal relationships can both prevent and facilitate change \nMany scholars broke away from social logic, pursuing instead individual level analysis from surveys and redefining social logic to fit their particular needs Newer research is returning to social logic\n\n\nDefinitions: \nSocial Logic of Politics:\na theory of politics that emphasizes the ways in which social factors (social identities, distribution of power across groups, public opinion) influence individual behaviors and how those interact with institutional structures to effect the political process. \n\n\nDiscussion Questions: \nWhat are the consequences of the social logic of politics for our understanding of those communities/persons with intersectional identities? Is this theory anglo centric, or does it retain its superior qualities across groups? \nHow might association through primary groups have an impact of institutions? At what point does the saliency of group association diminish? Do our politics/attitudes change drastically with each primary group we grow closer to, or do those groups we first develop our attitudes through matter most? \n\n\nAdditional Notes: \n\n\nIntro: \nThe immediate social circumstances of people’s lives influence what they believe and do about politics \nImmediate social circles are powerful forces in swaying ones political beliefs and actions \nGroup relationships are important for fixing and maintaining opinions in relation to large parts of the political system (such as parties) (Key, 1961) \n\n\nSources in Sociology: \nparticularly influential were Lazarsfeld, Katz, Berelson, Gaudet, and McPhee Electoral Sociology was where the onset of the behavioral revolution in political science first observed electoral choice \nVoting is a reflection of our behavior - of our conversations with other people and the attitudes we shape through those interactions \nBy this logic, we vote based on how the relationships we have shape us This is counter to theories of voting as a rational act \n\n\nSources in Social Psychology \nparticularly influential were Kurt Lewin and Leon Festinger \nhelped explain conformity\nsharing an identity (social class, ethnicity, religion, etc.) does NOT define a social group \nopinions, preferences, and beliefs are a joint function of how “real” the matter is, the views held by the members of a person’s groups, and the persons own conceptions joining a group tends to produce changes in opinions and attitudes toward alignment with the group \n\n\nThe Turn Away from the Social Logic of Politics \n\nStarted to examine individuals instead of groups \nBegan surveying a lot (members of the same social circles shouldn’t be surveyed together -- its no longer as-if random, and you introduce bias into your survey responses) \nTurned social groups into objects of individual identification \nElectoral research shifted towards understanding party identification and perceptions of candidates \n\n\nWhat factors explain this shift away from the social logic? \nOne set derives from decision to use national sample surveys as the exclusive source of empirical evidence of behavior \nAnother set derives from theory issues -- PoliSci insisted that sociologists couldn’t explain electoral decisions or outcomes -- opted instead for immediate determinants of vote choice: attitudes and calculations \nStatistics professors, rearing political sciences, conveyed cherished assumptions of independent sampling -- which makes it nearly impossible to capture the psychology of inter-dependance \n\n\nRecognizing that data problems inhibited their ability to follow theoretical preferences \nCampbell, Converse, Miller, Stokes altered the definition of the social group and conceptualized it according to a personal perceptions regarding a reference object - taking it far away from its original intent/definition from Lewis Many others would make even larger breaks, such as Key and Downs (immediate circle provides no more than time-saving sources of information to calculating citizens) \n\n\n\nReturning (Again) to the Social Logic of Politics \nWe returned! \nMichigan survey responses can be used to study the role of immediate social structures on political choice and behavior \nTon of other research is starting to do the same thing"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#stones-motivated-thoughts",
    "href": "notes/Comparative_Behavior/index.html#stones-motivated-thoughts",
    "title": "Comparative Behavior",
    "section": "Stone’s Motivated Thoughts",
    "text": "Stone’s Motivated Thoughts\nInsert here."
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#converse-philip-e.-2006-1964.-the-nature-of-belief-systems-in-mass-publics.-critical-review-a-journal-of-politics-and-society-181-1-74",
    "href": "notes/Comparative_Behavior/index.html#converse-philip-e.-2006-1964.-the-nature-of-belief-systems-in-mass-publics.-critical-review-a-journal-of-politics-and-society-181-1-74",
    "title": "Comparative Behavior",
    "section": "Converse, Philip E. 2006 [1964]. “The Nature of Belief Systems in Mass Publics.” Critical  Review: A Journal of Politics and Society 18(1): 1-74",
    "text": "Converse, Philip E. 2006 [1964]. “The Nature of Belief Systems in Mass Publics.” Critical  Review: A Journal of Politics and Society 18(1): 1-74\n\nNormative question: \nDoes the citizenry have the “capacity” to participate in democracy? Concern the age-old  question of the demos’ potential to access information and process it in a way that would be  individually coherent in election expressions. Converse’s (1964) “The Nature of Belief Systems  in Mass Publics” marks a turning point since it can quantitatively/statistically demonstrate\ndemocracy defenders’ worst fears: the mass public competence for a normative functioning of  democracy is blight. This question has also been approached with similar results by Lipmann (1922), “Public Opinion” and Schumpeter (1950), “Capitalism, Socialism and Democracy” (see  also Bennett, 2006, “Democratic Competence, Before Converse and After”). \n\n\nBumper Sticker:\nMass Public: Ideologically Innocent  \n\n\nDV:\nDegree of Belief System Coherence (Individual) (a continuum)\n\n\nIV:\nLevel of Constrained Idea-Elements (a continuum)\n\n\nData:\nConverse stipulates that the degree of coherence of the mass public belief system is contingent on the level of constraints of the idea-elements populating a belief system: the more constrained, the  more coherent. This, in turn, predicts a degree of capacity to participate in democracy according  to one’s self-assessed ideological placement. Idea-elements are mainly constrained by the  transmission of information and its digestibility. For this reason, Converse finds that 15% of the  population is deemed ideologue or near-ideologue (possessing a coherent belief system = elite political actors). In contrast, the remaining 85% (the masses) float between having a particular  interest (issue public) or having no idea. If ideology is not driving most of the public’s opinion  incentive, what is? Converse suggests that “visible” social groupings serve as a stable heuristic  informing public opinion: party ID (.70) has more bearing than policies, but policies concerning  visible groups (school desegregation .45) have more bearing than regular policies (government and housing .28). This “ideology by proxy” might produce idiosyncratic outcomes as well: the  working man associating with Socialists against the rich but against government reigning of private  enterprise’s provision of public goods. \n\nPICTURE HERE\nAcquiring and processing information is contingent on education. Education is more prevalent in  higher strata of society (Lipset, 1959, “Social Requisite of Democracy”).\n\nPICTURE HERE\n\n\nInteresting proposition: “The net result of these circumstances is that the elites of the leftist parties enjoy a”natural” numerical superiority, yet they are cursed with a clientele that is less dependable or solid in its support. The rightist elite has a natural clientele that is more limited but more dependable.”"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#huckfeldt-robert.-2009.-information-persuasion-and-political-communication.-ohpb.-ch.-6-100-122.",
    "href": "notes/Comparative_Behavior/index.html#huckfeldt-robert.-2009.-information-persuasion-and-political-communication.-ohpb.-ch.-6-100-122.",
    "title": "Comparative Behavior",
    "section": "Huckfeldt, Robert. 2009. “Information, Persuasion and Political Communication.” OHPB. Ch. 6: 100-122.",
    "text": "Huckfeldt, Robert. 2009. “Information, Persuasion and Political Communication.” OHPB. Ch. 6: 100-122.\n\nBumper Sticker:\nSocial networks allow us to rethink and re-conceptualize the role of groups in mass politics and public opinion.\n\n\nAbstract\nThis article presents a survey and interpretation of the contributions made by #network #theories on the study of citizens and democratic policies. The article serves as an overview of the topic. It begins by locating the network research within the rich substantive and theoretical tradition of individually and #group-based studies of #electoral #politics and #public #opinion . It addresses some methodological issues in the study of political information #networks. The article ends with the discussion of theoretical and substantive insights that were generated in several studies, such as the study of communication and persuasion among citizens.\n\n\nResearch Question:\nWhat is network analysis? What are the theoretical and substantive insights that we can derive from it, particularly regarding the study of communication and persuasion among citizens? What are some of the methodological issues of network analysis?\n\n\nSummary/Theory/Argument:\nThe author sets out to analyze the contribution of network theories, which have provided us the ability to analyze citizen and democratic politics from across the micro-macro spectrum. Network theories draw on the importance of groups, communities, and political information networks among and between individuals (Downs 1957) while incorporating a conceptual apparatus that extends far past the traditional notions of primary groups, organizations, and societal groups in order to define the relationships that exist among individuals at multiple levels of analysis. Despite its advantages, there are still some methodological issues with network analysis.\n\n\nData/Methods:\nIt is a literature review so no real data/methods.\n\n\nFindings:\n\nWhat is network analysis and how does it fit into the existing literature on the study of individuals and groups?\n\nNetwork studies can be seen as a particular species within a larger genus—as one type of a contextual analysis of politics (Knoke 1990)\nnetworks are formed at the complex intersection between individual preference, individual engagement, and individual location within particular contexts.\nEulau (1986) and Przeworski and Teune (1970) define contextual factors in terms of the aggregation of individual characteristics that affect individuals through processes of social interaction.\n\nNetwork Studies diverge from contextual studies in their effort to incorporate a direct mapping for the particular patterns of recurrent interaction among actors.\n\nIndividual level and aggregate level analysis both suffer from the same problem: they ignore the implications that arise due to patterns of individual interdependence located in time, place, and setting.\n\n\n\nWhat are some of the methodological issues of the study of political information networks?\n\nAbsent direct measures on patterns of communication, neither the individual measures nor their associated aggregate versions directly address the specifics of communication and persuasion among the individuals who make up the aggregates.\nNetwork studies are particularly useful in analyzing well defined populations - such as clergies, political elites, court, and legislatures.\n\nan unfortunate limitation of egocentric networks is the failure to embed dyads within larger networks and in the context of all other dyads in that same network.\nHowever, they aren’t straightforward when analyzing large populations - which is the primary object of study for scholars concerned with studies of mass behavior (egocentric network survey questions and snowball surveys can help address this; pg. 5-6)\n\nPolitical communication networks are created at the intersection of individual choice (demand) and environmental supply (which is stochastic).\n\n\n\nWhat are the substantive and theoretical insights generated by it?\n\nPeople are more inclined to discuss politics with others who share similar political beliefs\nStudies ALSO demonstrate that patterns of both agreement and disagreement can be profitably understood within complex processes of communication and persuasion\n\nThis means that there is persistent heterogeneity of opinions in less-dense (larger) social networks people aren’t as afraid of disagreement as we initially thought\n\nPolitical heterogeneity in the form of cross cutting cleavages can depress political participation BUT increased levels of tolerance (which might be good?)\nPolitical communication networks are more important in less developed democracy, where parties are less institutionalize and politics is more volatile\nDiversity of group discussion decreases the susceptibility of individuals to issue framing by elites\nNetwork studies of political communication and persuasion provide a theoretical, analytical response to the human limitations of the citizen in democratic politics.\n\nthis is important because it to do with the cognitive limitations of individuals in being self-contained, fully informed, independent maximizers\nparticularly useful is social capital - which allows people to rely on one another for accurate information\n\npolitical interdependence among citizens helps to explain why public opinion in the aggregate is more sophisticated than the opinions held by the average citizen - its more fully informed\nsolutions to collective action problems can be seen as occurring within networks of relationships among strategic actors who use the information they acquire through repeated interactions to facilitate group efforts"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#marcus-george-pavlos-vasilopoulos-and-martial-foucault.-2018.-emotional-responses-to-the-charlie-hebdo-attacks-addressing-the-authoritarianism-puzzle.-political-psychology-393-557-575.",
    "href": "notes/Comparative_Behavior/index.html#marcus-george-pavlos-vasilopoulos-and-martial-foucault.-2018.-emotional-responses-to-the-charlie-hebdo-attacks-addressing-the-authoritarianism-puzzle.-political-psychology-393-557-575.",
    "title": "Comparative Behavior",
    "section": "Marcus, George, Pavlos Vasilopoulos, and Martial Foucault. 2018. “Emotional Responses to the Charlie Hebdo Attacks: Addressing the Authoritarianism Puzzle.” Political Psychology 39(3): 557-575.",
    "text": "Marcus, George, Pavlos Vasilopoulos, and Martial Foucault. 2018. “Emotional Responses to the Charlie Hebdo Attacks: Addressing the Authoritarianism Puzzle.” Political Psychology 39(3): 557-575.\n\nBumper Sticker:\nThreat + conservative OR liberal ideology = increased authoritarian preference\n\n\nSummary:\nThe literature shows an interaction between threat and public preference for authoritarianism. There are two lines of literature that provide contradictory findings:\n\nOne argues that threat increases authoritarian preferences among those that are more prone to authoritarianism\nAnother argues that threat is associated with a switch in ideology among those who are normally non-authoritarian Using a two-wave panel study of the French population taken before and after the January 2015 twin attacks in Paris, the authors find that the two occur simultaneously.\n\n\n\nResearch Question:\nWhat best explains the relationship between individual perceptions of threat and increased preferences for authoritarian policy?\nIs the increase in support for authoritarian policies following a terrorist attack the result of:\n\nconservative and authoritarian prone individuals just more strongly manifesting their disposition in response to threat\nOR increases in ideological “switching” to support authoritarian policies in light of threatening events?\n\n\n\nTheory/Argument:\nThe authors argue that the endorsement of authoritarian policies can be explained by the theory of affective intelligence. When people experience anxiety due to novel circumstances, such as that induced by terrorist attacks, they will abandon their habitual political attachments and attend to the circumstance in a risk-averse and conservative manner.\n\nH1: increased anger will enhance decision making based on past dispositional convictions, leading to increased polarization between left and right wing individuals (strengthen authoritarian tendencies for right-wing and weaken those same tendencies for the right)\nH2: increased anxiety will initiate decision-making that is less reliant on extant convictions and hence make those on the left end of the left-right scale more attentive and responsive to calls from the right to adopt authoritarian policies.\n\n\n\nData/Methods:\n\nTwo-wave panel taken before and after the January 2015 twin attacks in Paris\nData retrieved from the CEVIPOF barometer of political confidence\n\nsurvey was conducted using a representative sample containing 1,524 respondents in two waves\n\nConstruct two OLS models\n\none measures attitude change as a function of demographic variables, ideology, as well as anxiety and anger\nsecond model includes two interaction terms that assess fear and anger with the left-right scale respectively with the expectation that the effect of fear and anger on authoritarian policies with be conditional on prior ideological conviction\n\n\nModel 1:\n\\[\nAttiude_{t2}=fear_{t2}+anger_{t2}+left/right\\: scale_{t1}+attitude_{t1}+demographics_{t1}\n\\]\nModel 2:\n\\[\n\\begin{align}Attitude_{t2} &= fear_{t2} + anger_{t2} + left/right\\: scale_{t1} \\\\&+ fear_{t2} * left/right\\: scale_1 + anger_{t2} * left/right\\: scale + attitude_{t1} + demographics_{t1}\\end{align}\n\\]\n\n\nDependent Variable:\n\nsupport for authoritarian policies\na scale consisting of all available items in the study that measure adoption or rejection of authoritarian politic preferences\n\neach item (survey question) was measured using 4-point response option (higher values = support for authoritarian policies)\nonly one factor possessed an eigenvalue over 1\n\n\n\n\nIndependent Variable:\n\nemotional reaction to the terrorist attacks\n\n\n\nFindings:\n\nThreat increases authoritarian preferences among those who are more prone to authoritarianism AND those with non-authoritarian ideology\nOverall, people who felt fear following the attack were more likely to have switched their opinion in an authoritarian direction\nLeft-wing citizens who felt predominantly fearful after the attack were more likely to change in the direction of endorsing authoritarian policies\nRight-wing citizens who felt predominantly fearful did not experience a change in direction of endorsing authoritarian policies\nRight-wing citizens who gelt predominantly angry following the attack experienced heightened endorsement of authoritarianism\nLeft-wing citizen on the far-left who felt predominantly angry were the least likely to support authoritarian politics following the attacks (see figure 3) but is not statistically significant\nH2 is supported by the first OLS model\nH1 is not supported UNTIL the conditional model\n\n\n\nImportance/Contribution:\n\ncontributes to our understanding of individual level variation in the endorsement of authoritarian policies as well as our understanding of attitudinal polarization (why fear might reduce polarization across a public)\nimportant because the psychological mechanisms that account for the public’s reaction to terrorist attacks is key to understanding enhanced support for the restriction of civil liberties\nalso advances and systematizes the literature of emotions and politics by assessing the impact of anxiety and anger on the formation of political attitudes\ntakes advantage of REAL threat perceptions, instead of experimentally manipulated ones\nhas external validity (uses a representative sample of the French population)\nchallenges conventional wisdom that emotions are irrational, turbulent states that hinder reasoning - affect activates political reasoning\n\n\n\nDefinitions/Concepts:\n\nTheories of Affective Intelligence\n\nprovides a framework for understanding the interplay of fear and anger with ideological convictions\nwith respect to anxiety, the theory holds that when citizens find themselves in novel circumstances, they tend to break from habitual political attachments (ideology/partisanship) and actively try to attend to contemporary circumstances about their environment.\nwith respect to anger, the theory holds that it may trigger authoritarian policy preferences through the activation of habitually learned routines, activating conservative or authoritarian attitudes among individuals who already hold a right-wing disposition.\n\nit is not the potency of threat the increases anger, but rather the degree of normative violation\n\n\n\n\nTerror Management Theory (TMT)\n\npeople feel threatened by their own death and therefore adopt worldviews that allow them to find meaning and worth in their lives.\n\nawareness of ones death can cause anxiety\nis used to explain that mortality-related threat reinforces extant political beliefs regardless of whether these are liberal or conservative.\npolitical ideology serves as a protective shield against death anxiety.\n\n\n\n\nLeft-Right Ideological Identification\n\ndefined by the authors as the an organized cluster of political values that make some individuals more inclined to support some political ideas than others.\n\n\n\nThreat\n\ndefined by the authors as an exogenous event that poses harmful consequences for the individual or her environment and evokes negative emotional reactions.\n\n\n\nAnxiety\n\nelevated in threatening circumstances and is one of the prime emotional reactions to a terrorist event when heightened, conveys a perception of increased risk and prompts individuals to adopt risk-averse behavior to eliminate or avert the threat\n\n\n\nAnger\n\ngenerated when people are obstructed from reaching a valued goal by an external agent whose conduct is deemed unfair and in cases when a threatening stimulus is perceived\n\n\n\nThreat Stimulus\n\nAssessed on two distinct grounds:\n\nis the event novel? (+anxiety)\nis the event a normative violation by a familiar enemy (+anger)"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#schwartz-shalom-h.-et-al.-2014.-basic-personal-values-underlie-and-give-coherence-to-political-values-a",
    "href": "notes/Comparative_Behavior/index.html#schwartz-shalom-h.-et-al.-2014.-basic-personal-values-underlie-and-give-coherence-to-political-values-a",
    "title": "Comparative Behavior",
    "section": "Schwartz, Shalom H., et al. 2014. “Basic Personal Values Underlie and Give Coherence to Political Values: A",
    "text": "Schwartz, Shalom H., et al. 2014. “Basic Personal Values Underlie and Give Coherence to Political Values: A\nCross-National Study in 15 Countries.” Political Behavior 36(4): 899-930.\n\nBumper Sticker:\nPersonal values explain political values!\n\n\nIndependent Variable:\nBasic Personal Values\n\n\nDependent Variable:\nPolitical Values\nResearch question: Do political attitudes and values of the general public form a coherent system? If so, what accounts for this structure and gives political values their coherence?\n\nPolitical values = political expressions of more basic personal values\nBasic personal values = security, achievement, benevolence, hedonism; organized on a circular continuum that reflects conflicting/compatible motivations\n\n\n\nData: data from 15 countries using 8 core political values and 10 basic personal values\n\nAdults eligible to vote in Australia, Brazil, Chile, Germany, Greece, Finland, Israel, Italy, Poland, Slovakia, Spain, Turkey, Ukraine, United Kingdom, and UUS\nExcept in Australia, UK, Germany, & Turkey, respondents recruited by university students and completed self-report questionnaire individually\n\n\n\nMethods:\n\nuse basic personal values to predict expected political values; different hypotheses for 12 non-communist and 3 post-communist countries\nData gathered in local language of each country\nPortrait Values Questionnaire (PVQ) measure of 40 short verbal “portraits” of different people matched to respondents’ gender, each describing a person’s goals, aspirations, or wishes\n\n3-6 items measure each value\nFor each portrait, respondents indicate how similar the person is to themselves from “not like me at all” –1 to “very much like me” –6\nMultimethod-multitrait analyses of the 10 values measured with PVQ and with the Schwartz Value Survey\n\n\n\n\nFindings:\n\ncorrelation and regression analyses support almost all hypotheses\nBasic values account for substantially more variance in political values than age, gender, education, and income\nMultidimensional scaling analyses demonstrate graphically how circular motivational continuum of basic personal values structures relations among core political values\nThis study strengthens assumption that individual differences in basic personal values play a critical role in political thought (seems kinda obvious).\n\nNEED TO INSERT FIGURES/PHOTOS!"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#taber-charles-s.-and-milton-lodge.-2006.-motivated-skepticism-in-evaluation-of-political-beliefs.-american-journal-of-political-science-503-755-769.",
    "href": "notes/Comparative_Behavior/index.html#taber-charles-s.-and-milton-lodge.-2006.-motivated-skepticism-in-evaluation-of-political-beliefs.-american-journal-of-political-science-503-755-769.",
    "title": "Comparative Behavior",
    "section": "Taber, Charles S., and Milton Lodge. 2006. “Motivated Skepticism in Evaluation of Political Beliefs.” American Journal of Political Science 50(3): 755-769.",
    "text": "Taber, Charles S., and Milton Lodge. 2006. “Motivated Skepticism in Evaluation of Political Beliefs.” American Journal of Political Science 50(3): 755-769.\n\nBumper Sticker:\nPeople process information through prior bias and double down in the face of new information, resulting in attitude polarization about different issues\nprior attitudes + bias for confirming evidence + skepticism of opposing evidence = attitude polarization\n\n\nResearch Question:\nHow do political beliefs evolve?\nWhat explains political polarization in the face of factual information?\n\n\nSummary of Theory/Argument:\nThe authors postulate what they call a theory of affect-driven motivated reasoning in trying to explain when and why citizens actively process biased information. This theory suggests that people will anchor their evaluation of new information in their own biases - being non-skeptical of information that confirms their biases and being “motivated- skepticals” when engaging in information that is counter to their biases - i.e. spending more time trying to discredit new information. The authors propose a process of “partisan processing” that results in attitude-polarization and which is conditional on the strength of ones prior attitudes and the level of one political sophistication. They test the mechanism as a series of hypothesis that assume the following form:\nH1: there is a prior attitude effect whereby people who feel strongly about an issue - even when encouraged to be objective and leave their preferences aside - will evaluate supportive arguments as stronger and more compelling than arguments that oppose their prior beliefs\nH2: there is a disconfirmation bias, such that people will spend more time and cognitive resources counter-arguing opposing arguments\nH3: there is a confirmation bias, such that when free to choose what information they will expose themselves to, people will seek out confirming arguments over disconfirming ones\nThese combined will results in:\nH4: attitude polarization, where attitudes will become MORE EXTREME, even when people have been exposed to a balanced set of pro and con arguments\nWhich is conditional upon:\nH5: the level of attitude strength effect, such that citizens voicing the strongest policy attitudes will be the most prone to motivated skepticism\nH6: and the degree of political sophistication effect, such that the politically knowledgeable will be more susceptible to motivated bias than will unsophisticates.\n\n\nData/Methods:\n\nTwo experimental studies explore how citizens evaluate arguments about affirmative action and gun control\nThe participants (Ps) were recruited from introductory political science courses at Stony Brook University\nStudy 1: N=126\nStudy 2: N=136\nFirst Part: Confirmation Bias\nThe participants (Ps) were seated at computers and their political attitudes were assessed through the evaluation of a series of contemporary political issues aimed at activating their priors - this was done through random assignment into either condition 1 or condition 2 (see figure 1)\nThey rated the items on a series of scales to assess attitude strength (0-100) and attitude position (like-dislike; 9 item scale)\nThey then viewed information on an information board, where they could seek out hidden policy arguments by known source alone (see figure 2); the amount of time they spend engaging in each argument was recorded by the software.\nThey viewed eight arguments without a time limit, but could only view each argument ONCE\nThey then completed the same attitude battery from the beginning of the experiment before filing out demographic information and a political knowledge scale ( to assess sophistication)\nSecond Part: Disconfirmation Bias\nadministered the battery again, but with the conditions (issues) swapped.\nthen asked to rank the strength of 8 arguments (4 pro and 4 con)\nthen there was a post test battery AGAIN and a recognition memory test\nthey were also asked to list their thoughts regarding two pro and two con arguments they were presented with Arguments were taken from online sources and edited such that they were similar in complexity and length.\n\n\n\nFindings:\n\nstrong evidence of a prior attitude effect (H1) such that attitudinally congruent arguments are evaluated as stronger than attitudinally incongruent arguments.\nParticipants counter-argued the contrary arguments and uncritically accept supporting arguments, evidence of a disconfirmation bias (H2) - this was supported by the participants spending MORE time on the policy arguments that they disagreed with\nAlso find a confirmation bias (H3)—the seeking out of confirmatory evidence—when Ps are free to self-select the source of the arguments they read - this was supported by the participants seeking out information in the matrix that they agreed with\nBoth the confirmation and disconfirmation biases lead to attitude polarization (H4) —the strengthening of t2 over t1 attitudes—especially among those with the strongest priors (H5) and highest levels of political sophistication (H6)."
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#achen-christopher.-2002.-parental-socialization-and-rational-party-id.-political-behavior-242-151-170.",
    "href": "notes/Comparative_Behavior/index.html#achen-christopher.-2002.-parental-socialization-and-rational-party-id.-political-behavior-242-151-170.",
    "title": "Comparative Behavior",
    "section": "Achen, Christopher. 2002. “Parental Socialization and Rational Party ID.” Political Behavior 24(2): 151-170.",
    "text": "Achen, Christopher. 2002. “Parental Socialization and Rational Party ID.” Political Behavior 24(2): 151-170.\n\nAbstract:\n\n\nBumper Sticker\n\n\nModel"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#bankert-alexa-leonie-huddy-and-martin-rosema.-2017.-measuring-partisanship-as-a-social-identity-in-multi-party-systems.-political-behavior-391-103-132.",
    "href": "notes/Comparative_Behavior/index.html#bankert-alexa-leonie-huddy-and-martin-rosema.-2017.-measuring-partisanship-as-a-social-identity-in-multi-party-systems.-political-behavior-391-103-132.",
    "title": "Comparative Behavior",
    "section": "Bankert, Alexa, Leonie Huddy, and Martin Rosema. 2017. “Measuring Partisanship as a Social Identity in Multi-Party Systems.” Political Behavior 39(1): 103-132.",
    "text": "Bankert, Alexa, Leonie Huddy, and Martin Rosema. 2017. “Measuring Partisanship as a Social Identity in Multi-Party Systems.” Political Behavior 39(1): 103-132.\n\nBumper Sticker:\n“multi-item scales of partisanship can work in and across multi-party systems”\n\n\nResearch Question:\nHow (if at all) can we create a multi-item scale that measures partisanship across multi-party systems?\n\n\nSummary/Theory/Argument:\nMost national election studies tend to rely on a single measure of partisanship. Such measurements tend to vary in terms of how they are phrased on surveys and also struggle to capture the range of partisan identity, typically at the low and high ends of the spectrum. The authors believe that they can develop a multi-item scale of partisan identity that better predicts political behavior ACROSS various multiparty democracies than the traditional single item scales common on surveys. \n\n\nData/Methods:\n\nPartisan Identity Scale\nCreates an Eight-Item Scale of Partisanship that is derived from Social Identity Theory Derives three (3) hypothesis to test whether the scale is better than traditional models: H1: The Partisan Identity Scale should differentiate equally well among low, middling, and high levels of identity to best uncover the link between identity and political activity across its full range. Expect each scale item to provide more complete information about partisan strength than the traditional single party identification item. \nIn terms of the IRT analysis, this means that each item’s information function will be more peaked and contain greater information than the standard single measure of partisan strength. In the three European multi-party systems under study, we expect both lower and higher levels of partisan intensity to remain less well detected when measured with the traditional item\nH2: the partisan identity scale to exhibit all three types of invariance, which means that the fit of the metric invariance model will be no worse than the fit of the configural model, and that the fit of the scalar model will be no worse than that of the metric model. \nH3: The partisan identity scale should more powerfully predict in-party voting and political engagement than the traditional party identification item. We also expect the partisan identity scale to better predict political behavior than a multi-item indicator of ideological intensity \n\n\n\nData:\nUses Netherlands, Sweden, and the U.K. as the three countries with which to derive and apply the scale.\n\nNetherlands:\n2012 Dutch Parliamentary elections among members of the Longitudinal Internet Studies for the Social Sciences (LISS) panel. The LISS contains 5000 households, entailing 8000 individuals Data are drawn from three time points: August 2012 (‘’Elections 2012’‘), after the national election in September 2012 (’‘Dutch Parliamentary Election Study’‘), and again as part of a module in December 2012/January 2013 (’‘Politics and Values: Wave 6’’).\n\n\nSweden:\nSwedish data were drawn from the Swedish Citizen Panel, a largely opt-in online panel run by the Laboratory of Opinion Research (LORE) at the University of Gothenburg. \nUtilize data from Panel 8 (11/14/13–12/18/13) and add-on Panel 8-2 (12/10/13–1/7/14). 16,130 panelists were invited to take the Panel 8 survey and 9279 completed it for a completion rate of 64 %. \n2000 panelists were invited to complete Panel 8-2 of which 1496 answered the survey. All panelists in Panel 8.2 and a randomly selected 2000 panelists in Citizen Panel 8 received the identity model. \nOur analytic sample is confined to those in Panel 8 and Panel 8-2 who completed the identity items (N = 2464). \n\n\nUK:\nData for the U.K were taken from the 2015 British Election Study (BES), an online panel study conducted by YouGov. \ndraw on data from pre-election wave 3 of the BES, conducted between September 19, 2014 and October 17, 2014 and pre- election wave 4, conducted in March 2015. \nIn total, 27,839 respondents participated in wave 3 and 6141 were randomly assigned and 5954 completed a module that included the partisan identity items. \nIn wave 4, 16,629 respondents participated and 3500 of them completed the partisan identity module.\n\n\n\nMeasures:\nPartisanship Strength by Country vs. Partisan Identity Strength (the 8 Item Scale that is also known as the Partisan Identity Scale) \n\n\nMethods:\nthe methods are pretty complex, but essentially each hypothesis undergoes a different set of tests/inquiry. \nH1 - figure 1 shows how the 8 item scale that the authors derived is better measure of latent partisan strength than the individual measures in each country. This is also event in the peaks that you can see in each figure. They are more much more sensitive to to the full range of the low- high spectrum. H2 - invariance is measured using R software to perform multi-group confirmatory analysis (CFA) Configural invariance is a good fit \nThe partisan scale has the same metric across countries \nIts also scalar across countries \nH3 - analyse the determinants of in-party voting using a logistic regression model \nPartisan identity strength has a sizeable influence on in-party voting and its effects exceed that of partisanship strength. \nPartisan identity predicted in- party vote and participation while holding partisan strength constant\n\n\nFindings:\nLevels of partisanship have decreased in Europe in recent decades. \nThe eight-item partisan identification scale: \n\nprovides greater information about partisan intensity than the standard single-item\npossess the same measurement across all three countries \nbetter predicts in-party voting and political participation than ideological intensity measures"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#campbell-angus-philip-e.-converse-warren-e.-miller-and-donald-e.-stokes.-1960.-the-american-voter.-ch-6-7.",
    "href": "notes/Comparative_Behavior/index.html#campbell-angus-philip-e.-converse-warren-e.-miller-and-donald-e.-stokes.-1960.-the-american-voter.-ch-6-7.",
    "title": "Comparative Behavior",
    "section": "Campbell, Angus, Philip E. Converse, Warren E. Miller, and Donald E. Stokes. 1960. The American Voter. Ch 6 & 7.",
    "text": "Campbell, Angus, Philip E. Converse, Warren E. Miller, and Donald E. Stokes. 1960. The American Voter. Ch 6 & 7.\n\nBumper Sticker:\nParty ID influences attitudes and behavior & is sticky!\n\nMostly descriptive statistics. Lots of tables and figures.\n\n\n\nResearch Question:\nWhat are the various impacts of individual party identification?\n\n\nData:\n\nSelf-identified party identification from individuals on repeated cross sections of national population from 1952-1956 and strength and direction of partisan orientation\n\n\n\nGeneral Notes: TK\n\n\nTK TK TK TK"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#keele-luke-and-jennifer-wolak.-2006.-value-conflict-and-volatility-in-party-identification.-british-journal-of-political-science-364-671-690.",
    "href": "notes/Comparative_Behavior/index.html#keele-luke-and-jennifer-wolak.-2006.-value-conflict-and-volatility-in-party-identification.-british-journal-of-political-science-364-671-690.",
    "title": "Comparative Behavior",
    "section": "Keele, Luke, and Jennifer Wolak. 2006. “Value conflict and volatility in party identification.” British Journal of Political Science 36(4): 671-690.",
    "text": "Keele, Luke, and Jennifer Wolak. 2006. “Value conflict and volatility in party identification.” British Journal of Political Science 36(4): 671-690.\n\nBumper Sticker:\n*Value conflict can explain why partisanship assumes both a steady AND changeable nature\n\n\nResearch Question:\nThe paradox of partisanship: Why is partisan identification stable for some people and not stable for others?\n\n\nSummary/Theory/Argument:\nKeele and Wolak seek to explain why some people are more prone to #partisan “instability” (or “volatility”) than others. To the authors, the reason for this has to do with the “hierarchy of values”. When people have obvious values that rank higher than others (humanitarianism, egalitarianism, equality) AND these #values align with political elites values, then partisan identity remains a steady tool with which to measure the political world against. HOWEVER, for some people, the values rank similarly and therefore cause conflicting #preferences that individuals struggle to resolve. Competing values will produce #volatility in partisan opinions specifically when individuals endorse values that cross the fault lines of partisan dialogue \n\n\nData/Methods:\nData: ANES data from 1992, 1994, and 1996\nModel: Estimates several heteroskedastic regression models as well as ordered probit models The heteroskedastic regression models are used because they expect differences in the response variance of party identification survey items to reflect value-conflict \nthis is because citizens with ideologically inconsistent value structures will struggle to identify with one party or another \nthere should be unequal variance across observations (which is heteroskedastic) \nthis heteroskedasticity is modeled to see if those who experience value conflict reveal less predictable responses to party identification survey items than those who do \n\n\nIndependent Variable:\nValue conflict \nis measured using 4 core values in American Politics: \nEgalitarianism, limited government, moral traditionalism, and humanitarianism \nThe measures for these values are derived from a set of ANES survey questions on each value\n1992 and 1996 are the years they were taken from (1992 is missing humanitarianism)\n\n\nDependent Variable:\nPartisan Instability \nTwo “manifestations” \n\npredictability of survey response (those experience value conflict should have less predictable responses to partisan questions) to two survey questions: \n\nseven-point party identification scale and ideological self identification scale \n\nchange in party identification over time (this one is tested using the 1994 and 1996 panel data) \n\n\n\nFindings:\n1992 and 1996 elections reveal value-driven volatility in partisan identification \n\nInstabilities in political views reflect either: \n\nlow information (for some) \ntrade-offs of the political world that cause conflict of values \n\n\nValue conflict will disrupt partisan stability when an individuals value organization is non-ideological - i.e. it doesn’t quite fit the rhetoric or portrayal by the liberal vs. conservative, left-right spectrum. what makes it non-ideological is that you support values on BOTH sides \nThose that have values the match political elites leads to stable partisanship \nThose that have internalized value organization that doesn’t match elites leads to conflicts of values and partisan volatility. \nElites identify and interpret core values to justify their policy positions \nValues and ideology are traditionally distinct concepts - but values can be distributed along an ideological spectrum"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#hatemi-peter-k.-et-al.-2008.-is-there-a-party-in-your-genes-political-research-quarterly-623-584-600.",
    "href": "notes/Comparative_Behavior/index.html#hatemi-peter-k.-et-al.-2008.-is-there-a-party-in-your-genes-political-research-quarterly-623-584-600.",
    "title": "Comparative Behavior",
    "section": "Hatemi, Peter K., et al. 2008. “Is there a ‘Party’ in Your Genes?” Political Research Quarterly 62(3): 584-600.",
    "text": "Hatemi, Peter K., et al. 2008. “Is there a ‘Party’ in Your Genes?” Political Research Quarterly 62(3): 584-600.\n\nBumper Sticker:\nGenes don’t choose party, but they influence intensity!\n\n\nIndependent Variable:\nTwins! MZ and DZ and familial party ID and intensity\n\n\nDependent Variable:\nparty identification and party intensity\n\n\nResearch question:\nDo your genes determine political party identification?\n\n\nHypothesis:\n\nParty identification (PID) is primarily the result of familial socialization and not other latent social or genetic influences.\nPartisan intensity is influenced by genes as well as the environment.\n\n\n\nData:\n\nData collected in mid- to late 1980s as part of Virginia 30,000 Health and Life-Style Survey for Twins (VA30K)\n\n\n\nMethods:\n\nPolychoric correlations by twin pair zygosity calculated for each of the traits.\nCorrelations between PID, partisan intensity, sociodemographic items, selected personality traits, and political attitudes calculated for males & females separately\nUsing structural equation modeling (SEM), variance of phenotypes separated into additive genetic (A), common environmental (C), & unique environmental influences (E)\n\n\n\nFindings:\n\nNOPE – your genes don’t determine your political party\n\nBUT!\n\nGenes play a pivotal role in the strength of your party ID\n\n\n\nGeneral notes:\n\nDirection and Intensity of Political Affiliation\n\nFamily member political affiliations are highly correlated\n\nLargely held as evidence of familial socialization\nResearchers have failed to consider possibility of genetic component\n\nOther social traits, behaviors, and attitudes are genetically influenced\n\nChurch attendance\nIssue positions\nPolitical ideology (isn’t this what we’re already talking about?)\n\nWe need to know both direction and strength of partisan attachments\n\nBehavior Genetics and Biometric Theory\n\nDeveloped in an attempt to understand why individuals in a population differ\n\nAnalyses explain variation around a population mean = info on individual differences in a population\nPhenotype (specific trait value) = combo of genetics + environment\nMonozygotic (MZ) twins often used in behavior genetic pop samples\nIf PID is influenced by genes, co-twin correlation of MZ twins should be higher than that of DZ twin pairs\n\nMaximum likelihood (ML) structural equation modeling (SEM) most commonly used to analyze twin samples (Bayesian also used)\n\nTests validity of theories\n\n\nModel Assumptions and Addressing Critics of Biometric Designs\n\nClassical Twin Design (CTD) assumes no differences in means (or prevalences) and variances of different zygosity groups\nAlso assumes magnitude and correlation of shared environmental influences are the same for MZ and DZ co-twin pairs (“equal environment assumption”)\nCurrent polysci critiques outdated a priori assumptions that don’t test predictions\n“The heuristic that genes influence behavior is unashamedly empirical, because that is the nature of science” (586).\nLimitations exist:\n\nTwin samples are not random\n\n\nDescribing the Sample and the Measurement of Concepts\n\nData collected in mid- to late 1980s as part of Virginia 30,000 Health and Life-Style Survey for Twins (VA30K)\nPID assessed by survey question: “Write in the number which best describes [your] political affiliation: (1) don’t know (2) always supports Republicans (3) usually supports Republicans (4) varies (5) usually supports Democrats (6) always supports Democrats (7) other (8) prefer not to answer.”\n\n2 or 3 = Republican, 5 or 6 = Democrat, 4 = varies\nPartisan intensity determined by usually and always\n\nSociodemographic variables: age, income, education, religion, occupation, marital status, church attendance\nPolitical attitudes assessed using 28-item version of Wilson-Patterson attitudes inventory\n\nSelection of covariates for partisan intensity include measures from Eysenck’s Personality Quotient (EPQ) with 3 main personality factors:\n\nPsychoticism (versus impulse control)\nExtraversion (versus introversion)\nNeuroticism (versus instability)\n\nTwo additional subfactors – impulsivity and social conformity – also in sample and included in analysis\n\n\n\n\n\nDefinitions:\n\nMonozygotic (MZ) twins = twins developed from a single fertilized ovum that are genetically identical\nDizygotic (DZ) twins = twins from 2 different fertilized ova by different sperm; on average only sharing 50% of segregating genes (like non-twin siblings)\nAdditive genetic (A) = combined influence of all genes\nCommon environment (C) = common or shared among family members, including familial and cultural socialization\nUnique environment (E) = idiosyncratic (unique) personal experience and all environmental stimuli unique to the individual\nA priori = knowledge independent from experience\nCollapsed variable = combining several cases into single lines\nPolychoric correlation = A technique for estimating the correlation between two hypothesized normally distributed continuous latent (indirectly measured) variables, from two observed ordinal variables\nDiscriminant function analyses = serve the same purpose as beta weights in linear regression and indicate the relative importance of the covariate in predicting the dependent variable"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#bratten-michael-and-robert-mattes.-2001.-support-for-democracy-in-africa-intrinsic-or-instrumental-british-journal-of-political-science-313-447-474.",
    "href": "notes/Comparative_Behavior/index.html#bratten-michael-and-robert-mattes.-2001.-support-for-democracy-in-africa-intrinsic-or-instrumental-british-journal-of-political-science-313-447-474.",
    "title": "Comparative Behavior",
    "section": "Bratten, Michael, and Robert Mattes. 2001. “Support for Democracy in Africa: Intrinsic or Instrumental?” British Journal of Political Science 31(3): 447-474.",
    "text": "Bratten, Michael, and Robert Mattes. 2001. “Support for Democracy in Africa: Intrinsic or Instrumental?” British Journal of Political Science 31(3): 447-474.\n\nBumper Sticker:\nAfrican democracy persists b/c of economic AND political change.\n\n\nIndependent Variable:\neconomic, political, and general performance factors on democratic regime\n\n\nDependent Variable:\nattitudes about democracy\n\n\nResearch Question:\nWhy do Africans support democracy?\n\n\nHypothesis:\n\nExplanatory factors: why variation in support for/satisfaction with democracy?\n\nSocial characteristics of population such as literacy, income and gender\nEconomic goods: popular perceptions of national economic conditions, personal quality of life, and access to materials and services shape feelings about democracy\nPolitical goods: Is the delivery of civil rights and political equality enough?\nGeneral performance factors: citizens’ overall assessment of governmental performance for 3 reasons:\n\n\n\n\nData:\n\n3 separate surveys on political attitudes in Ghana, Zambia, and South Africa\n\nZambia: 1,182 respondents following “dubious” 1996 election\nSouth Africa: 3,500 stratified by race, province, community size in 1997 following free & fair elections\nGhana: 2,005 respondents in 1999 following free & fair 1996 elections\n\nAll 3 surveys included questions on citizen understanding of the meaning of democracy, support for, and satisfaction with democracy in theory and n practice\n\n\n\n\n\nMethods:\nMultiple regression in OLS\n\n\nFindings:\nPopular support for democracy in African countries similar to other Third Wave countries\n\nLower levels of mass satisfaction w/regime performance in African countries\nGeneral public in African countries thinks instrumentally: support for democracy hinges critically upon popular approval of government achievements\nOutside of South Africa, many Africans value political goods\n\nAdditional findings:\n\nAfricans [here] more likely to associate democracy w/individual liberties than w/communal solidarity, especially if they live in urban areas.\nPopular conceptions of democracy have both procedural and substantive dimensions (though the former is more common than the latter).\nCitizens rank procedural and substantive attributes in different order across countries.\nRankings differ even within the category of political goods"
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#benton-a.-l.-2005.-dissatisfied-democrats-or-retrospective-voters-384-417442.",
    "href": "notes/Comparative_Behavior/index.html#benton-a.-l.-2005.-dissatisfied-democrats-or-retrospective-voters-384-417442.",
    "title": "Comparative Behavior",
    "section": "Benton, A. L. (2005). “Dissatisfied Democrats or retrospective voters?”, 38(4), 417–442.",
    "text": "Benton, A. L. (2005). “Dissatisfied Democrats or retrospective voters?”, 38(4), 417–442.\n\nAbstract:\nThis article examines recent trends in #Latin-American #voting-behavior and casts them in terms of sincere (economic) and strategic (electoral) concerns. It argues that thanks to years of economic adversity, Latin Americans have developed long, sophisticated #economic-memories . Although this has resulted in rising frustration with democratic government, according to recent opinion polls, it has not always led voters to #punish all parties responsible for hardship at election time. A panel study of the region’s presidential systems demonstrates that citizens punish incumbents by voting for established #nonincumbents when electoral laws reduce opportunities available to small parties in the systems, even if #nonincumbents have also been blamed for hard economic times. More #permissive-electoral-systems , in contrast, encourage citizens to reject all parties responsible for economic decline. The analysis demonstrates how economic and electoral concerns interact to affect voting behavior, #political-accountability , and #public-opinion in Latin America.\n\n\nBumper Sticker:\nEconomic Evaluations Voter Laws Voter Behavior\n\n\nResearch Question:\nIf most Latin Americans have faced economic hardship during successive governments, why do they reject both the incumbent and non-incumbent parties in some systems and only the incumbent party in others?\n\n\nSummary/Theory/Argument:\nMany Latin American economies have experienced some form poor performance that has resulted in different forms of electoral punishment. Barton, drawing on previous literature regarding economic theories of voter behavior, devises a theory that enables her to assess the institutional features that interact with economic performance to determine who voters choose to punish in subsequent elections. In highly permissible systems, where there are more parties to choose from, voters memories of economic performance can interact with electoral strategies to allow the punishment of multiple parties (as has been the case in multiple countries). However, more restrictive systems, which limit the number of viable parties, forces voters to focus their punishing efforts on the incumbent alone, as they require an alternative option of some form and therefore increases support for the form incumbent.\n\n\nData/Methods:\n\npanel data from 13 Latin American Countries\n\nrequired (a) to be a presidential system and have had (b) more than one party win the presidency, so that current and former incumbents compete; (c) to have elections deemed free of fraud by international observers, so that their results reflect voters’ reactions to party performance; and (d) to have an economic crisis and/or economic reform during democratic rule.\nin these countries, 64 elections were held, with 39 instances in which current and former incumbents competed for power\n\n\n\n\nIndependent Variable:\neconomic performance as measured by the percentage change in GDP per capita during each parties last 2 years in office.\n\n\nDependent Variable:\n\nChange in support for incumbents - measured as the difference between votes received by an incumbent party in presidential elections while in office and votes received in the previous election when the party won.\nChange in support for non-incumbents out of power - measured as change in support received in elections after leaving office and support when the party originally came into power.\n\n\n\nModel:\n\nUses OLS with #panel-corrected standard errors #PCSEs , which were selected because of the limited number of countries and small (often unbalanced) number of observations in each panel.\n\n\n\nHypothesis:\n\nH1: When incumbent parties are blamed for poor economic performance, they will lose support, regardless of the electoral system used to elect them.\nH2: Nonincumbent parties previously in power will derive greater electoral benefits in restrictive, compared with permissive, electoral contexts when incumbent parties are blamed for economic decline, all else being equal.\nH3: Former incumbents blamed for economic decline will continue to receive lower levels of support in permissive, as opposed to restrictive, electoral contexts, all else being equal.\nH4: Current incumbents will gain support in permissive, as opposed to restrictive, electoral contexts when former incumbents are blamed for economic decline, all else being equal.\n\n\n\nFindings:\n\n\n\n\n\n\n\n\nHypothesis\nSupport\nDetails\n\n\n\n\nH1\nYes\nincumbent parties’ support decreases with economic downturns, without regard to institutional change (p.430)\n\n\nH2\nYes\nwhen electoral laws construct institutional barriers to small parties (restrictive), groups once holding the presidency will suffer losses in support when incumbents manage economic growth; as GDP declines, parties previously in power begin to experience a surge of supprot (p.431)\n\n\nH3\nYes\naccounting for the presence of Honduras, there was no relationship between poor economic performance and punishment of former incumbents in restrictive systems; there was a relationship between poor economic performance and the punishment of incumbents in permissive systems (pg.436)\n\n\nH4\nYes\nnonincumbents’ performance has implications for incumbents’ support. As predicted, incumbents gain about 10% of the national votes when nonincumbents are responsible for neoliberal economic reform in all electoral settings\n\n\n\n\n\nOther Findings:\n\nRestrictive institutions reduce the incentive for voters to waste support on parties unlikely to win.\nRather than helping nonincumbents weather the electoral effect of incumbents’ economic growth, runoffs cause nonincumbents to suffer additional electoral losses."
  },
  {
    "objectID": "notes/Comparative_Behavior/index.html#lewis-beck-michael-s.-and-mary-stegmaier.-2009.-economic-models-of-voting.-ohpb.-ch.-27-518-537.",
    "href": "notes/Comparative_Behavior/index.html#lewis-beck-michael-s.-and-mary-stegmaier.-2009.-economic-models-of-voting.-ohpb.-ch.-27-518-537.",
    "title": "Comparative Behavior",
    "section": "Lewis-Beck, Michael S., and Mary Stegmaier. 2009. “Economic Models of Voting.” OHPB. Ch. 27: 518-537.",
    "text": "Lewis-Beck, Michael S., and Mary Stegmaier. 2009. “Economic Models of Voting.” OHPB. Ch. 27: 518-537.\n\nBumper Sticker:\nThe state of the economy can help predict voter decisions.\n\n\nPurpose:\nThis is a literature review on micro-level, survey research studies of economic voting in the U.S., Britain, France, and Globally.\n\n\nOverview:\nQuestions regarding the precise nature of economic voting led to four general “types”.\n\nRetrospective Voting\n\nOriginating with V.O. Key (1966), this type of voting occurs when voters review the performance of the incumbent government.\n\nProspective Voting\n\nOriginating with Downs (1957), this type of voting occurs when voters look to the future and vote according to the governments potential economic performance.\n\nPocketbook Voting (personal finances)\n\nThis theory of voting suggests that when personal or household financial conditions have deteriorated, voters will punish the incumbent.\n\nSociotropic Voting (national economy rather than personal finances).\n\nThis theory of voting suggests that voters are more likely to be considering the national economic situation when casting their vote.\n\n\nOf these types, the retrospective sociotropic and prospective sociotropic appear to be the most relevant across empirical evaluations. That is, people tend to vote based of their perceptions of national economic performance, both in the past (retrospective sociotropic) and the future (prospective sociotropic). The type of voting that is cued depends on whether or not an incumbent is running, and therefore a reliable place to point blame. Essentially, the reward-punishment hypothesis holds up across settings, with slight variation according to group membership (party id, gender, etc.), institutional context (voting system, prime-minister v. president, number of parties) and economic context (personal, neighborhood, national).\n\n\nFindings:\n\nUnited States:\n\nANES is good source for election studies\nMost theories of economic voting focus on the U.S.\nPocketbook voting is weak in presidential elections -- sociotropic is strong\nStrength of economic voting can cary based on context -- such is the case when considering the attribution of responsibility (e.g. the attribution of blame to particular party/candidate/branch of government)\n\nin the U.S. the presidency is an important institutional context for which to blame punish electorally\n\nWhile other types of factors matter and interact them, sociotropic economic effects (both retrospective and prospective) themselves cannot be understated when assessing the general behavior of the voter population.\n\n\n\nBritain:\n\nBES is a good source for election studies\nBritish voters show the same trends of reward and punishment regarding economic voting\nThere may be important regional contexts\n\nJohnston 1997 finds that when voter’s perceptions of their neighborhoods are that they are worse off, they will punish the incumbent\n\nRetrospective and Prospective are both important\nPerceptual shifts in economic conditions are almost as great as realigning with a different party (Clarke et al. 2004)\n\n\n\nFrance:\n\nEurobarometer and FNES are good sources for election studies\nA general finding—significant retrospective sociotropic effects, but no retrospective pocketbook effects\nIt is largely sociotropic and more or less equally retrospective or prospective, depending in part on the institutional context.\nThe French voter is sophisticated, knowing whom and how much to blame.\n\nWhen government is unified, the president is the lightning rod for economic discontent. However, under cohabitation, the burden shifts to the prime minister.\n\nSmaller parties are blamed less than large parties, legislative candidates are blamed less than presidential candidates, and presidential candidates who are prime minister are blamed more than presidential candidates who are not prime minister.\n\n\n\nCross-National Studies:\n\nSimilar trends have been found in cross-national studies\n\nLewis‐Beck and Mitchell (1990), on the five major western European countries; Chappell and Veiga (2000), on thirteen western European nations; Pacek (1994) and Tucker (2001) on central European samples; Remmer (1991) on twelve Latin American countries; Pacek and Radcli (1995) on eight low‐income nations; Wilkin, Haller, and Norporth (1997) on a worldwide sample of countries.\n\nImportant Takeaways:\n\nResponsibility Hypothesis seems to hold - that when responsibility is hard to pin down, voters are less likely to punish incumbents - this is an important contextual factors\n\nSuch is the case in more countries with more “coalitional complexity” -- when more parties make up the incumbent government, is it difficult to assign blame\nPresidential Systems and systems with more restrictive electoral laws tend to strengthen the economic vote\nIn the context of globalization, there is a significant decrease in economic voting in the face of open trade, as electorates can’t assign blame to the government alone"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html",
    "href": "notes/ICPSR_Intro_to_Python/index.html",
    "title": "ICPSR - Introduction to Python",
    "section": "",
    "text": "Introduction to Python with Professor Sarah Hunter was a workshop taken at ICPSR in the summer of 2024. You cannot memorize this. Practice makes perfect. You must use it or you will lose it. This course covers the absolute basics."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#getting-to-know-python",
    "href": "notes/ICPSR_Intro_to_Python/index.html#getting-to-know-python",
    "title": "ICPSR - Introduction to Python",
    "section": "Getting to know Python",
    "text": "Getting to know Python\n\nPython is more flexible and general than R.\nObject oriented\n\nR is also object oriented.\n\nR is very similar to Python.\n\nbasic syntax is similar.\n\nWhy learn python?\n\nits the most popular programming language\nPython is used a lot by data analyst\n\ntext analysis, machine learning, and AI are big on Python.\n\nWeb scraping very big on Python\nAccessing APIs with Python good as well.\n\nAPIs are just a way to access data easily.\n\n\nGeneral-purpose language. Not a statistical language\nFree and open source\nUser created packages\nSteep learning curve\ndifferent paths to the “correct” answer\nNat Silver apparently does everything in Stata (gross)"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#basic-syntax",
    "href": "notes/ICPSR_Intro_to_Python/index.html#basic-syntax",
    "title": "ICPSR - Introduction to Python",
    "section": "Basic Syntax",
    "text": "Basic Syntax\ncommand(object)\nex: print(“Hello World”)\nThere are different types of objects.\n\nObjects are assigned with =\n\nin R it is &lt;-\n\nObjects are defined by type:\n\nscalar (cannot be subdivided)\n\nint - integer, e.g. 3 or 142\nfloat - real numbers, e.g. 4.2 or -3.5\nbool - Boolean, known as logical in some languages, True or False\nNoneType - a special type with on possible value, none. Basically means NULL.\n\nnon-scalar (has an internal structure that can be assessed)\n\nvector, data frame, list.\n\nCan find type in Python with type()\n\nin R it is class()\n\n\nWe can convert different types to other types\n\njust be careful!\n\n\nExpressions = objects + operators\n\nakin to taking words and making sentences\n\nWhat are operators?\n\nAdditions +\nSubtraction -\nMultiplication *\nDivision /\nModulus %\nExponentiation **\nFloor division // - rare\n\nPython knows order of operations\nExample code chunk:\n\n# this is an example of assigning an object and adding operators to the objects. \npi = 3.14159\nradius = 5 \narea = pi * radius**2 # pi is an object * another object (radius) raised to the second power\nprint(area)\n\n78.53975\n\n\nWe can “rebind”. This assigns a new value to the object of the same name. The object is getting a new value. So be careful when you rebind!\nnote: need to use the print command. In R you could just type the object and it would print. This is not the same in Python.\n\n# Rebinding example \npi = 3.14\nradius = 10 \ncircumference = 2 * pi * radius \nprint(circumference)\n\n62.800000000000004\n\n\nEND DAY 1!"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#exercise-describe-how-to-make-coffee-in-as-much-detail-as-possible",
    "href": "notes/ICPSR_Intro_to_Python/index.html#exercise-describe-how-to-make-coffee-in-as-much-detail-as-possible",
    "title": "ICPSR - Introduction to Python",
    "section": "Exercise: Describe how to make coffee in as much detail as possible",
    "text": "Exercise: Describe how to make coffee in as much detail as possible\nTo make coffee I first grab a Keurig coffee cup and put it in my Keurig . Once starting the Keurig , I wait for it to heat up and it begins filling the cup. After, I put my sugar free vanilla creamer inside the the mug. I then mix it up and it is ready to be served.\n\nWhat is the point of this?\n\nThink about how many different tasks you have to go through.\n\nLearning Python is similar. It is a series of small tasks.\n\nwe need to spell out each of those tasks that we do without thinking.\n\nthey need to be in our code.\n\nAny big tasks we want Python to do, we need to have it do all those small tasks to make it do that one large task…this is called control flow."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#strings",
    "href": "notes/ICPSR_Intro_to_Python/index.html#strings",
    "title": "ICPSR - Introduction to Python",
    "section": "Strings",
    "text": "Strings\n\nStrings must be in quotes.\ndefined: text, letter, character, space, digits, etc.\nUse triple quotes for multiple lines of strings.\n\n\ngreeting = \"Hello! How are you\"\nwho = 'Anastasia' \nprint(greeting)\nprint(greeting + who + '?') # this is concatenating. notice the output. \nprint(greeting + \" \" + who + '?') # this is one way to fix the spacing issue\n# we could also add a space to the object. \n\nHello! How are you\nHello! How are youAnastasia?\nHello! How are you Anastasia?\n\n\nLet’s try an example of a multi line string:\n\nmy_string = '''\nThis is a string. It is \nspanning multiple lines. \n''' \nprint(my_string)\n\n\nThis is a string. It is \nspanning multiple lines. \n\n\n\nWe can combine strings with integers.\n\nn_apples = 3 \nprint(\"I ate\", n_apples, \"apples.\") # this is NOT a concatination. The n_apples is still an integer. \n\nprint(\"I ate\", str( n_apples), \"apples.\") # this is a concatination. We convert the int to a string. \n\n# now try to assign a new object\nsentence=(\"I ate \", n_apples, \"apples.\") \nprint(sentence)\ntype(sentence) # notice the type is not a string. We will discuss tuples later.   \n\nI ate 3 apples.\nI ate 3 apples.\n('I ate ', 3, 'apples.')\n\n\ntuple\n\n\n\nInput\nAllows a user to input a response.\nExample:\n\n# note this code won't run on this website. But you can copy it somewhere else and it will execute. \ntext = input(\"Tell me something…\")\nprint(\"So you are saying\", text) \n\nWe can go further:\n\n# this will give you your age. Pretty fun. Note this code will not run on this website. \nbirth_yr = input(\"Type in your birth year:\")\nprint('You are ' + str(2024 - int(birth_yr)) + ' years old.') # if we were to assign this to an object, we would return a string because the middle input is wrapped in a str() function. So it will convert our input which is originally an integer, to a string."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#boolean",
    "href": "notes/ICPSR_Intro_to_Python/index.html#boolean",
    "title": "ICPSR - Introduction to Python",
    "section": "Boolean",
    "text": "Boolean\n\nUsed to compare to variables to one another\nUsed for binary outcomes. True or False?\n\nvar1 &lt; var2\nvar1 &gt;= var2\nvar1 &gt; var2\nvar1 &lt;= var2\nvar1 == var2\nvar1 != var2\n\nThese will help once we start talking about control flow of a model.\nLogical operators on Booleans\n\nnot, or , and are special words for logical operators\nnot a\na or b\na and b\n\nExamples:\n\nhours = 20 \n# more than a day   \nprint(hours&gt;24) # this will return FALSE. The boolean operator is \"&gt;\"\n\nFalse\n\n\n\n\nfrom pickle import TRUE # this is just a package. the prof originally wrote TRUE but thats for R. Python likes True.\n# how did you commute? \nbike=True\nbus=False\nprint(bike or bus)\nprint(bike and bus)\n\nTrue\nFalse\n\n\n\nControl Flow: Branching\nExample: (four) spaces\nif &lt;condition&gt;:\n&lt;expression&gt;\n&lt;expression&gt;\n\nSpaces/ white space matters in python!\nthe expressions should be (by convention) be indented by 4 spaces or a Tab\nthat’s how Python understands that those are the expression to be run if the condition is True\nonce indented is removed, it’ll be back to evaluating everything.\n\n\nif &lt;condition&gt;:\n  &lt;expression1&gt; # evaluate expression1 if condition is True, otherwise evaluate expression2. \nelse: \n  &lt;expression&gt; # notice all the white space. This matters in Python! \n\nLet’s use the modulus boolean as an example:\n\nnumber=12 # change this number and see how the output changes! \nif number % 2 == 0: # if the number after being divided by 2 has a remainder of zero then it is even. \n  print(\"Number is even.\")\nelse: \n  print(\"Number is odd.\")\n\nNumber is even.\n\n\n\n\nif statements\nLonger example of control flow:\n\nelif is short for else if\nif condition 1 is true, evaluate expression 1\nif condition 1 is not true but condition 2 is true, evaluate expression 2.\nLast expression is evaluated only if all the other conditions are False.\nBasically Python hits the first condition that returns as True.\n\n\nif &lt;condition1&gt;:\n  &lt;expression1&gt; \nelif &lt;condition2&gt;: \n  &lt;expression2&gt; \nelif &lt;condition3&gt;: \n  &lt;expression3&gt; \nelse: \n  &lt;expression4&gt;\n# Basically start from the top. If not this condition then move to next one until condition is met. you can have as many elif \n\nFurther example:\n\nnumber=0 # change this number and notice how the output changes. \nif number &gt; 0: \n  print(\"positive number\")\nelif number == 0: \n  print(\"Zero\")\nelse: \n  print(\"Negative number\")\n  \nprint(\"This statement is always executed\") # notice the white space \n\nZero\nThis statement is always executed\n\n\nBeware the Nested Statements!\n\nhow do you know which else belongs to which if?\n\nAnswer: Indention\n\n\nnumber=72 # change this number to see how the output changes! \nif number % 2 == 0:\n  print(\"Number is even.\")\n  if number % 3 == 0: \n    print(\"Number is divisible by 6.\")\n  else: \n    print(\"Number is not divisable by 6.\")\nelse: \n  print(\"Number is odd.\")\n\nNumber is even.\nNumber is divisible by 6.\n\n\n\n\n\nwhile statements\n\nKeeps running as long as condition is True\n\nExamples:\n\n# program to display numbers from 1 to 5\n# intialize the variable \ni=1\nn=5\n# while loop from i = 1 to 5 \nwhile i &lt;= n: \n  print(i)\n  i=i+1 # see what happens when you take this part of the function out. (its not good)\n\n1\n2\n3\n4\n5\n\n\n\nnumber=700\n# this function below keeps adding 1 until the number is divisible by 13.\nwhile not number %13==0: #notice the not function\n  print(number, \"is not divisible by 13.\")\n  number=number+1 \n\nprint(number, \"is divisible by 13.\")\n\n700 is not divisible by 13.\n701 is not divisible by 13.\n702 is divisible by 13.\n\n\n\n\nfor statements\n\nuseful for when number of iterations are known\nIts function can be achieved by a while loop, but for loop is easier\nevery time through the loop, &lt;variable&gt; assumes a new value (iterating through &lt;iterable&gt;)\n\n\nfor &lt;variable&gt; in &lt;iterable&gt;: \n  &lt;expression&gt;\n  &lt;expression&gt;\n\n\niterable is usually range (&lt;some_num&gt;)\ncan also be a list\nrange(start, stop, step)\nstart =0 and step = 1\nonly stop is required\nit will start at 0, loop until stop-1.\nPython starts counting at ZERO NOT at one!\n\n\nfor i in range(5):\n  print(i)\n\n0\n1\n2\n3\n4\n\n\n\nfor i in range(11, 15):\n  print(i)\n\n11\n12\n13\n14\n\n\n\nfor i in range(10, 30, 5):\n  print(i)\n\n10\n15\n20\n25\n\n\n\nfor char in 'MICHIGAN':\n  print(char+ \"!\") # this iterates through strings. \n  # we use i for integers usually, so we are using char to denote string. \n\nM!\nI!\nC!\nH!\nI!\nG!\nA!\nN!\n\n\n\nfor i in range(10, 30, 5):\n  print(i%10)\n\n0\n5\n0\n5\n\n\n\n\nBreak statements\n\nexits the loop it is in\nremaining expressions are not evaluated\nin nested loops, only innermost loops exited\n\n\nfor i in range(1,4):\n  for j in range(1,4):\n    if i==2 and j==2: \n      break\n    print(i,j)\n\n1 1\n1 2\n1 3\n2 1\n3 1\n3 2\n3 3\n\n\n\ncontinue statement is similar but continues the loop over the specific iteration.\n\n\nvar=7\nwhile var&gt;0:\n  var-=1\n  if var==5: #this will skip 5 \n    continue\n  if var==2: # this will terminate the loop at 2\n    break\n  print(\"current variable value\", var)\n  \nprint(\"goodbye!\")\n\ncurrent variable value 6\ncurrent variable value 4\ncurrent variable value 3\ngoodbye!"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#lists",
    "href": "notes/ICPSR_Intro_to_Python/index.html#lists",
    "title": "ICPSR - Introduction to Python",
    "section": "Lists",
    "text": "Lists\n\nlists are on of four built-in data types to store collections of data\nthe other are tuples, dictionaries, and sets.\nused to store items in a single variable.\n\n\nmy_list=[\"apple\", \"orange\", \"banana\", \"cherry\"]\ntype(my_list)\nmy_list[2]\n\n'banana'\n\n\n\nlarge lists require more computer power.\nlists always start with a square bracket\n\nparenthesis create a tuple.\n\nitems in a list don’t need to be of the same type.\n\n\n# quarot doesn't like empty lists for some reason. So this code won't run on here.\nmisc_list=[\"apple\", 3, False, None]\nempty_list[ ]\nprint(misc_list)\nprint(empty_list)\n\n\nlists are ORDERED\nlists contain the same elements."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#methods",
    "href": "notes/ICPSR_Intro_to_Python/index.html#methods",
    "title": "ICPSR - Introduction to Python",
    "section": "Methods",
    "text": "Methods\n\nin Python, “methods” are functions that belong to an object\nthey only work with that object\nSome list methods include:\n\nappend - adds element to end.\ninsert - adds an element at the specified position\nreverse - reverses the order of the list\nsort - sorts the list - object type determines method of sort.\nindex - returns the index of the first element with the specified value\nsorted\nextend - adds the elements of a list (or any interable), to the end of the current list\n+ add lists together without modifying original lists.\ndel - remove an element from a list.\n\n\n\ncars=[\"Ford\", \"BMW\"]\ncars.append('Mazda')\nprint(cars)\n\n['Ford', 'BMW', 'Mazda']\n\n\n\nnote that no re-assignment is necessary\nonce append() is run, the list is modified in memory.\navoid “.” (dots) in the naming of objects because they have usage in python.\n\nEND DAY 2"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#review",
    "href": "notes/ICPSR_Intro_to_Python/index.html#review",
    "title": "ICPSR - Introduction to Python",
    "section": "Review",
    "text": "Review\nWrite a script that checks whether a number is even.\n\nnumber = int(input(\"choose any number \")) # we wrap in int() b/c input returns a string.\nif number % 2 == 0: \n  print(number, \"is even.\")\nelse:\n  print(number, \"is odd.\")\n\nprint(\"Goodbye!\")"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#slicing",
    "href": "notes/ICPSR_Intro_to_Python/index.html#slicing",
    "title": "ICPSR - Introduction to Python",
    "section": "Slicing",
    "text": "Slicing\nLists can be sliced with the following syntax:\n\n[start:stop:step]\n\nstart at start (default is zero)\nstop one step before stop (default is length of list)\nstep specifies how many indices to jump.\n\nnumbers = [1,2,3,4,5,6,7,8]\nnumbers[:3] # count and stop at 2 \n# or \nnumbers [::2] # move in steps of 2 \n\n[1, 3, 5, 7]"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#tuples",
    "href": "notes/ICPSR_Intro_to_Python/index.html#tuples",
    "title": "ICPSR - Introduction to Python",
    "section": "Tuples",
    "text": "Tuples\n\nordered sequence of items\na type of object.\nunlike lists, tuples are immutable\n\nimmutable means the values cannot be changed after it has been created.\n\nThey are typically created with parenthesis ()\nExample:\n\ntpl = ('a',5,True)\nprint(tpl)\n\n('a', 5, True)\n\n\n\n\nWhy use tuples?\n\nused to conveniently swap variable values\nused to return more than one value from a function, since it conveniently packages many values of different type into one object.\nnot super common TBH. Probably won’t use much. But they are just something to be aware of.\nTuples have two methods\n\ncount()\nindex()\n\n\ntpl=('a','b','a')\nprint(tpl.count('a'))\nprint(tpl.index('b'))\n\n2\n1"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#sets",
    "href": "notes/ICPSR_Intro_to_Python/index.html#sets",
    "title": "ICPSR - Introduction to Python",
    "section": "Sets",
    "text": "Sets\n\nSets do not order items\nsets store unique elements - no duplicates\nuses hashing to efficiently store and retrieve\ngreat for quick lookup (does not take much time/RAM)\nsets created with curly {} braces\n\nmy_set={15,'a',4,'k'}\nmy_empty_set=set() # creates an empty set\n\n\nAdditional Set example:\n\nflight_banned = {\"Jane\", \"Josh\", \"John\", \"Jess\"}\n\"John\" in flight_banned\n\nTrue\n\n\nDifference between sets and Lists:\n\nSets:\n\nwill only check the memory location where item could be\n\nLists:\n\nit will check all of the lists one by one, till the end if necessary."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#strings-1",
    "href": "notes/ICPSR_Intro_to_Python/index.html#strings-1",
    "title": "ICPSR - Introduction to Python",
    "section": "Strings",
    "text": "Strings\n\nDefined: text, letter, character, space, digits, etc.\ncreate. with single or double quotes (needs to be consistent use)\nstrings can also be created with triple quotes.\n\nthese handle multi-line strings.\n\n\n\nString Methods\n\nstartswith()\nendswith()\ncapitalize() capitalizes the first character\ntitle() capitalizes the first character in every word\nupper() capitalizes everything\nlower() converts string to all lowercase.\n\n\nexample_string = \"the New York Times\"\nexample_string.upper() # can also wrap this in a print() function. \n\n'THE NEW YORK TIMES'"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#dictionaries",
    "href": "notes/ICPSR_Intro_to_Python/index.html#dictionaries",
    "title": "ICPSR - Introduction to Python",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nDictionaries are objects in Python that contain both key and value pairs:\n\nsalary = {\"Jane\":100, \"Jess\":150, \"Janet\": 200}\nsalary[\"Jane\"] #notice the value returned.  \n\n100\n\n\nValues\n\nany type (mutable and immutable)\ncan be duplicates\ncan be lists, other dictionaries, any type\n\nkeys\n\nmust be unique\nmust be immutable type (int, float, string, tuple, bool)\n\nno order to keys (and thus values), just like there is no order in a set.\n[key:value, key:value, key:value…]\n\n\nDictionary methods\n\n.index\n.keys\n.values\n\n\nsalary = {\"Jane\":100, \"Jess\":150, \"Janet\": 200}\nsalary[\"Jane\"] #find Jane's salary \nsalary[\"Jess\"] = 175 # change Jess' salary \nsalary[\"Allison\"] = 130 #adding allison to dictionary \n\n\nIterating over a dictionary\n\ngrades = {\"Ali\" : \"A+\", \"Bella\" : \"A+\", \"Rose\" : \"A\", \"Sam\" : \"B+\"}\nfor person in grades:\n  print(person + \"'s grade is \" + grades[person]+\".\")\n\nAli's grade is A+.\nBella's grade is A+.\nRose's grade is A.\nSam's grade is B+."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#functions",
    "href": "notes/ICPSR_Intro_to_Python/index.html#functions",
    "title": "ICPSR - Introduction to Python",
    "section": "Functions",
    "text": "Functions\n\nreusable pieces of code\nfunctions are not run until they are called/invoked somewhere.\nfunction characteristics:\n\nhas a name\nhas parameters\nhas a docstring (optional but recommended)\n\nhelp file for your function. Tells you what the function does basically.\n\nhas a body\nreturns something\n\nSaving bits of code to be used later.\n“def” is the keyword used to define the function\nname of function comes after “def”\nthen, in (), comes the parameters/arguments\n\ndef is_even(i): # is_even is name of function. i is what we input for the function to evaluate. \n  \"\"\"\n  Input: i is a positive integer\n  Returns True if i is even, otherwise False\n  \"\"\"\n  return i % 2 == 0 \nis_even(5) # we are saying use the function is_even, which checks to see if we have a remainder after dividing by 2. If we do not, then it is even. \n# returns a boolean (False or True) based on the input. \nis_even(4)\n\nTrue\n\n\nthe docstring, enclosed in “““, provides info on how to use the function to the end user.\nthe docstring can be called with help()\nBe cautious of the variable scope issue.\n\n\nReturns in Functions\n\nreturns can only be used inside a function\nthere can be multiple returns in a function\nonly of them will be used each time function is invoked\nonce return is hit, function’s scope is exited and nothing else in the function is run\n\n\ndef check_number(number):\n  if number &gt; 0:\n    return \"positive\"\n  elif number &lt; 0: \n    return \"negative\"\n  else: \n    return \"zero\"\n  \ncheck_number(5)\ncheck_number(0)\ncheck_number(-3)\n\n'negative'\n\n\n\n\nTest my knowledge:\nWrite a function that tests if number is divisible by 6:\n\ndef divisible_check(x):\n  if x % 6 == 0: \n    return \"this number is divisble by 6\"\n  elif x % 6 != 0: \n    return \"this number is not divisible by 6\"\n  else:\n    return \"undefined\"  \n\ndivisible_check(108) # change the number in the parenthesis to test the output. \n\n'this number is divisble by 6'\n\n\nWrite a function that creates a dictionary within the function. This function will take a sentence, assign each word as a key, and the value will correspond with the number of times that word appears in sentence.\n\ndef word_freq(sentence):\n  words_list=sentence.split()\n  freq={}\n  for word in words_list:\n    if word in freq:\n      freq[word] += 1\n    else:\n      freq[word] = 1\n  return freq\n\nquote = '''Let me tell you the story when the level 600 school gyatt walked \npassed me, I was in class drinking my grimace rizz shake from ohio during my \nrizzonomics class when all of the sudden this crazy ohio bing chilling gyatt got \nsturdy, past my class. I was watching kai cenat hit the griddy on twitch. \nThis is when I let my rizz take over and I became the rizzard of oz. I screamed, \nlook at this bomboclat gyatt'''\nword_freq(quote)\n\n{'Let': 1,\n 'me': 1,\n 'tell': 1,\n 'you': 1,\n 'the': 5,\n 'story': 1,\n 'when': 3,\n 'level': 1,\n '600': 1,\n 'school': 1,\n 'gyatt': 3,\n 'walked': 1,\n 'passed': 1,\n 'me,': 1,\n 'I': 5,\n 'was': 2,\n 'in': 1,\n 'class': 2,\n 'drinking': 1,\n 'my': 4,\n 'grimace': 1,\n 'rizz': 2,\n 'shake': 1,\n 'from': 1,\n 'ohio': 2,\n 'during': 1,\n 'rizzonomics': 1,\n 'all': 1,\n 'of': 2,\n 'sudden': 1,\n 'this': 2,\n 'crazy': 1,\n 'bing': 1,\n 'chilling': 1,\n 'got': 1,\n 'sturdy,': 1,\n 'past': 1,\n 'class.': 1,\n 'watching': 1,\n 'kai': 1,\n 'cenat': 1,\n 'hit': 1,\n 'griddy': 1,\n 'on': 1,\n 'twitch.': 1,\n 'This': 1,\n 'is': 1,\n 'let': 1,\n 'take': 1,\n 'over': 1,\n 'and': 1,\n 'became': 1,\n 'rizzard': 1,\n 'oz.': 1,\n 'screamed,': 1,\n 'look': 1,\n 'at': 1,\n 'bomboclat': 1}\n\n\n\nWhy do we use the lm() command in R?\n\nwhy not just use the formula (X’X)^-1 X’y?\n\nthe lm command is a function.\n\nits easier to use as it executes the formula."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#modules",
    "href": "notes/ICPSR_Intro_to_Python/index.html#modules",
    "title": "ICPSR - Introduction to Python",
    "section": "Modules",
    "text": "Modules\n\npython modules are files (.py) that (mainly) contain function definitions\nthey allow us to organize, distribute code; to share and reuse others’ code.\nkeep code coherent and self-contained.\none can import modules or some functions from modules.\n\n\nexample:\ninstead of below\n\ndef add(a,b):\n  return a+b\n\nwe could create a module that contains this function:\n\n# use math_operations.py\n# note this code did not work. Skip for now\nimport math_operations \nmat_operations.add(3,5)\n\nTry this example instead:\n\nfrom datetime import date\ntoday = date.today()\nprint(\"Today's date:\", today)\n\nToday's date: 2024-08-16\n\n\nWe are basically bringing in packages and incorporating the functions contained within them to use for our code."
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#comprehensions",
    "href": "notes/ICPSR_Intro_to_Python/index.html#comprehensions",
    "title": "ICPSR - Introduction to Python",
    "section": "Comprehensions",
    "text": "Comprehensions\n\nshort hand code to replace for/while loops and if/else statements\ncomprehensions provide simple syntax to achieve it in a single line.\ncan be used for lists, sets, and dictionaries\nOverall: makes code shorter and easier to read\n\n\nExample:\nWith for loop:\n\nnumbers = [1,2,3,4,5,6,7,8,9,10]\nnew_list=[]\nfor number in numbers: \n  new_list.append(number)\nprint(new_list)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nwith list comprehension:\n\nnumbers = [1,2,3,4,5,6,7,8,9,10]\nnew_list = [num for num in numbers] # this is the exact same thing as the loop above. Just more condensed \nprint(new_list)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nEND DAY 3!"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#review-1",
    "href": "notes/ICPSR_Intro_to_Python/index.html#review-1",
    "title": "ICPSR - Introduction to Python",
    "section": "Review:",
    "text": "Review:\nWrite a function that, given dictionary consisting of vehicles and their weights in kilograms, constructs a list of the names of vehicles with weight below 2000 kilograms. Use list comprehension to achieve this.\nWith list comprehension:\n\nd={\"Sedan\": 1500, \"SUV\":2000, \"Pickup\": 2500, \"Minivan\":1600, \"Van\":2400, \"Semi\":13600, \"Bicycle\":7, \"Motorcycle\":110}\nget_lighter_vehicles=[weight for weight in d if d[weight]&lt;2000]\nprint(get_lighter_vehicles)\n\n['Sedan', 'Minivan', 'Bicycle', 'Motorcycle']\n\n\nWithout list comprehension:\n\nd={\"Sedan\": 1500, \"SUV\":2000, \"Pickup\": 2500, \"Minivan\":1600, \"Van\":2400, \"Semi\":13600, \"Bicycle\":7, \"Motorcycle\":110}\nfor weight in d:\n  if d[weight]&lt;2000:\n    print(weight)\n\nSedan\nMinivan\nBicycle\nMotorcycle"
  },
  {
    "objectID": "notes/ICPSR_Intro_to_Python/index.html#requests-and-apis",
    "href": "notes/ICPSR_Intro_to_Python/index.html#requests-and-apis",
    "title": "ICPSR - Introduction to Python",
    "section": "Requests and APIs",
    "text": "Requests and APIs\n\nLet’s first talk about how the internet works.\n\nClients & Servers:\n\ndata (web pages) lives on servers\nbrowsers, apps, etc. are clients\nclients send requests to servers\nservers serve the necessary files to users\n\n\nTo request data from these servers we use the “requests” library in Python\n\nallows us to send requests to servers\nneed internet connection\n\n\nExample:\n\nimport requests\nr = requests.get('https://www.python.org/')\nr.status_code\n# you should get 200\n# if you get anything else. Something is wrong and is not working. \n\n200\n\n\nIf I were to run the following code:\n\nprint(r.text) # this gives you all the html code of the page. \n\nThis would print out the html code for the entire webpage. While this may seem scary, this is actually great! Because html is another coding language, by knowing just a little of html, I can pick and choose what parts of the webpage I want. Below is some basic code and information for html documents:\n\nstyle information, including links to CSS files\nJavascript scripts and links to javascript files\nhtml tags (just add “&lt;&gt;” around these head, li, div, img, etc)\nclasses, ids, toggle buttons, many more\nnavigation bar, side bar, footer.\n\nHow do parse through all of this code? We use a parser.\n\na parser is a software that recognizes the structure of an HTML document\nallows the extraction of certain parts of the code\nthe “BeautifulSoup” library serves that purpose\n\n\nAPIs\n\nApplication Programming Interface (API) provide structured data.\n\nstructured basically means csv files, etc.\n\nthey allow for the building of applications\nseparate design from content\naccess the data directly\n\n\nRequests to APIs:\n\nGET (get/retrieve data from server)\n\nWe only looked at this for the workshop.\n\nPOST (update data on server)\nPUT (add data to server)\nDELETE (delete data from server)\n\nMany governmental agencies, newspapers, and common data sources have public APIs that can be accessed from R or Python\n\nyou might need a key (permission) to access the data.\n\n\n\n\nGET requests to an API\n\nrequests typically start with an endpoint defined by the host (server)\nFor example:\n\nWikipedia provides one endpoint\nYouTube provides many endpoints, depending on what one is working with.\n\nFormat of parameters\n\n?param1=value1&param2=value2&param3=value3…\nparameters is how we define what we want from the API.\n\nFollow example in pdf documentation for class.\n\nEND DAY 4!"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html",
    "href": "notes/ICPSR_Machine_Learning/index.html",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "",
    "text": "Professor: Christopher Hare\nAffiliation: UC Davis\nTHESE ARE NOTES! These do not and cannot replace learning the material through a classroom setting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction\nto Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.\nstatlearning.com/\nBoehmke, Bradley and Brandon Greenwell. 2019. Hands-On Machine Learning with R. Boca\nRaton, FL: CRC Press. https://koalaverse.github.io/homlr/\nChollet, Francois, J.J. Allaire, and Tomasz Kalinowski. 2022. Deep Learning with R. 2nd ed.\nShelter Island, NY: Manning.\n\n\n\n\nThis course teaches machine learning through R. R is pretty good with machine learning and covers everything we need to. Python is good but in the interest of time, we focus on R."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#required-reading",
    "href": "notes/ICPSR_Machine_Learning/index.html#required-reading",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "",
    "text": "James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction\nto Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.\nstatlearning.com/\nBoehmke, Bradley and Brandon Greenwell. 2019. Hands-On Machine Learning with R. Boca\nRaton, FL: CRC Press. https://koalaverse.github.io/homlr/\nChollet, Francois, J.J. Allaire, and Tomasz Kalinowski. 2022. Deep Learning with R. 2nd ed.\nShelter Island, NY: Manning."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#coding-software",
    "href": "notes/ICPSR_Machine_Learning/index.html#coding-software",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "",
    "text": "This course teaches machine learning through R. R is pretty good with machine learning and covers everything we need to. Python is good but in the interest of time, we focus on R."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#history-of-machine-learning-in-social-science",
    "href": "notes/ICPSR_Machine_Learning/index.html#history-of-machine-learning-in-social-science",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "History of Machine Learning in Social Science",
    "text": "History of Machine Learning in Social Science\n\nThe People-Machine\nIn the 1950/1960s, some researchers put together a model for Kennedy’s presidential campaign. The attempt was to predict voter turnout based on issue salience. Specifically, if anti-Catholicism became more salient near the election, how would that influence voter turnout? The idea was to simulate human behavior.\nThe point of this reading is to draw that lineage of social science and machine learning. This field is not new.\n\n\nStatistical Modeling: The Two Cultures - Leo Breiman (2001)\nVery important reading. If there is any reading you should do, it should be this one - Professor Hare\n\nData modeling culture\n\nspecify the data generating process between X and y\nfocus on inference from model parameters\nmore emphasis on deductive reasoning\n\nAlgorithmic modeling culture\n\nCapture the black box process between X and y\nrather than specifying the relationship (i.e. the effect is linear, education’s effect is quadratic, etc), we let the model specify or figure it out.\nFocus is on the quality of model predictions\nNo true model: more emphasis on inductive reasoning/learning from data\n\nlets let the data guide the model\n\ncontrast this with letting theory guide the model.\n\nNo one is saying inductive &gt; deductive or deductive &gt; inductive\n\nIsaac Asimov: “The most exciting phrase to hear in science, the one that heralds new discoveries, is not ‘Eureka’ but ‘That’s funny’…”\nKarl Popper emphasized deductive reasoning, but creating or discovering new theories is a necessarily inductive process (no deductive “machinery” is possible for theory building)."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#parametric-vs.-non-parametric-models",
    "href": "notes/ICPSR_Machine_Learning/index.html#parametric-vs.-non-parametric-models",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Parametric vs. Non-Parametric models",
    "text": "Parametric vs. Non-Parametric models\nOur goal is to use data to estimate f(x) (this is the line or true relationship). We want to find a function \\(\\hat{f}\\) such that \\(Y\\approx \\hat{f}(\\textbf{X})\\) for any observation (X, Y). So how do we estimate f(X) (the line/relationship)? We can use either a parametric or non-parametric approach/model. Up until now, I have only been taught parametric (this being linear regression, etc.).\nSo which is better, parametric or non-parametric? The answer is simple: the parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of \\(f\\).\n\nParametric Methods\n\nWe make an assumption about the functional form, or shape of f (the function/line). Usually this assumption is that \\(f\\) is linear.\nWe then need to fit or train the model. This means we need to estimate the parameters (\\(\\beta_1,\\beta_2,...,\\beta_p\\)).\n\nthe most common approach and the one we have been trained extensively is ordinary least squares (OLS).\n\nOLS is one of the many possible ways to fit the linear model\n\nWe use OLS because when the assumptions are met, it is BLUE.\n\n\n\nOne potential downside of a parametric approach is that the model we choose will usually not match the true unknown form of \\(f\\).\n\nwe want to be careful about over-fitting the data.\n\nthis means they follow the errors, or noise, too closely.\nthis is bad because your model will struggle to estimate responses on new observations that were not part of the original training data set.\n\n\nMaybe the true relationship has some curvature (it isn’t linear) but the linear model we create gets us reasonably close to describing the relationship between the X and Y variables.\n\n\n\nNon-Parametric Methods\n\nWe do not make assumptions about the functional form of \\(f\\).\n\nnon-parametric approaches estimate \\(f\\) as close to the data points as possible without being too rough or wiggly\n\nlower residuals\n\n\nOne major disadvantage of non-parametric approaches is that a lot more observations are needed to get an accurate estimate for \\(f\\).\nYou can still over fit a non parametric model.\n\n\n\nPrediction Accuracy and Model Interpretability\n\n\n\n\n\n\nAn obvious question that follows is: Why would we ever chose a more restrictive method instead of a very flexible approach? Wouldn’t more flexibility get us closer to the true \\(f\\)?\n\nrestrictive models are easier to interpret\n\nthe relationship is straightforward and easy to understand!\nOLS is not only really good (it is the best when assumptions are met) but it is super easy to interpret.\n\n\n\n\n\nSupervised versus Unsupervised Learning\n\nSupervised\n\nPretty much all the models I have used thus far (and listed prior) use supervised learning.\n\neach observation of the predictor measurement has an associated response measurement for \\(x_i\\) there is an associated \\(y_i\\) value.\n\nGoal: prediction and inference (both involve modeling the relationship between predictors and outcome of interest)\nThe goal of supervised learning is to learn a predictive model that maps features of the data (e.g. house size, location, floor type, …) to an output (e.g. house price). If the output is categorical, the task is called classification, and if it is numerical, it is called regression.\n\n\n\nUnsupervised\n\nMore challenging\nnot focused on in this workshop.\nGoal: search for simple structure underlying data set (what are the relationships between variables)\nYou have a bunch of dependent variables and you think there is some cause underlying them. We are trying to back out an X.\nfor every observation we observe a vector of measurements \\(x_i\\) but no associated response \\(y_i\\)\n\nlike going in blind\n\ncalled unsupervised because we lack a response variable that can supervise our analysis.\nOne type is called cluster analysis.\n\n\n\nSemi-supervised learning\n\nmore rare\nkind of in between the two.\ndon’t worry about this, we don’t go over it."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#overfitting",
    "href": "notes/ICPSR_Machine_Learning/index.html#overfitting",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Overfitting",
    "text": "Overfitting\n\nVery important concept in machine learning.\nOverfitting occurs when a model starts to memorize the aspects of the training set and in turn loses the ability to generalize.\nOverfitting is reducing bias at a tremendous cost to variance.\nWe want a balance that gets us as close as possible.\n\nwhat is the right amount of complexity for our model?\n\nthis bleeds into “tuning” our parameters."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#the-bias---variance-trade-off",
    "href": "notes/ICPSR_Machine_Learning/index.html#the-bias---variance-trade-off",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "The Bias - Variance Trade Off",
    "text": "The Bias - Variance Trade Off\n\nWe need to find a balance between overfitting and underfitting our model. This balance is the bias-variance trade off.\nWhen we are estimating \\(f\\), we don’t want to ignore true nonlinear complexities, but we also want parsimonious models of social/behavioral phenomena that generalize well.\nOur estimated function \\(\\hat{f}\\) should be an approximation to \\(f\\) (i.e., the true data generating process) that minimizes bias and variance.\nThis is referred to as a trade-of because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by ftting a horizontal line to the data). The challenge lies in finding a method for which both the variance and the squared bias are low.\n\n\nVariance\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set.\n\nremember that we are trying to estimate \\(f\\) using our sample. There is uncertainty around it. Different data may give us a different \\(\\hat{f}\\).\n\nWe can use one sample to estimate the variance around the estimate.\n\nthis of course is called Standard Error. If a method has high variance then small changes in the training data can result in large changes in \\(\\hat{f}\\).\n\n\n\nIn general more flexible statistical methods have higher variance.\n\n\n\nBias\n\nrefers to the coefficient.\nAs a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.\nOverfitting a model reduces the bias as much as possible at the expense of variance.\nDoes our estimate \\(\\approx\\) the true population estimate"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#model-error",
    "href": "notes/ICPSR_Machine_Learning/index.html#model-error",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Model Error",
    "text": "Model Error\nThe goal is to reduce both of these errors. But it is a bit more complex. How do we know the level of error is reducible or irreducible? The answer is gauge its predictive power. You do this through the error of your test sample. The section on cross validation will clear this up a bit more (hopefully). We want to minimize the test error.\n\nReducible error\n\nError due to disparities between \\(\\hat{f}\\) and \\(f\\).\nThis relates to the functional form.\nDo we include all of the relevant parameters (variables)?\nis it linear or quadratic?\nthese are the elements we can control\n\n\n\nIrreducible error\n\nError due to stochastic elements that is built into Y, separate from data-generating process of \\(f\\).\nThis is error that we cannot model.\n\nit is the “randomness” of the observations.\n\nThink about people. We can try to model them but every human is different for infinite reasons. We can’t model that and thus, that is irreducible error.\nthere will still be error remaining even if we approximate \\(f\\) exactly."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#cross-validation",
    "href": "notes/ICPSR_Machine_Learning/index.html#cross-validation",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nBefore diving into cross-validation and what it is, we need to continue the thread started above on model error. We need to estimate two kinds of model error to deal with bias-variance trade off.\n\nTraining Error:\n\nError rate produced when model is applied to in-sample data\n\nThe error rate for the data used to estimate the model: how well does the model fit to existing data?\n\n\n\n\nTesting Error:\n\nThe average error that results from using a statistical learning method to predict the response of new, out of sample observation.\n\nThe error rate for outside data: how well does the model fit new data.\n\n\nThese two types of error are quite different: in particular, the training error rate can drastically understate the test error rate.\n\n\nHow do we estimate these errors?\nWe use cross-validation! What is it?\n\nCross-validation explanation\n\nRandomly divide the available set of samples into two parts: a training set and a test or hold-out set.\n\nkeep 80% of the data for training and then use 20% of the data for testing\n\nthis ratio can range. Could use 70-30, 50-50, etc.\n\n\nEstimate the model using the remaining of the data.\n\nApply the model to the observations in that subset, generating predictions \\((\\hat{Y}_{test}=\\hat{f}(X_{test}))\\) and residuals \\(Y_{test}-\\hat{Y}_{test}\\) to estimate testing error.\n\nOne issue in doing this: we lose data.\n\nMaybe we have very little data…we don’t want to give up some of that data! What do we do?\n\nTo get around this, we use K-fold cross-validation.\n\n\n\n\n\n\nK-fold Cross-Validation\n\nRandomly divide the data into K equal sized parts. We leave out part k, fit the model to the other K-1 parts (combined), and then obtain predictions for the left-out kth part.\nImagine we have all the data and we bin it into 4 compartments/sections. This would be called 4-fold cross validation.\n\nImagine we use the first three compartments to train and then the last section to test\n\nthen do it again but use 3 different compartments and a different section to test.\n\nkeep going through the permutations.\n\nyou average across all\n\nall the data is used.\n\n\n\n\nYou can then compare which method/model does a better job at prediction.\n\nThe amount of folds you chose is arbitrary\n\nmost popular is 5 or 10.\n\nYou do this for any ML model.\n\n\n\n\nk-fold Cross Validation Visualized"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#multivariate-adaptive-regression-splines-mars",
    "href": "notes/ICPSR_Machine_Learning/index.html#multivariate-adaptive-regression-splines-mars",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Multivariate Adaptive Regression Splines (MARS)",
    "text": "Multivariate Adaptive Regression Splines (MARS)\n\nImagine doing a regression BUT instead of one line, you can add a bunch to fit the data.\nAllow for nonlinearities and automate feature selection within the linear regression framework by searching for optimal hinge points or knots.\n\nthis produces a series of hinge functions h(x-a), where a denotes the location of the knot and h is a regression coefficient.\n\nmore flexible than regression (duh)\nWe are concerned with tuning, not really the variables.\nWe just throw a bunch of Xs and specify our dependent variable and we just let the machine figure it out.\ncoefficients are not attached to variables but regions.\nSee mars.r for example code and more discussion."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#classification-problems",
    "href": "notes/ICPSR_Machine_Learning/index.html#classification-problems",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Classification problems",
    "text": "Classification problems\n\nWhen we move from regression problems (where y is continuous) to classification problems (where y is categorical), we need to assess model performance differently\nInclude different kind of fit statistics in your MLE model.\nConfusion matrix:\n\nmore to discuss here. TK 06/20/24"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#curse-of-dimensionality",
    "href": "notes/ICPSR_Machine_Learning/index.html#curse-of-dimensionality",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Curse of Dimensionality",
    "text": "Curse of Dimensionality\n\nIncluding a large number of predictor variables introduces data sparsity.\n\nthis seems to relate (at least to me) as degrees of freedom (might be wrong)\n\nShrinkage or regularization methods purposefully bias the coefficients (towards zero) in such a way that improves overall predictive performance by navigating the bias-variance trade-off.\nThis is more of a conceptual problem. Every additional independent variable we include increases the space exponentially: the space that we’re operating in for DV ~ IV1 + IV2 + IV3 is exponentially larger than DV ~ IV1 + IV2.\n\nAnd so we’ll end up in cases where we don’t have observations in unique combinations of IV1, 2, and 3. A parametric model, like OLS, will fit a line through the middle of the space (to minimize error) but it will end up giving us predictions for observations that don’t exist\n\ne.g., if we are predicting vote choice using race (white, black, Asian) and partisanship (democrat, independent, republican), if we have 1,000 observations we should observe every combination of those IVs. However, if we add additional covariates, like income, gender, state of residence etc., we are likely to end up with situations where we have no observations with a given combination of covariate values (e.g., perhaps we have no black, female, republicans who make &gt;$100k, and live in Alaska)\n\nThis is a problem ML field cares about but the political science field does not give this issue much attention (they should)."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#receiver-operator-characteristic-curves",
    "href": "notes/ICPSR_Machine_Learning/index.html#receiver-operator-characteristic-curves",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Receiver-operator characteristic curves",
    "text": "Receiver-operator characteristic curves\n\nnavan.name/roc/\n\ngood explanation of ROC.\n\nthe more we separate the sets of outcomes (blue and red pdfs), the better our model is.\n\n\nTrue positive rate against the False positive rate\nBinary Classification method\nseems popular in health.\nArea under the Curve (AUC) - we want to maximize this value.\nI think we use to evaluate different models?????????"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#shrinkage-and-regularization",
    "href": "notes/ICPSR_Machine_Learning/index.html#shrinkage-and-regularization",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Shrinkage and Regularization",
    "text": "Shrinkage and Regularization\nI’m confused about this…still not clear. Why would we need this when we have k-fold cross validation…doesn’t that minimize bias? How do we know when to use these methods? Answer from 06/21 - we apply cross validation to these methods to determine what amount of shrinkage we want.\n\nOur end-goal is an accurate model (as measured by out-of-sample or test set performance)\n\nWe can use shrinkage and regularization to help us.\n\nhelps us prevennt over fitting.\n\n\nShrinkage pulls coefficients closer to zero as the regularization parameter increases.\n\n\nRidge Regression (L2)\n\nAKA penalized least squares or penalized likelihood) is another shrinkage method that penalizes large magnitude values of beta coefficients in regression models.\nwe add bias to our coefficients\n\nwhy would we do that?\n\npush our regression parameters closer to zero.\n\nhelps our variance.\n\nbias-variance trade off to maximize fit for out of sample observations.\n\n\n\n\nLiterally a regression model, just adding a penalization function. (square the beta)\nShrinkage: when you have less data shrink to the overall mean.\n\nthink of batting averages - Hank Aaron vs. some random guy who had one at bat and has a perfect batting average.\n\n\\(\\lambda\\) is important\n\ncan be any value between zero and infinity\nif zero then the ridge regression penalty will equal the same line as the OLS line.\nthe larger the value, the greater the penalty\n\nthink of the slope of the line changing.\n\nHow do we figure out which \\(\\lambda\\) is the most optimal?\n\nWe use cross-validation!\n\nto determine what \\(\\lambda\\) produces lowest variance\n\n\n\nCan also be used if IV is discrete.\n\nand logistic regression.\n\n\n\n\nLasso (L1)"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#simulations",
    "href": "notes/ICPSR_Machine_Learning/index.html#simulations",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Simulations",
    "text": "Simulations\n\nSimulations are most of what we do here.\nRe-sampling to estimate uncertainty intervals\nBootstrapping to estimate uncertainty intervals\nMonte Carlo experiments to learn about the properties of an estimator, uncover predictor effects, and evaluate counterfactual/future scenarios."
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#bootstrapping",
    "href": "notes/ICPSR_Machine_Learning/index.html#bootstrapping",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nWe can make a whole bunch of new samples with the original sample.\nWe can’t see the population parameter, but we can estimate it using repeated samples via bootstrapping.\n\nwe can get pretty damn close to it!"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#trees",
    "href": "notes/ICPSR_Machine_Learning/index.html#trees",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Trees",
    "text": "Trees\n\nDecision trees use splitting rules to stratify or segment the predictor space into a number of simple regions, either for regression or classification problems\nCan be applied to both regression and classification problems (continuous and discrete).\nWe can present these rules in tree form\nIt is important to remember that what trees are doing is recursive partitioning the feature space (X) with binary (yes/no) splits.\nTheir goal is to specify regions of the covariate space such that the outcome is homogeneous and the number of observations in each region is sufficiently large, yet where the regions themselves are sufficiently numerous and unstructured to allow for complex relationships between covariates and the outcome.\n\n\n\n\n\n\n\nThe tree above has two internal nodes and three terminal nodes (or leaves)\nWe need to know one tree and then we can get more and more trees and create a forest.\n\nthis will be very powerful\n\nTree-based methods to be very useful for making accurate predictions when the underlying Data Generating Process (DGP) includes nonlinearities, discontinuities, and interactions among many covariates.\n\nappropriate to use when researchers’ primary goal is to correctly capture the nuances of a potentially complex but known DGP in a setting with many potential predictors related in nonlinear and interactive ways to the outcome.\n\nTREES SHOULD NOT BE USED FOR THEORY TESTING BUT FOR BETTER PREDICTION.\n\n\nThe power of trees becomes apparent when we add a bunch together.\nTrying to find splits in nodes that have the largest differences between means.\nTrees split based on maximizing the information gain.\n\nwe want pure nodes\n\nIndividual trees are very sensitive.\nModel is GREEDY\n\nit does not backtrack\n\nit finds the best split and then moves on.\n\n\n\n\nDepth\n\nHow deep do we want a tree? The deeper the tree, the more complex.\n\nGreater depth may lead to an overfit.\n\n\nThis is what an overly complex tree looks like.\n\nwe need a balance b/c this is overfitting.\n\n\n\nThe shallower the tree -&gt; less variance.\n\nbut we need to find a balance so that the tree can at least capture nuances and potential interactions within the data.\n\nWe control this by either limiting how deep it can go OR pruning the tree\n\n\nEarly Stopping\n\nEarly stopping refers to specifying how many terminal nodes the tree will have.\n\nwill it have 2, 3, 4, etc.\n\n\n\n\nThe shallower the tree -&gt; less variance.\n\nbut too shallow and we have bias\n\nwe need to find a balance so that the tree can at least capture nuances and potential interactions within the data.\n\n\n\n\n\nPruning\n\nAnother method to specify the depth of the tree\nwe find an optimal sub-tree to\nbasically determine where we can optimally set splits. (see below for an example)\n\n\n\n\n\nBenefits of trees:\n\nAllows p&gt;&gt;n\nstraightforward interpretation\nnaturally models interaction effects\nimplicitly conducts feature selection.\nWhen we combine individual trees and average - they are spooky good\n\n\n\nEntropy\n\nVery important for decision trees.\nQuantifies similarities and differences\nwe want to minimize entropy.\nHow do we decide to divide the data?\nFormula:\n\n\n\nBagging\n\nBootstrap aggregating\n\nbootstrap original dataset -&gt; create a bunch of individual trees -&gt; aggregate and average the individual trees.\n\nbecause we are bootstrapping from the original sample, they are all somewhat similar.\nAlthough usually applied to decision tree methods, it can be used with any type of method.\n\n\n\n\nDrawn in class by Prof. Hare\n\n\n\nStill one problem:\n\nAlthough the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree.\n\nRather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships.\n\nin layman terms, the independent trees we create. Look very similar.\n\nthis means these independent trees have a high degree of correlation with each other.\n\nRandom forrests do a better job by reducing this correlation and improve the accuracy of the model.\n\n\n\n\n\n\n\n\nRandom Forrest\n\nStill do bagging but add a new rule:\n\nevery time a split is considered, only consider a random subset of prediction variables in which to split.\n\nForces trees to be different from each other\n\nindividual trees become “dumber”.\n\nWe want diverse exploration\n\ntrees are exploring different components of the data generating process that may not be as obvious.\n\nThe individual trees lack complete understanding but if we put them together, we can get a more comprehensive account of the data generating process.\n\n“Rashomon effect”\n\nBreimen uses this example.\n\n\nHonda civic of machine learning\n\nneural networks are spaceships\n\nStill optimizing out of sample data.\n\n\n\nBoosting\n\nFit a single tree to the entire data\n\nthen take the residuals from the result\n\nnow, fit another tree to the residuals.\n\nkeep going.\n\nit is SEQUENTIAL.\n\n\n\n\nStill have an overfitting problem\npeople like boosting &gt; random forest\nVERY POWERFUL!\n\nbest tree based method\n\nand tree based methods are good for tabular data.\n\n\nPartial Dependence Plot - no idea what this is"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#interpretability-usefulness",
    "href": "notes/ICPSR_Machine_Learning/index.html#interpretability-usefulness",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Interpretability Usefulness",
    "text": "Interpretability Usefulness\n\nInterpretability is useful for detecting bias\n\nIf we know why it gave a prediction, we can tune it and fix any bias\n\nThis is important for racial bias issues.\n\n\nInterpretability can helps us extract insights and actionable information from ML models.\n\nQuestions like such can be answered with more interpretability:\n\nWhat are the most important customer attributes driving behavior?\nHow are these attributes related to the behavior output?\nDo multiple attributes interact to drive different behavior among customers?\nWhy do we expect a customer to make a particular decision?\nAre the decisions we are making based on predicted results fair and reliable?"
  },
  {
    "objectID": "notes/ICPSR_Machine_Learning/index.html#local-interpretation-v.-global-interpretation",
    "href": "notes/ICPSR_Machine_Learning/index.html#local-interpretation-v.-global-interpretation",
    "title": "Machine Learning: Applications in Social Science Research (ICPSR)",
    "section": "Local Interpretation v. Global Interpretation",
    "text": "Local Interpretation v. Global Interpretation\n\nGlobal Interpretation:\n\nunderstanding how the model makes predictions, based on a holistic view of its features and how they influence the underlying model structure.\n\nthink of a zoomed out view\nDescribe how features affect the prediction on average.\n\nGlobal model interpretability helps to understand the relationship between the response variable and the individual features (or subsets thereof).\nVery hard to do in practice.\n\nwe usually have hundreds of variables (even millions).\n\nWe can get an idea of what variables are the most powerful but to the extent that we can fully grasp the model from a zoomed out view is extremely difficult.\n\n\nSince they describe average behavior, they are good for the modeler when we want to understand the general mechanisms in the data or debug a model.\n\n\nTypes of Global Interpretation Methods:\n\nPartial Dependence Plot\nAccumulated Local Effects Plot\n\n\n\n\n\n\n\nLocal Interpretation:\n\nGlobal interpretation can be deceptive. It may tell us what variable is important BUT that is not necessarily true for all observations.\n\nLocal interpretation helps us understand what features are influencing the predicted response for a given observation (or small group of observations).\n\nHelps us understand why our model is making a specific prediction for THAT (or a group of) observation\n\n\nAims to explain individual predictions"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "My first blog!\n\n\n\n2024\n\n\nBlog\n\n\nPersonal\n\n\n\nTest blog to make sure everything is working.\n\n\n\nStone Neilon\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Text_Data/index.html",
    "href": "notes/Text_Data/index.html",
    "title": "Text as Data",
    "section": "",
    "text": "Week 1\n\n\n\n\nCitationBibTeX citation:@online{neilon2024,\n  author = {Neilon, Stone},\n  title = {Text as {Data}},\n  date = {2024-08-26},\n  url = {https://stoneneilon.github.io/notes/Text_Data/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNeilon, Stone. 2024. “Text as Data.” August 26, 2024. https://stoneneilon.github.io/notes/Text_Data/."
  },
  {
    "objectID": "notes/American_Politics/index.html",
    "href": "notes/American_Politics/index.html",
    "title": "American Politics Core",
    "section": "",
    "text": "Week 1\n\n\nWeek 2\n\n\n\n\nCitationBibTeX citation:@online{neilon2024,\n  author = {Neilon, Stone},\n  title = {American {Politics} {Core}},\n  date = {2024-08-26},\n  url = {https://stoneneilon.github.io/notes/American_Politics/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNeilon, Stone. 2024. “American Politics Core.” August 26,\n2024. https://stoneneilon.github.io/notes/American_Politics/."
  }
]