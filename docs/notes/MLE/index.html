<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stone Neilon">
<meta name="dcterms.date" content="2024-08-26">
<meta name="description" content="Professor: Andy Philips">

<title>Stone Neilon - MLE - Data 3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stone Neilon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/file/d/1eZT7KaLsqdW3k4WxoeloMPm_9cdf-9A6/view?usp=sharing"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#syllabus" id="toc-syllabus" class="nav-link active" data-scroll-target="#syllabus">Syllabus</a></li>
  <li><a href="#topic-1---introduction-to-probability-models" id="toc-topic-1---introduction-to-probability-models" class="nav-link" data-scroll-target="#topic-1---introduction-to-probability-models">Topic 1 - Introduction to Probability Models</a>
  <ul class="collapse">
  <li><a href="#required-reading" id="toc-required-reading" class="nav-link" data-scroll-target="#required-reading">Required Reading:</a>
  <ul class="collapse">
  <li><a href="#king-chapters-1-2" id="toc-king-chapters-1-2" class="nav-link" data-scroll-target="#king-chapters-1-2">King Chapters 1 &amp; 2</a></li>
  <li><a href="#ward-and-ahlquist-chapter-1" id="toc-ward-and-ahlquist-chapter-1" class="nav-link" data-scroll-target="#ward-and-ahlquist-chapter-1">Ward and Ahlquist Chapter 1:</a></li>
  </ul></li>
  <li><a href="#lecture" id="toc-lecture" class="nav-link" data-scroll-target="#lecture">Lecture:</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example:</a></li>
  </ul></li>
  <li><a href="#homework-1" id="toc-homework-1" class="nav-link" data-scroll-target="#homework-1">Homework 1:</a>
  <ul class="collapse">
  <li><a href="#probability-v.-likelihood" id="toc-probability-v.-likelihood" class="nav-link" data-scroll-target="#probability-v.-likelihood">Probability v. Likelihood:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#topic-2-estimation-looking-under-the-hood" id="toc-topic-2-estimation-looking-under-the-hood" class="nav-link" data-scroll-target="#topic-2-estimation-looking-under-the-hood">Topic 2: Estimation: Looking Under the Hood</a>
  <ul class="collapse">
  <li><a href="#under-the-hood-of-ml" id="toc-under-the-hood-of-ml" class="nav-link" data-scroll-target="#under-the-hood-of-ml">Under the Hood of ML:</a></li>
  <li><a href="#measures-of-uncertainty" id="toc-measures-of-uncertainty" class="nav-link" data-scroll-target="#measures-of-uncertainty">Measures of Uncertainty:</a></li>
  <li><a href="#properties-of-ml" id="toc-properties-of-ml" class="nav-link" data-scroll-target="#properties-of-ml">Properties of ML:</a></li>
  <li><a href="#disadvantages-of-ml" id="toc-disadvantages-of-ml" class="nav-link" data-scroll-target="#disadvantages-of-ml">Disadvantages of ML:</a></li>
  <li><a href="#information-criteria" id="toc-information-criteria" class="nav-link" data-scroll-target="#information-criteria">Information Criteria:</a>
  <ul class="collapse">
  <li><a href="#lagrange-multiplier-test" id="toc-lagrange-multiplier-test" class="nav-link" data-scroll-target="#lagrange-multiplier-test">LaGrange Multiplier Test:</a></li>
  <li><a href="#wald-test" id="toc-wald-test" class="nav-link" data-scroll-target="#wald-test">Wald Test:</a></li>
  <li><a href="#the-3-likelihood-tests" id="toc-the-3-likelihood-tests" class="nav-link" data-scroll-target="#the-3-likelihood-tests">The 3 Likelihood Tests:</a></li>
  </ul></li>
  <li><a href="#topic-3-generalized-linear-models" id="toc-topic-3-generalized-linear-models" class="nav-link" data-scroll-target="#topic-3-generalized-linear-models">Topic 3: Generalized Linear Models</a></li>
  </ul></li>
  <li><a href="#topic-4" id="toc-topic-4" class="nav-link" data-scroll-target="#topic-4">Topic 4:</a>
  <ul class="collapse">
  <li><a href="#limited-dependent-variables" id="toc-limited-dependent-variables" class="nav-link" data-scroll-target="#limited-dependent-variables">Limited Dependent Variables</a></li>
  </ul></li>
  <li><a href="#topic-5-binary-choice-models" id="toc-topic-5-binary-choice-models" class="nav-link" data-scroll-target="#topic-5-binary-choice-models">Topic 5: Binary Choice Models</a>
  <ul class="collapse">
  <li><a href="#dichotomous-dependent-variables" id="toc-dichotomous-dependent-variables" class="nav-link" data-scroll-target="#dichotomous-dependent-variables">Dichotomous dependent variables</a></li>
  <li><a href="#logit-log-odds" id="toc-logit-log-odds" class="nav-link" data-scroll-target="#logit-log-odds">Logit (log-odds)</a></li>
  <li><a href="#probit" id="toc-probit" class="nav-link" data-scroll-target="#probit">Probit</a></li>
  <li><a href="#latent-variable" id="toc-latent-variable" class="nav-link" data-scroll-target="#latent-variable">Latent Variable</a></li>
  <li><a href="#measures-of-fit-r2" id="toc-measures-of-fit-r2" class="nav-link" data-scroll-target="#measures-of-fit-r2">Measures of Fit: <span class="math inline">\(R^2\)</span></a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation:</a></li>
  <li><a href="#first-differences-lpm" id="toc-first-differences-lpm" class="nav-link" data-scroll-target="#first-differences-lpm">First Differences (LPM)</a></li>
  <li><a href="#first-differences-logitprobit" id="toc-first-differences-logitprobit" class="nav-link" data-scroll-target="#first-differences-logitprobit">First Differences (Logit/Probit)</a>
  <ul class="collapse">
  <li><a href="#read-hanmer-and-kalkan" id="toc-read-hanmer-and-kalkan" class="nav-link" data-scroll-target="#read-hanmer-and-kalkan">Read Hanmer and Kalkan</a></li>
  </ul></li>
  <li><a href="#stochastic-simulation" id="toc-stochastic-simulation" class="nav-link" data-scroll-target="#stochastic-simulation">Stochastic Simulation</a></li>
  <li><a href="#example-flores-macias-and-kreps-2013" id="toc-example-flores-macias-and-kreps-2013" class="nav-link" data-scroll-target="#example-flores-macias-and-kreps-2013">Example: Flores-Macias and Kreps (2013)</a></li>
  </ul></li>
  <li><a href="#topic-6" id="toc-topic-6" class="nav-link" data-scroll-target="#topic-6">Topic 6:</a>
  <ul class="collapse">
  <li><a href="#heteroskedasticity" id="toc-heteroskedasticity" class="nav-link" data-scroll-target="#heteroskedasticity">Heteroskedasticity</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias">Omitted variable bias</a></li>
  </ul></li>
  <li><a href="#topic-7" id="toc-topic-7" class="nav-link" data-scroll-target="#topic-7">Topic 7:</a>
  <ul class="collapse">
  <li><a href="#ordered-models" id="toc-ordered-models" class="nav-link" data-scroll-target="#ordered-models">Ordered Models:</a></li>
  </ul></li>
  <li><a href="#topic-8" id="toc-topic-8" class="nav-link" data-scroll-target="#topic-8">Topic 8:</a>
  <ul class="collapse">
  <li><a href="#count-data" id="toc-count-data" class="nav-link" data-scroll-target="#count-data">Count Data:</a>
  <ul class="collapse">
  <li><a href="#deriving-the-poisson-function" id="toc-deriving-the-poisson-function" class="nav-link" data-scroll-target="#deriving-the-poisson-function">Deriving the Poisson Function</a></li>
  </ul></li>
  <li><a href="#poisson-log-likelihood" id="toc-poisson-log-likelihood" class="nav-link" data-scroll-target="#poisson-log-likelihood">Poisson Log-Likelihood</a></li>
  <li><a href="#interpreting" id="toc-interpreting" class="nav-link" data-scroll-target="#interpreting">Interpreting</a></li>
  <li><a href="#negative-binomial" id="toc-negative-binomial" class="nav-link" data-scroll-target="#negative-binomial">Negative Binomial</a></li>
  <li><a href="#interpreting-1" id="toc-interpreting-1" class="nav-link" data-scroll-target="#interpreting-1">Interpreting</a></li>
  </ul></li>
  <li><a href="#topic-9" id="toc-topic-9" class="nav-link" data-scroll-target="#topic-9">Topic 9:</a>
  <ul class="collapse">
  <li><a href="#zero-inflated-poisson" id="toc-zero-inflated-poisson" class="nav-link" data-scroll-target="#zero-inflated-poisson">Zero-inflated Poisson</a></li>
  <li><a href="#zero-inflated-negative-binomial" id="toc-zero-inflated-negative-binomial" class="nav-link" data-scroll-target="#zero-inflated-negative-binomial">Zero-inflated Negative Binomial</a></li>
  <li><a href="#how-do-i-test-for-zero-inflation" id="toc-how-do-i-test-for-zero-inflation" class="nav-link" data-scroll-target="#how-do-i-test-for-zero-inflation">How do I test for Zero-inflation?</a></li>
  <li><a href="#truncated-counts" id="toc-truncated-counts" class="nav-link" data-scroll-target="#truncated-counts">Truncated Counts</a></li>
  <li><a href="#censored-count" id="toc-censored-count" class="nav-link" data-scroll-target="#censored-count">Censored Count</a></li>
  <li><a href="#hurdle-models" id="toc-hurdle-models" class="nav-link" data-scroll-target="#hurdle-models">Hurdle Models</a></li>
  </ul></li>
  <li><a href="#topic-10" id="toc-topic-10" class="nav-link" data-scroll-target="#topic-10">Topic 10</a>
  <ul class="collapse">
  <li><a href="#duration-models---time-to-event-data" id="toc-duration-models---time-to-event-data" class="nav-link" data-scroll-target="#duration-models---time-to-event-data">Duration Models - time to event data</a></li>
  <li><a href="#kaplan-meier-curves" id="toc-kaplan-meier-curves" class="nav-link" data-scroll-target="#kaplan-meier-curves">Kaplan-Meier Curves</a></li>
  <li><a href="#duration-models" id="toc-duration-models" class="nav-link" data-scroll-target="#duration-models">Duration Models</a></li>
  <li><a href="#the-hazard-rate" id="toc-the-hazard-rate" class="nav-link" data-scroll-target="#the-hazard-rate">The Hazard Rate</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  <li><a href="#exponential-functions" id="toc-exponential-functions" class="nav-link" data-scroll-target="#exponential-functions">Exponential Functions</a></li>
  <li><a href="#weibull-distribution" id="toc-weibull-distribution" class="nav-link" data-scroll-target="#weibull-distribution">Weibull Distribution</a></li>
  <li><a href="#gamma" id="toc-gamma" class="nav-link" data-scroll-target="#gamma">Gamma</a></li>
  <li><a href="#interpretation-proportional-hazards" id="toc-interpretation-proportional-hazards" class="nav-link" data-scroll-target="#interpretation-proportional-hazards">Interpretation: Proportional Hazards</a></li>
  <li><a href="#cox-model" id="toc-cox-model" class="nav-link" data-scroll-target="#cox-model">Cox Model</a></li>
  </ul></li>
  <li><a href="#topic-11" id="toc-topic-11" class="nav-link" data-scroll-target="#topic-11">Topic 11</a>
  <ul class="collapse">
  <li><a href="#event-history-review" id="toc-event-history-review" class="nav-link" data-scroll-target="#event-history-review">Event History Review:</a></li>
  <li><a href="#proportional-hazards" id="toc-proportional-hazards" class="nav-link" data-scroll-target="#proportional-hazards">Proportional Hazards</a></li>
  <li><a href="#time-varying-covariates" id="toc-time-varying-covariates" class="nav-link" data-scroll-target="#time-varying-covariates">Time-Varying Covariates</a></li>
  <li><a href="#schoenfeld-residuals" id="toc-schoenfeld-residuals" class="nav-link" data-scroll-target="#schoenfeld-residuals">Schoenfeld Residuals</a></li>
  <li><a href="#cox-interpretation" id="toc-cox-interpretation" class="nav-link" data-scroll-target="#cox-interpretation">COX interpretation</a></li>
  </ul></li>
  <li><a href="#topic-12" id="toc-topic-12" class="nav-link" data-scroll-target="#topic-12">Topic 12</a>
  <ul class="collapse">
  <li><a href="#missing-data" id="toc-missing-data" class="nav-link" data-scroll-target="#missing-data">Missing Data</a></li>
  <li><a href="#mcar" id="toc-mcar" class="nav-link" data-scroll-target="#mcar">MCAR</a></li>
  <li><a href="#mar" id="toc-mar" class="nav-link" data-scroll-target="#mar">MAR</a></li>
  <li><a href="#mnar" id="toc-mnar" class="nav-link" data-scroll-target="#mnar">MNAR</a></li>
  <li><a href="#important-for-missingness" id="toc-important-for-missingness" class="nav-link" data-scroll-target="#important-for-missingness">Important for Missingness:</a></li>
  <li><a href="#why-we-dont-really-care-if-our-dv-is-missing" id="toc-why-we-dont-really-care-if-our-dv-is-missing" class="nav-link" data-scroll-target="#why-we-dont-really-care-if-our-dv-is-missing">Why we don’t really care if our DV is missing</a></li>
  <li><a href="#interpolation" id="toc-interpolation" class="nav-link" data-scroll-target="#interpolation">Interpolation</a>
  <ul class="collapse">
  <li><a href="#drawbacks" id="toc-drawbacks" class="nav-link" data-scroll-target="#drawbacks">Drawbacks:</a></li>
  </ul></li>
  <li><a href="#extrapolation" id="toc-extrapolation" class="nav-link" data-scroll-target="#extrapolation">Extrapolation</a></li>
  <li><a href="#imputation" id="toc-imputation" class="nav-link" data-scroll-target="#imputation">Imputation</a></li>
  <li><a href="#standard-errors" id="toc-standard-errors" class="nav-link" data-scroll-target="#standard-errors">Standard Errors</a></li>
  </ul></li>
  <li><a href="#topic-13" id="toc-topic-13" class="nav-link" data-scroll-target="#topic-13">Topic 13</a>
  <ul class="collapse">
  <li><a href="#robustness" id="toc-robustness" class="nav-link" data-scroll-target="#robustness">Robustness:</a></li>
  </ul></li>
  <li><a href="#topic-14" id="toc-topic-14" class="nav-link" data-scroll-target="#topic-14">Topic 14</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MLE - Data 3</h1>
  <div class="quarto-categories">
    <div class="quarto-category">2024</div>
    <div class="quarto-category">Methods</div>
    <div class="quarto-category">Fall</div>
  </div>
  </div>

<div>
  <div class="description">
    Professor: Andy Philips
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://stoneneilon.github.io/">Stone Neilon</a> <a href="https://orcid.org/0009-0006-6026-4384" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.colorado.edu/polisci/people/graduate-students/stone-neilon">
            PhD student of political science @ The University of Colorado Boulder
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="syllabus" class="level1">
<h1><a href="https://www.dropbox.com/scl/fi/32ji3ucw54taatfon4ja4/MLE-syllabus.pdf?rlkey=jhqzrrb2ymqxg70s7wuwzeeyr&amp;st=taxpe036&amp;dl=0">Syllabus</a></h1>
</section>
<section id="topic-1---introduction-to-probability-models" class="level1">
<h1>Topic 1 - Introduction to Probability Models</h1>
<section id="required-reading" class="level2">
<h2 class="anchored" data-anchor-id="required-reading">Required Reading:</h2>
<section id="king-chapters-1-2" class="level3">
<h3 class="anchored" data-anchor-id="king-chapters-1-2">King Chapters 1 &amp; 2</h3>
<section id="chapter-1" class="level4">
<h4 class="anchored" data-anchor-id="chapter-1">Chapter 1:</h4>
<ul>
<li><p>Introduction of the book. Political science methodology is disjointed and not as completely coherent as it should (and needs) to be. King seeks to organize and centralize the political science methodology.</p></li>
<li><p>Statistic Model: a formal representation of the <em>process</em> by which a social system produces output.</p>
<ul>
<li>Since no interesting social systems generate outcomes deterministically, statistical models are assumed to have both systematic and stochastic components.</li>
</ul></li>
<li><p><strong>Inference:</strong> the general process by which one uses observed data to learn about the social system and its outputs.</p></li>
<li><p><strong>Estimation:</strong> the specific procedure by which one obtains estimates of features (usually parameters) of the statistical model.</p></li>
<li><p><strong>The important question for political science research:</strong></p>
<ul>
<li><p>is the underlying process that gives rise to the observed data</p>
<ul>
<li><p>What are the characteristics of the social system that produced these data?</p></li>
<li><p>What changes in known features of the social system might have produced data with different characteristics?</p></li>
<li><p>What is the specific stochastic process driving one’s results?</p></li>
</ul></li>
<li><p>By posing these questions, statistical modelling will be more theoretically relevant and empirically fruitful.</p></li>
</ul></li>
</ul>
</section>
<section id="chapter-2" class="level4">
<h4 class="anchored" data-anchor-id="chapter-2">Chapter 2:</h4>
<ul>
<li><p>Conditional probabilities: describing the uncertainty of an observed or hypothetical event given a set of assumptions about the world.</p></li>
<li><p>Likelihood: a measure of relative uncertainty</p></li>
</ul>
</section>
</section>
<section id="ward-and-ahlquist-chapter-1" class="level3">
<h3 class="anchored" data-anchor-id="ward-and-ahlquist-chapter-1">Ward and Ahlquist Chapter 1:</h3>
</section>
</section>
<section id="lecture" class="level2">
<h2 class="anchored" data-anchor-id="lecture">Lecture:</h2>
<p><a href="https://www.dropbox.com/scl/fi/n8anv7gle9cit9sp8v329/MLE-week1-Handout.pdf?rlkey=38st1zfju2yrsyh0ybgw51zw5&amp;st=8rnk9udt&amp;dl=0">Week 1 Slides</a></p>
<ul>
<li><p>Summary: Run models that find parameters that are most likely to have generated the observed data.</p></li>
<li><p>These models are hard to interpret.</p></li>
<li><p>Goal: Familiarize you with a variety of MLE models used in the social sciences.</p></li>
<li><p>Probability has to sum to 1.</p>
<ul>
<li>We want to find the best estimate <span class="math inline">\(\theta\)</span></li>
</ul></li>
<li><p>Probabilities are:</p>
<ul>
<li><p>Bounded between 1 and 0.</p></li>
<li><p>Sum of probabilities equal 1.</p></li>
<li><p>Trials -&gt; <span class="math inline">\(\infty\)</span></p></li>
<li><p>Mutually exclusive outcomes (Independent).</p></li>
</ul></li>
<li><p>Theta is the only parameter we need to estimate.</p></li>
<li><p>We are still specifying the distribution of the outcome variable. Is it a poisson, bernoulli, normal, etc?</p>
<ul>
<li>this will help us specify which model to use.</li>
</ul></li>
<li><p>L stands for “likelihood function”</p></li>
<li><p>Our goal is to select some <span class="math inline">\(\theta\)</span>* -&gt; <span class="math inline">\(\hat{\theta}\)</span> as to maximize the likelihood of these data being generated. Ways to do this:</p>
<ul>
<li><p>plug in candidate <span class="math inline">\(\theta\)</span>* values</p></li>
<li><p>look at the graph</p></li>
<li><p>optimize function (solve for <span class="math inline">\(\theta\)</span>*)</p></li>
</ul></li>
<li><p>No priors! (that would be Bayesian)</p>
<ul>
<li><p>for our coin flip example, we know .5 is the probability but we only have a set of {H,H,T}</p></li>
<li><p>Without anymore knowledge, the best estimate of <span class="math inline">\(\theta\)</span> is 2/3 or .66.</p></li>
</ul></li>
<li><p>We use ML anytime our dependent variable has a distribution that was not generated by a Gaussian (normal) process.</p>
<ul>
<li><p>see slide 23 for examples.</p></li>
<li><p>We can estimate all of these using OLS but we may hit a few snags and violation assumptions.</p></li>
</ul></li>
<li><p><strong>Working through an ML problem is as follows:</strong></p>
<ul>
<li><p>Build a parametric statistical model</p></li>
<li><p>Define the probability density for <span class="math inline">\(Y_i\)</span> (uncertainty component)</p></li>
<li><p>Define the systematic component (<span class="math inline">\(\theta\)</span>)</p></li>
<li><p>Maximize the likelihood function, given the data.</p></li>
<li><p>Interpret</p></li>
</ul></li>
<li><p>We will pretty much always use log-likelihood.</p>
<ul>
<li><p>why?</p></li>
<li><p>logarithms turn multiplication problems into addition problems.</p></li>
<li><p>likelihood starts to breakdown around N=1000. Log-likelihood does not. Why?</p>
<ul>
<li><p>Our optimization function is multiplying probabilities</p>
<ul>
<li><p>what happens when we multiply a bunch of probabilities?</p>
<ul>
<li>Multiplying thousands of probabilities together is simply not a viable approach without infinite precision.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example:</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare what happens when we increase n. </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>l <span class="ot">&lt;-</span> (p<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">^</span><span class="dv">3</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(l <span class="sc">~</span> p, <span class="at">type=</span><span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p[<span class="fu">which</span>(l <span class="sc">==</span> <span class="fu">max</span>(l))]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>l <span class="ot">&lt;-</span> (p<span class="sc">^</span><span class="dv">40</span>)<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">^</span><span class="dv">20</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(l <span class="sc">~</span> p, <span class="at">type=</span><span class="st">"l"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-1-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>p[<span class="fu">which</span>(l <span class="sc">==</span> <span class="fu">max</span>(l))]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.67</code></pre>
</div>
</div>
</section>
</section>
<section id="homework-1" class="level2">
<h2 class="anchored" data-anchor-id="homework-1">Homework 1:</h2>
<ul>
<li>“Lab 1.R”</li>
</ul>
<section id="probability-v.-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="probability-v.-likelihood">Probability v. Likelihood:</h3>
<ul>
<li><p>Probability = we know which universe we are in, and the probabilities of all events in that one universe add up to 1.</p>
<ul>
<li><p>area under a fixed distribution</p></li>
<li></li>
</ul></li>
<li><p>Likelihood = we know what we observed, and we consider the probability of <strong>what we observed</strong> in any possible universe.</p></li>
</ul>
</section>
</section>
</section>
<section id="topic-2-estimation-looking-under-the-hood" class="level1">
<h1>Topic 2: Estimation: Looking Under the Hood</h1>
<ul>
<li><p><span class="math inline">\((\theta|y_i)=Pr(y|\theta)\)</span></p></li>
<li><p>you have to pick which distribution generated y.</p>
<ul>
<li>assume a probability model.</li>
</ul></li>
<li><p>Remember:</p>
<ul>
<li><p>Traditional probability is a measure of absolute uncertainty. It comes from three axioms:</p>
<ul>
<li>See slide 5.</li>
</ul></li>
<li><p>However, the likelihood is only a relative measure of uncertainty.</p></li>
<li><p>Likelihood model is never absolutely true. It is assumed. We always have to assume a probability model.</p></li>
<li><p>Therefore, we assume that information about <span class="math inline">\(\theta\)</span> comes from</p>
<ul>
<li><p>the data</p></li>
<li><p>assumption about the DGP (assumed probability distribution)</p></li>
</ul></li>
</ul></li>
<li><p>Important to assume outcomes are independent.</p></li>
<li><p>Pick a theta and figure out the probability/distribution of outcomes.</p>
<ul>
<li>higher theta better.</li>
</ul></li>
<li><p>Whath happens when we multiply a bunch of probabilities together?</p>
<ul>
<li><p>they get really small</p>
<ul>
<li><p>so we use logs.</p>
<ul>
<li><p>what happens when we take natural logs of probabilities?</p>
<ul>
<li><p>we get negative numbers - and they will become more negative with more observations.</p>
<ul>
<li><p>take value closes to zero</p>
<ul>
<li>maximizing a negative number. (making it less negative)</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<section id="under-the-hood-of-ml" class="level2">
<h2 class="anchored" data-anchor-id="under-the-hood-of-ml">Under the Hood of ML:</h2>
<ul>
<li><p>We are calculating the derivative of the highest point of the joint distribution.</p></li>
<li><p>Types of optimization methods:</p>
<ul>
<li><p>Numerical</p>
<ul>
<li><p>grid search: Give me a bunch of plausible values of theta and evaluate.</p>
<ul>
<li><p>we will find a global maximum.</p></li>
<li><p>very slow</p></li>
<li><p>computationally becomes crazy very quickly.</p></li>
</ul></li>
</ul></li>
<li><p>Iterative</p>
<ul>
<li><p>this is the “default” one - everyone does this.</p>
<ul>
<li>these are optimization methods of “steepest ascent” or “gradient Ascent” since the algorithm ‘crawls’ up the surface of the likelihood.</li>
</ul></li>
</ul></li>
<li><p>Others…</p></li>
</ul></li>
</ul>
</section>
<section id="measures-of-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="measures-of-uncertainty">Measures of Uncertainty:</h2>
<ul>
<li><p>We have discussed how to obtain the MLE, <span class="math inline">\(\hat{\theta}\)</span>. Yet it is an estimate.</p></li>
<li><p>uncertainty is kinda measured by the curvature.</p></li>
<li><p>standard errors are derived from the negative of the inverse of the second derivative.</p>
<ul>
<li><p>standard errors can’t be negative</p></li>
<li><p>bigger values imply smaller variance.</p></li>
<li><p>bigger negative = more curvature. see equation 8/9 on slides.</p></li>
<li><p>we take take the inverse since larger (more negative) values indicate a sharper curvature, which indicates more certainty in our estimate.</p></li>
</ul></li>
<li><p>We use the Hessian for standard errors in MLE.</p>
<ul>
<li><p>Variance = <span class="math inline">\(-[\textbf{H}^-1]\)</span></p></li>
<li><p>SE: <span class="math inline">\(\sqrt{-[\textbf{H}^-1]}\)</span></p></li>
</ul></li>
</ul>
</section>
<section id="properties-of-ml" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-ml">Properties of ML:</h2>
<ul>
<li><p>Sufficiency: there exists a single <span class="math inline">\(\theta\)</span></p></li>
<li><p>Consistency</p>
<ul>
<li><span class="math inline">\(\hat{\theta} -&gt;\theta as \textit{n} -&gt; \infty\)</span></li>
</ul></li>
<li><p>Asymptotic normality</p></li>
<li><p>Invariance: ML estimate is invariant to functional transformations.</p></li>
<li><p>Efficiency: MLE has the smallest variance (asymptotically), as given by the Cramer-Rao Lower Bound</p></li>
</ul>
</section>
<section id="disadvantages-of-ml" class="level2">
<h2 class="anchored" data-anchor-id="disadvantages-of-ml">Disadvantages of ML:</h2>
<ul>
<li><p>small sample issues. Since ML is asymptotically normal, use Z- rather than t-statistics.</p></li>
<li><p>We know the VCV is biased in small samples</p></li>
<li><p>(not a disadvantage) but most MLE models use z rather than t-stat.</p></li>
<li><p>Have to make distributional assumptions. We must characterize the nature of the statistical experiment.</p></li>
<li><p>some regularity conditions must be met.</p></li>
</ul>
</section>
<section id="information-criteria" class="level2">
<h2 class="anchored" data-anchor-id="information-criteria">Information Criteria:</h2>
<ul>
<li><p>Provides goodness-of-fit with penalization for model complexity</p>
<ul>
<li>basically R^2</li>
</ul></li>
<li><p>Used for feature (i.e., covariate) selection</p></li>
<li><p>Relative, not absolute.</p></li>
<li><p>Data-dependent (sample-dependent, just like log likelihood): numerical values of Y must be identical.</p></li>
<li><p>No hypothesis test</p>
<ul>
<li>no p-value.</li>
</ul></li>
<li><p>Akaike Information criterion (AIC)</p>
<ul>
<li><p><span class="math inline">\(AIC=2k-2ln(L)\)</span></p></li>
<li><p>Lower AIC is preferred model.</p></li>
</ul></li>
<li><p>Schwartz Bayesian information criterion (SBIC)</p>
<ul>
<li><p><span class="math inline">\(SBIC=ln(n)k-2ln(L)\)</span></p></li>
<li><p>Lower SBIC is preferred model</p></li>
<li><p>stronger penalty for over fitting than AIC. Penalty derived from “prior” information.</p></li>
</ul></li>
<li><p><strong>AIC AND BIC ARE NOT TESTS THEY ARE VALUES</strong></p></li>
<li><p>Restricted Mode: Less parameters</p></li>
<li><p>unrestricted model: all parameters.</p></li>
<li><p>likelihood ratio test basically tells you if there is statistically significant difference between two models</p>
<ul>
<li><p>complex v. simple</p>
<ul>
<li>if the complex model doesn’t do that much better than the simple model, then the ratio can help you decide how to change your model specification (may be overkill).</li>
</ul></li>
</ul></li>
</ul>
<section id="lagrange-multiplier-test" class="level3">
<h3 class="anchored" data-anchor-id="lagrange-multiplier-test">LaGrange Multiplier Test:</h3>
<ul>
<li><p>Lots of tests for additional “nuisance” parameters</p></li>
<li><p>Reject H_0: restricted parameter sufficiently improves model fit; should be unrestricted</p></li>
<li><p>Fail to reject H_0:</p></li>
</ul>
</section>
<section id="wald-test" class="level3">
<h3 class="anchored" data-anchor-id="wald-test">Wald Test:</h3>
<ul>
<li><p>Similar to LR tst,</p></li>
<li><p>only estimate unrestricted model.</p></li>
<li><p>if MLE and the restriction are quite different, W becomes large.</p>
<ul>
<li>uncertainty of the coefficients matter.</li>
</ul></li>
<li><p>reject: parameters sufficiently different from the restriction</p>
<ul>
<li>use the unrestricted model.</li>
</ul></li>
</ul>
</section>
<section id="the-3-likelihood-tests" class="level3">
<h3 class="anchored" data-anchor-id="the-3-likelihood-tests">The 3 Likelihood Tests:</h3>
<ul>
<li><p>LR-Test requires estimating two models; may be computationally intensive</p></li>
<li><p>LM -Test requires estimating only a restricted model. Yet finding MLE of constrained model is sometimes difficult. Some LM derivations get around this.</p></li>
<li><p>Wald requires estimating only an unrestricted model. Can also test non-linear restrictions.</p></li>
<li><p>All are asymptotically equivalent</p></li>
<li><p>In small samples, LM is most conservative, then LR, then Wald.</p>
<ul>
<li>small sample - use LM test.</li>
</ul></li>
<li><p>When doing these in code just keep track of which is restricted and unrestricted.</p></li>
</ul>
</section>
</section>
<section id="topic-3-generalized-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="topic-3-generalized-linear-models">Topic 3: Generalized Linear Models</h2>
<ul>
<li><p>GLMs are generalized version of linear regression.</p></li>
<li><p>basically just a bunch of different link functions</p>
<ul>
<li>we will call link function g().</li>
</ul></li>
<li><p>GLMs are linear in parameters.</p></li>
<li><p>Basic order:</p>
<ul>
<li><p>figure out what DV is</p>
<ul>
<li><p>pick a distribution (based on DV)</p>
<ul>
<li>follow the guide basically.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Begin by specifying the random (stochastic) component.</p></li>
<li><p>Normal:</p>
<ul>
<li><p>mean, media, and mode all occur at <span class="math inline">\(\mu\)</span></p></li>
<li><p>The distribution is symmetric about <span class="math inline">\(\mu\)</span> (eliminate any random variable that is skewed)</p></li>
<li><p>Distribution is continuous on the real-number line (eliminates any discrete random variable or bounded random variable)</p></li>
<li><p>central limit theorem</p></li>
<li><p>mean and variance are separable</p>
<ul>
<li>normal distribution is the only one that does this.</li>
</ul></li>
<li><p>Most distributions are not normal!</p></li>
</ul></li>
<li><p>MLE uses z-statistics because it is asymptotically normal.</p></li>
<li><p><span class="math inline">\(\sigma^2\)</span> is not usually reported. It is an ancillary parameter.</p></li>
<li><p>do likelihood test</p></li>
</ul>
</section>
</section>
<section id="topic-4" class="level1">
<h1>Topic 4:</h1>
<ul>
<li><p>rho is the correlation coefficient of epsilon and its prior value.</p></li>
<li><p>when you take the lag of a series the first observation goes away.</p></li>
</ul>
<section id="limited-dependent-variables" class="level2">
<h2 class="anchored" data-anchor-id="limited-dependent-variables">Limited Dependent Variables</h2>
<p>censoring changes shape of the distribution, truncation does not change the distribution for the un-affected range.</p>
<p>Both cause bias (attenuation) and inconsistency. More data does not help us here!</p>
<ul>
<li><p>Censoring</p>
<ul>
<li><p>Censoring is a symptom of our measuring</p></li>
<li><p>is in sample (just a discrete value though)</p></li>
<li><p>“values in a certain range are all transformed to (or reported as) a single value”</p>
<ul>
<li><p>income in surveys often censored ($250,000 or more) since there are so few individuals that would comprise these categories</p></li>
<li><p>ex: if an individual on a survey responds 1, they are a strong democrat, 2,3,4, weak affiliation/independent, 5, strong Republican.</p></li>
</ul></li>
<li><p>This results in lumping/bunching near the censoring point <span class="math inline">\(\tau\)</span></p></li>
<li><p>Estimates are biased (attenuated) since observations farthest from the center of the distribution are restricted to some arbitrary upper (lower limit)</p></li>
<li><p>Three types:</p>
<ul>
<li><p>left (lower) censor</p>
<ul>
<li>anyone with incomes below $20k coded as &lt; $20k</li>
</ul></li>
<li><p>Right upper censor</p>
<ul>
<li>Anyone with incomes above $250k coded as &gt; $250k</li>
</ul></li>
<li><p>Interval censor.</p></li>
</ul></li>
<li><p>We have an observation in the region <em>somewhere</em>…but we don’t know exactly what the true value is”</p></li>
<li><p>Dealing with Censoring</p>
<ul>
<li>censored data can sometimes be though fo as two distributions, one discrete and one continuous.</li>
</ul></li>
</ul></li>
<li><p>Truncation</p>
<ul>
<li><p>truncation effects arise when one attempts to make inferences about a larger population from a sample that is drawn from a distinct sub-population.</p></li>
<li><p>Theory will tell you where the truncation is. To fix truncation, you have to <em>know</em> you have truncation.</p></li>
<li><p><strong>Truncation is a symptom of our sampling</strong></p></li>
<li><p>Observation? what observation?</p></li>
<li><p>produces bias</p>
<ul>
<li>shrinks estimates towards zero.</li>
</ul></li>
<li><p>moves mean away from tau</p></li>
<li><p>shrinks variance too</p></li>
<li><p>sample selection is a form of truncation</p></li>
<li><p>This is in my Y.</p></li>
<li><p>Examples: data on GDP of countries from the World Bank (excludes those that are too poor to report data from their statistical agency)</p>
<ul>
<li>data on car damage from insurer claims (excludes any damage that was below deductible)</li>
</ul></li>
<li><p>Dealing with Truncation:</p>
<ul>
<li><p>truncated normal distirbutions are not full probability distributions since the area under the curve (the CDF) does nto sum to one</p></li>
<li><p>thus we cannot form the likelihood function.</p></li>
</ul></li>
</ul></li>
<li><p>Sample Selection Bias</p>
<ul>
<li><p>a type of truncation</p></li>
<li><p>nonrandom sampling of observations.</p></li>
<li><p>incidental truncation (truncation caused by some other variable, not y itself)</p></li>
<li><p>What’s the population we are generalizing to?</p></li>
</ul></li>
<li><p>Selecting on the DV</p>
<ul>
<li><p>sample selection bias is not the same as selecting on the DV.</p></li>
<li><p>Sampling on DV means deliberately choosing certain y outcomes.</p></li>
</ul></li>
<li><p>Discrete outcomes</p></li>
</ul>
</section>
</section>
<section id="topic-5-binary-choice-models" class="level1">
<h1>Topic 5: Binary Choice Models</h1>
<section id="dichotomous-dependent-variables" class="level2">
<h2 class="anchored" data-anchor-id="dichotomous-dependent-variables">Dichotomous dependent variables</h2>
<ul>
<li><p>We can estimate <span class="math inline">\(\pi_i\)</span> using OLS.</p>
<ul>
<li><p><span class="math inline">\(\pi_i = X_i\beta\)</span> using OLS</p></li>
<li><p>benefits:</p>
<ul>
<li><p>linear interpretation of betas</p></li>
<li><p>simple…much faster since using OLS</p></li>
<li><p>Works well if <span class="math inline">\(X_i\)</span> is also distributed Bernoulli</p></li>
</ul></li>
<li><p>Drawbacks:</p>
<ul>
<li><p>“impossible” predictions; probabilities exceed 0 and 1.</p></li>
<li><p>censoring issue.</p></li>
</ul></li>
<li><p>error not normally distributed.</p></li>
<li><p>error will not have constant variance.</p></li>
</ul></li>
<li><p>LPM is not ideal but its not terrible. You can run it sometimes. What we want is</p>
<ul>
<li><p><span class="math inline">\(\pi_i=g(X_i\beta)\)</span></p></li>
<li><p>we can first express <span class="math inline">\(\pi_i\)</span> as odds</p>
<ul>
<li><p>approach infinity in the positive direction</p></li>
<li><p>approach zero</p></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="logit-log-odds" class="level2">
<h2 class="anchored" data-anchor-id="logit-log-odds">Logit (log-odds)</h2>
<p>logit(<span class="math inline">\(\pi_i\)</span>) &lt; 0</p>
<ul>
<li><p>creates a sigmoid curve.</p>
<ul>
<li>cumulative density function.</li>
</ul></li>
<li><p>near inflection point of zero</p></li>
<li><p>we are solving for <span class="math inline">\(\pi_i\)</span></p></li>
<li><p><strong>GO THROUGH SLIDE 17!</strong></p></li>
<li><p>constant shifts the inflection point in logit.</p></li>
<li><p>big coefficients should have steeper slopes.</p>
<ul>
<li><p>if X is smaller, we get a more stretched out curve.</p>
<ul>
<li>slide 18.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="probit" class="level2">
<h2 class="anchored" data-anchor-id="probit">Probit</h2>
<ul>
<li><p>z-score usage</p></li>
<li><p>different link function</p></li>
<li><p>coefficients in probit model show the increase/decrease in the z-score in response to a change in <span class="math inline">\(x_ik\)</span></p></li>
<li><p>probit usually steeper but not always</p></li>
<li><p>Logit more common in poli sci.</p>
<ul>
<li>prob cause coefficients are bigger</li>
</ul></li>
<li><p>Do not compare logit or probit coefficients - they are different.</p></li>
</ul>
</section>
<section id="latent-variable" class="level2">
<h2 class="anchored" data-anchor-id="latent-variable">Latent Variable</h2>
<ul>
<li><p>think of our Xbetas as a unbounded latent variable</p></li>
<li><p>slide 28.</p></li>
</ul>
</section>
<section id="measures-of-fit-r2" class="level2">
<h2 class="anchored" data-anchor-id="measures-of-fit-r2">Measures of Fit: <span class="math inline">\(R^2\)</span></h2>
<ul>
<li><p>There are several psuedo R^2 measures used for fit of logit/prboit models.</p></li>
<li><p>kinda useless. They aren’t exactly R^2.</p></li>
<li><p>no need to report really.</p></li>
<li><p>AIC/BIC more important.</p></li>
</ul>
</section>
<section id="interpretation" class="level2">
<h2 class="anchored" data-anchor-id="interpretation">Interpretation:</h2>
<ul>
<li><p>Signs matter</p></li>
<li><p>magnitude less so…</p></li>
<li><p>Interpreting single <span class="math inline">\(\beta\)</span> can be done, but be careful about predictions, as log-odds are not a change in Pr(y=1)</p></li>
<li><p>Example: predict Pr(Farm Laborer)</p></li>
<li><p>Odds = 1 mean increase in x does not make Pr(y_i =1) more or less likely</p></li>
<li><p>odds &lt; 1 mean increase in X makes Pr(y_i) less likely</p></li>
<li><p>cant say probability but how much more likely you are to be a farm laborer.</p></li>
<li><p>ODD RATIO SHOWN (this is what we should say when we report this).</p></li>
<li><p>predictions always depend on the value of other covariates. see slide 37.</p></li>
</ul>
</section>
<section id="first-differences-lpm" class="level2">
<h2 class="anchored" data-anchor-id="first-differences-lpm">First Differences (LPM)</h2>
<ul>
<li><p>How much does Y change given a change at X.</p>
<ul>
<li>holding other X’s at some interesting value</li>
</ul></li>
</ul>
</section>
<section id="first-differences-logitprobit" class="level2">
<h2 class="anchored" data-anchor-id="first-differences-logitprobit">First Differences (Logit/Probit)</h2>
<ul>
<li><p>First differences in the logit/probit context do not have all the properties listed previously</p></li>
<li><p>first-differences for logit are given by: see slide 40.</p></li>
</ul>
<section id="read-hanmer-and-kalkan" class="level3">
<h3 class="anchored" data-anchor-id="read-hanmer-and-kalkan">Read Hanmer and Kalkan</h3>
<ul>
<li><p>critique of expected values/expected change: “we are not aware of any theories that are specifically concerned with 48-year old white women who are independent politically and have an in income of $40k - $45k.</p></li>
<li><p>Instead of setting X, we keep all X_i’s at their observed value for each observation, and fix our variable of interest to some value, giving us an expected value for each observation.</p></li>
</ul>
</section>
</section>
<section id="stochastic-simulation" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-simulation">Stochastic Simulation</h2>
<ul>
<li><p>Growing use of simulation techniques designed to show statistical and substantive significance of the results.</p></li>
<li><p>Typically used to make predictions of Y.</p></li>
</ul>
</section>
<section id="example-flores-macias-and-kreps-2013" class="level2">
<h2 class="anchored" data-anchor-id="example-flores-macias-and-kreps-2013">Example: Flores-Macias and Kreps (2013)</h2>
<ul>
<li>When do states adopt war taxes to finance the cost of war?</li>
</ul>
</section>
</section>
<section id="topic-6" class="level1">
<h1>Topic 6:</h1>
<ul>
<li>Need to understand AIC better.</li>
</ul>
<section id="heteroskedasticity" class="level2">
<h2 class="anchored" data-anchor-id="heteroskedasticity">Heteroskedasticity</h2>
<ul>
<li><p>hetero can lead to inefficient estimates in OLS,though coefficients remain unbaised</p></li>
<li><p>often worse in logit and probit models, leading to inconsistent estimates</p></li>
<li><p>we can model out hetero through ‘robust’ standard errors</p></li>
<li><p>or better? if we suspect determinants of hetero, we can model error variance directly through a hetero probit.</p></li>
<li><p>use probit for hetero stuff</p></li>
</ul>
</section>
<section id="omitted-variable-bias" class="level2">
<h2 class="anchored" data-anchor-id="omitted-variable-bias">Omitted variable bias</h2>
<ul>
<li>might be good to use LPM to deal with omitted variable bias</li>
</ul>
</section>
</section>
<section id="topic-7" class="level1">
<h1>Topic 7:</h1>
<section id="ordered-models" class="level2">
<h2 class="anchored" data-anchor-id="ordered-models">Ordered Models:</h2>
<ul>
<li><p>We are estimating <span class="math inline">\(\tau\)</span> cut points.</p></li>
<li><p>Why do we this over OLS?</p>
<ul>
<li>think of latent space. The difference between <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span>; and <span class="math inline">\(\tau_3\)</span> and <span class="math inline">\(\tau_4\)</span> can be different space.</li>
</ul></li>
<li><p>It is just a generalization of the logit/probit to accommodate multiple cut-points.</p></li>
<li><p>trying to find betas and taus such that we are maximizing our probability that this observation is a 1 (or 2, 3,etc - dependent on category).</p></li>
<li><p>Why would I ever run MLE over OLS - this is a comp question</p>
<ul>
<li><p>if cut points are equidistant - just run OLS</p>
<ul>
<li>if not or substantively interesting - MLE</li>
</ul></li>
</ul></li>
<li><p>these are good with vote choice models.</p></li>
<li><p>We get log odds when running this</p>
<ul>
<li><p>and cut points</p>
<ul>
<li>usually reported</li>
</ul></li>
</ul></li>
<li><p>Conceptually, when we plug in values and multiply by the coefficients estimated - we get a value that will tell us where the output falls in relation to the estimated cut points (<span class="math inline">\(\tau\)</span>).</p></li>
<li><p>How are Taus getting estimated?</p>
<ul>
<li>not sure - seems like its just being optimized.</li>
</ul></li>
<li><p>Code needs to use clarify - Andy hasn’t done that yet.</p></li>
<li><p>i.i.a is important. - need to know it better.</p></li>
</ul>
</section>
</section>
<section id="topic-8" class="level1">
<h1>Topic 8:</h1>
<p>we are like basically modeling a rate (count/exposure time)</p>
<section id="count-data" class="level2">
<h2 class="anchored" data-anchor-id="count-data">Count Data:</h2>
<ul>
<li><p>What if we observed multiple bernoulli trials over time?</p>
<ul>
<li>this is count data!</li>
</ul></li>
<li><p>by definition, these data are truncated at 0.</p></li>
<li><p>log count.</p></li>
</ul>
<section id="deriving-the-poisson-function" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-poisson-function">Deriving the Poisson Function</h3>
<ul>
<li><p>think about time. We are segmenting time and seeing the Bernoulli trial with that increasingly smaller time segment.</p></li>
<li><p>Poisson function does this.</p>
<ul>
<li><p>lambda in our example is the expectation of the # of job changes over 5 years.</p></li>
<li><p>y=2 because i wanted to the know the probability you had 2 job changes.</p></li>
</ul></li>
<li><p>Example: Prussian soldiers who died after being kicked by a horse.</p>
<ul>
<li>“law of small numbers”</li>
</ul></li>
<li><p>count data is a time story and is almost always not independent.</p></li>
<li><p>only used count data when you are pushed up to zero.</p></li>
<li><p>when your count data is far from zero, then you prob don’t have to use count models.</p></li>
</ul>
</section>
</section>
<section id="poisson-log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="poisson-log-likelihood">Poisson Log-Likelihood</h2>
<ul>
<li>take the exponents to get rid of any negatives</li>
</ul>
</section>
<section id="interpreting" class="level2">
<h2 class="anchored" data-anchor-id="interpreting">Interpreting</h2>
<ul>
<li><p>IRR</p></li>
<li><p>substantively show you the change in counts.</p></li>
<li><p>concerned about overdispersed</p>
<ul>
<li><p>mean &lt; variance</p></li>
<li><p>basically outliers</p></li>
<li><p>If variable is overdispersed, it was not generated via a Poisson process.</p>
<ul>
<li>betas fine, standard errors are biased downwards.</li>
</ul></li>
</ul></li>
<li><p>i mean you can always run a negative binomial and its best if you have overdispersion but if you don’t then poisson is basically better.</p></li>
</ul>
</section>
<section id="negative-binomial" class="level2">
<h2 class="anchored" data-anchor-id="negative-binomial">Negative Binomial</h2>
<ul>
<li>The negative binomial relaxes the assumption that the conditional mean and variance are equivalent; in other words, it allows for overdispersion.</li>
<li>diff between poisson - basically if you have overdispersion do negative binomial.</li>
<li>variance is larger than the mean.</li>
</ul>
</section>
<section id="interpreting-1" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-1">Interpreting</h2>
<ul>
<li><p>change in log-count</p>
<ul>
<li>easier if you exponential - basically an odds ratio.</li>
</ul></li>
<li><p>Still use clarify.</p></li>
<li><p>first difference</p></li>
<li><p>irr means count doesn’t move up and down.</p></li>
<li><p>if alpha is zero - run poisson.</p>
<ul>
<li>not zero - run negative binomial</li>
</ul></li>
<li><p>null is poisson.</p></li>
</ul>
</section>
</section>
<section id="topic-9" class="level1">
<h1>Topic 9:</h1>
<ul>
<li><p>Recall: <strong>we use anytime we suspect that the data are from a Bernoulli random variable with a large number of trials, with a very low probability of an “event” occurring.</strong></p></li>
<li><p>What if our model has a substantial amount of zeros?</p>
<ul>
<li><p>mean will shift to the left of the zero.</p>
<ul>
<li>you get overdispersion</li>
</ul></li>
</ul></li>
<li><p>we need to figure out if we have lots of zeros which is hard</p>
<ul>
<li><p>depends on theory</p></li>
<li><p>see Netflix example.</p></li>
</ul></li>
<li><p>poisson is negative binomial when alpha is zero.</p></li>
</ul>
<section id="zero-inflated-poisson" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-poisson">Zero-inflated Poisson</h2>
<ul>
<li><p>One way of modeling zeros is to envision two regimes.</p></li>
<li><p>What is <span class="math inline">\(F\)</span> ? -one of the Bernoulli link functions.</p>
<ul>
<li>logit function</li>
</ul></li>
<li><p>Joint probability</p></li>
</ul>
</section>
<section id="zero-inflated-negative-binomial" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-negative-binomial">Zero-inflated Negative Binomial</h2>
<ul>
<li><p>variance is higher than conditional mean</p></li>
<li><p>overdispersion</p></li>
<li><p>do you have overdispersion or excess zeros?</p></li>
</ul>
</section>
<section id="how-do-i-test-for-zero-inflation" class="level2">
<h2 class="anchored" data-anchor-id="how-do-i-test-for-zero-inflation">How do I test for Zero-inflation?</h2>
<ul>
<li><p>lots of debate.</p></li>
<li><p>Poisson vs NB?</p>
<ul>
<li>wald test…is alpha zero or not?</li>
</ul></li>
<li><p>ZIP vs.&nbsp;ZINB</p>
<ul>
<li>account for zero-inflation and include alpha zero test.</li>
</ul></li>
<li><p>Vuong test - bunch of issues with it.</p></li>
</ul>
</section>
<section id="truncated-counts" class="level2">
<h2 class="anchored" data-anchor-id="truncated-counts">Truncated Counts</h2>
<ul>
<li><p>See protest question on midterm</p></li>
<li><p>we don’t observe zero.</p></li>
</ul>
</section>
<section id="censored-count" class="level2">
<h2 class="anchored" data-anchor-id="censored-count">Censored Count</h2>
<ul>
<li>for instance, a respondent codes the number of cars they’ve owned in the past 10 years.</li>
</ul>
</section>
<section id="hurdle-models" class="level2">
<h2 class="anchored" data-anchor-id="hurdle-models">Hurdle Models</h2>
<ul>
<li><p>these assume that there is a unique DGP that determines whether we observe a zero or truncated Poisson</p></li>
<li><p>confusing</p></li>
</ul>
</section>
</section>
<section id="topic-10" class="level1">
<h1>Topic 10</h1>
<section id="duration-models---time-to-event-data" class="level2">
<h2 class="anchored" data-anchor-id="duration-models---time-to-event-data">Duration Models - time to event data</h2>
<ul>
<li><p>The key focus is on the time until an event occurs</p></li>
<li><p>Originates from epidemiology where observations were patients, the start was the onset of the study, and th event was death, disease, recovery, etc.</p></li>
<li><p>Differs from event count models; we’re now trying to see how long until a single event occurs.</p></li>
<li><p>1=failure, 0=no failure</p></li>
<li><p>presence of time-varying covariates is going to influence how we structure our dataset.</p></li>
</ul>
</section>
<section id="kaplan-meier-curves" class="level2">
<h2 class="anchored" data-anchor-id="kaplan-meier-curves">Kaplan-Meier Curves</h2>
<ul>
<li><p>Epidemiology loves this, poli sci not so much</p></li>
<li><p>the proportion/percent surviving is on the vertical axis</p></li>
<li><p>time is on the horizontal axis</p></li>
</ul>
</section>
<section id="duration-models" class="level2">
<h2 class="anchored" data-anchor-id="duration-models">Duration Models</h2>
<ul>
<li><p>Data consist of a cross-section of <em>i</em> units</p></li>
<li><p>units enter at <span class="math inline">\(t_0\)</span></p></li>
<li><p>How does <span class="math inline">\(x\)</span> affect <span class="math inline">\(T_i\)</span> ?</p></li>
<li><p>not a normal distribution!</p></li>
<li><p>Weibull, exponetnial, Gompertz</p></li>
<li><p>Add in explanatory covariates</p></li>
<li><p>Can either be constant across t, or time-varying, depending how the data are structured.</p></li>
<li><p>Semi-proportional model:</p>
<ul>
<li><p>you don’t have to specify the hazard.</p>
<ul>
<li><p>Cox proportional hazard model</p>
<ul>
<li>like everyone runs this.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="the-hazard-rate" class="level2">
<h2 class="anchored" data-anchor-id="the-hazard-rate">The Hazard Rate</h2>
<ul>
<li><p>whats the probability that a unit fails in the next period</p></li>
<li><p>As the time interval between periods, <span class="math inline">\(\delta\)</span>, gets smaller and smaller, we approximate the hazard rate</p></li>
<li><p>the hazard rate is the rate at which units fail after time t, given that they have last up to time t.</p></li>
</ul>
</section>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<ul>
<li>We choose a distribution, then can condition the shape of the distribution by adding covariates.</li>
</ul>
</section>
<section id="exponential-functions" class="level2">
<h2 class="anchored" data-anchor-id="exponential-functions">Exponential Functions</h2>
<ul>
<li><p>Exponential is “memoryless”</p></li>
<li><p>this means that there is a constant risk of failure at all times.</p></li>
</ul>
</section>
<section id="weibull-distribution" class="level2">
<h2 class="anchored" data-anchor-id="weibull-distribution">Weibull Distribution</h2>
<ul>
<li>monotonically increasing, decreasing, or flat hazard rate.</li>
</ul>
</section>
<section id="gamma" class="level2">
<h2 class="anchored" data-anchor-id="gamma">Gamma</h2>
<ul>
<li><p>The most general one</p></li>
<li><p>look at gamma first and test down basically.</p></li>
</ul>
</section>
<section id="interpretation-proportional-hazards" class="level2">
<h2 class="anchored" data-anchor-id="interpretation-proportional-hazards">Interpretation: Proportional Hazards</h2>
<ul>
<li><p>For most models, such as the exponential and Weibull, the metric output is in proportional hazards.</p></li>
<li><p>a one unit increase in x makes the instantaneous rate of failure twice as likely..</p></li>
</ul>
</section>
<section id="cox-model" class="level2">
<h2 class="anchored" data-anchor-id="cox-model">Cox Model</h2>
<ul>
<li><p>It allows users to avoid explicitly specifying the baseline hazard</p></li>
<li><p>We don’t care about the baseline hazard (often, since we only guess at what it might be)</p></li>
<li><p>We just want to see how covariates affect the hazard rate</p></li>
<li><p>Assumption</p>
<ul>
<li><p>covariates affect the relative risk of failure, not the shape of the overall hazard function</p>
<ul>
<li>when testing, make sure to interact all RHS variables with t or g(t). Significant interaction terms indicate violation of PH assumption.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="topic-11" class="level1">
<h1>Topic 11</h1>
<section id="event-history-review" class="level2">
<h2 class="anchored" data-anchor-id="event-history-review">Event History Review:</h2>
<ul>
<li><p>f(t) was the probability distribution of failure. At what <em>time</em> does our observation fail.</p></li>
<li><p>h(t) was the hazard rate; given that an observation has surived to time t, whats the instantaneous rate of failure?</p></li>
<li><p>Need to probably know the hazard rate formula</p></li>
<li><p>Hazard function is the ratio of the PDF of the failure function over the survivor function.</p></li>
</ul>
</section>
<section id="proportional-hazards" class="level2">
<h2 class="anchored" data-anchor-id="proportional-hazards">Proportional Hazards</h2>
<ul>
<li><p>Cox Model need to be worried about proportional hazard models!!!!!!!</p></li>
<li><p>proportional hazard?</p></li>
<li><p>Cox model implicitly assumes that the effects are constant (proportional) across time</p></li>
<li><p>Need to assume affect of X is the same across time</p>
<ul>
<li><p>if this is met you satisfy the proportional hazard.</p></li>
<li><p>Another way of thinking about htis is that the coefficients do not change w.r.t. time (since the hazard is some function of time)</p></li>
</ul></li>
<li><p>The log likelihood function of the Cox model - think of it between actually failed and at risk of failiure. We want to maximize the difference between the two.</p></li>
</ul>
</section>
<section id="time-varying-covariates" class="level2">
<h2 class="anchored" data-anchor-id="time-varying-covariates">Time-Varying Covariates</h2>
<ul>
<li><p>we have assumed our coefficients, x, are constant over time within an individual.</p></li>
<li><p>However, some may vary over time.</p></li>
<li><p>Dont get this confused with time-varying coefficients!</p></li>
<li><p>g(t) is effectively a fix for proportional hazards</p></li>
</ul>
</section>
<section id="schoenfeld-residuals" class="level2">
<h2 class="anchored" data-anchor-id="schoenfeld-residuals">Schoenfeld Residuals</h2>
<ul>
<li><p>These help us assess whether PH is violated.</p></li>
<li><p>want these residuals to be flat</p>
<ul>
<li>if not it means they are changing over time. We violate proportional hazards.</li>
</ul></li>
</ul>
</section>
<section id="cox-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="cox-interpretation">COX interpretation</h2>
<ul>
<li><p>look at exponential coefficient.</p></li>
<li><p>above 1 quicker, lower than 1 slower. see R code.</p></li>
</ul>
</section>
</section>
<section id="topic-12" class="level1">
<h1>Topic 12</h1>
<section id="missing-data" class="level2">
<h2 class="anchored" data-anchor-id="missing-data">Missing Data</h2>
<ul>
<li><p>Respondent chooses not to answer a question about their race/ethniciy</p></li>
<li><p>Venezeula fails to report inflation data for 1980 to IMF</p></li>
<li><p>Missingness in the DV is not a big problem.</p></li>
</ul>
</section>
<section id="mcar" class="level2">
<h2 class="anchored" data-anchor-id="mcar">MCAR</h2>
<ul>
<li><p>Missingness that may occur is missing completely at random</p></li>
<li><p>this almost never happens</p></li>
<li><p>If have MCAR, we still have an unbiased estimate of <span class="math inline">\(\beta\)</span></p></li>
<li><p>Hardest to achieve</p></li>
</ul>
</section>
<section id="mar" class="level2">
<h2 class="anchored" data-anchor-id="mar">MAR</h2>
<ul>
<li><p>Missing at random</p></li>
<li><p>weak</p></li>
<li><p>means that a missing y variable is not determined by y.</p></li>
<li><p>We don’t observe a respondent’s views on abortion if they have a high school level of education or lower</p></li>
<li><p>we have another variable that kind of explains the missingness of another variable.</p></li>
<li><p>will produce biased estimates unless using FIML</p></li>
</ul>
</section>
<section id="mnar" class="level2">
<h2 class="anchored" data-anchor-id="mnar">MNAR</h2>
<ul>
<li><p>Another weak form of missingness is known as missing <strong>not at random</strong></p></li>
<li><p>this is bad!</p></li>
<li><p>Also called observed at random (OAR), or non-ignorable non-response</p></li>
<li><p>i.e.&nbsp;we don’t observe a respondent’s views on abortion if they have particular strong (pro or anti) views on abortion.</p></li>
</ul>
</section>
<section id="important-for-missingness" class="level2">
<h2 class="anchored" data-anchor-id="important-for-missingness">Important for Missingness:</h2>
<ul>
<li><p>we need to know where our missingness is!</p></li>
<li><p>we normally do listwise deletion - also called complete-case analysis.</p></li>
</ul>
</section>
<section id="why-we-dont-really-care-if-our-dv-is-missing" class="level2">
<h2 class="anchored" data-anchor-id="why-we-dont-really-care-if-our-dv-is-missing">Why we don’t really care if our DV is missing</h2>
<ul>
<li><p>you could fill it in but its really just noise. It’ll effectively be our <span class="math inline">\(\hat{y}\)</span></p></li>
<li><p>its just not really a big issue</p></li>
</ul>
</section>
<section id="interpolation" class="level2">
<h2 class="anchored" data-anchor-id="interpolation">Interpolation</h2>
<ul>
<li><p>Another way of handling data is interpolation</p></li>
<li><p>assuming time series data, we might perform linear model interpolation</p></li>
<li><p>whats a good guess of Y, the time before it and the time after.</p></li>
</ul>
<section id="drawbacks" class="level3">
<h3 class="anchored" data-anchor-id="drawbacks">Drawbacks:</h3>
<ul>
<li><p>hard to do outside of time series</p></li>
<li><p>will reduce variation</p>
<ul>
<li>we are interpolating from other values (taking the mean) thus decreasing variance…less extremes.</li>
</ul></li>
</ul>
</section>
</section>
<section id="extrapolation" class="level2">
<h2 class="anchored" data-anchor-id="extrapolation">Extrapolation</h2>
<ul>
<li><p>When data are at the end/beginning of a series, they are sometimes extrapolated</p></li>
<li><p>filling in values at the beginning or end of a series.</p></li>
<li><p>seems a little dodgy</p></li>
</ul>
</section>
<section id="imputation" class="level2">
<h2 class="anchored" data-anchor-id="imputation">Imputation</h2>
<ul>
<li><p>Imputation allows us to fill in missing data, but preserve the inherent uncertainty associated with the filled in value.</p></li>
<li><p>Amelia package by King.</p></li>
</ul>
</section>
<section id="standard-errors" class="level2">
<h2 class="anchored" data-anchor-id="standard-errors">Standard Errors</h2>
<ul>
<li><p>Listwise deletion standard errors are bigger than they probably should be</p></li>
<li><p>interpolation smaller they should be</p></li>
<li><p>imputation probably where SE should be.</p>
<ul>
<li>if we do this and our results don’t really change, thats good. It is more robust.</li>
</ul></li>
</ul>
</section>
</section>
<section id="topic-13" class="level1">
<h1>Topic 13</h1>
<section id="robustness" class="level2">
<h2 class="anchored" data-anchor-id="robustness">Robustness:</h2>
<ul>
<li><p>the results are not specific to the model you estimated.</p></li>
<li><p>Even if i violate assumptions ill still get same results</p></li>
<li><p>your model doesn’t rely on model assumptions</p></li>
<li><p>Read Robustness Tests for Quantitative Research</p></li>
<li><p>why do people run OLS over others?</p>
<ul>
<li>its simple and easy to interpret</li>
</ul></li>
<li><p>Andy’s definition:</p>
<ul>
<li><p>Do your findings (magnitude, direction and statistical significance) change under differing assumptions?</p></li>
<li><p>assumptions: identificiation assumptions, measurement assumptions, estimator assumptions</p>
<ul>
<li><p>identification:</p>
<ul>
<li>how do i know you have a identified a causal mechanism?</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Robustness is NOT ensuring the model is well-specified. It starts with the assumed model you chose and builds from there; you’re not going to show a bad model to begin with.</p></li>
<li><p>Robustness is not about predictive ability or forecasting.</p></li>
<li><p>Robustness checks are a way to check for internal and external validity, but id be wary about saying the model is valid just b/c its robust.</p></li>
<li><p>Are our models ever correct?</p>
<ul>
<li>NO!</li>
</ul></li>
<li><p>What are robust standard errors:</p>
<ul>
<li><p>changing standard errors</p></li>
<li><p>another name: heteroskedastic consistent standard errors</p></li>
<li><p>standard errors are consistent even with heteroskedasticity</p></li>
<li><p>even if we violate spherical assumptions, standard errors will still be correct</p></li>
<li><p>does not alter our <span class="math inline">\(\hat{\beta}\)</span> ’s, but makes our standard errors “robust to” certain misspecifications.</p></li>
<li><p>sandwich operator</p>
<ul>
<li><p>typically standard errors will be a little larger after but more correct.</p></li>
<li><p>spherical error violation? Do this.</p></li>
</ul></li>
<li><p>Cluster-robust:</p></li>
<li><p>normally distributed errors is probably going to be violated</p>
<ul>
<li><p>standard errors will technically be wrong too</p></li>
<li><p>so what do we do?</p>
<ul>
<li>estimate boot-strap standard errors.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Model Variation tests</p>
<ul>
<li>add/change control variables</li>
</ul></li>
<li><p>Functional form changes</p></li>
</ul>
</section>
</section>
<section id="topic-14" class="level1">
<h1>Topic 14</h1>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{neilon2024,
  author = {Neilon, Stone},
  title = {MLE - {Data} 3},
  date = {2024-08-26},
  url = {https://stoneneilon.github.io/notes/Comparative_Behavior/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-neilon2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Neilon, Stone. 2024. <span>“MLE - Data 3.”</span> August 26, 2024. <a href="https://stoneneilon.github.io/notes/Comparative_Behavior/">https://stoneneilon.github.io/notes/Comparative_Behavior/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>