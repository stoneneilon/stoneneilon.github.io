<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stone Neilon">
<meta name="dcterms.date" content="2024-05-15">
<meta name="description" content="Combined notes from Data 1 &amp; 2 with Anand Sokhey and Andy Phillips">

<title>Stone Neilon - Data 1 &amp; 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stone Neilon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/file/d/1RgKR73KnxS3K_AP2nqiTiZZzAL-zJZ2N/view?usp=sharing"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface">Preface</a></li>
  <li><a href="#section-1-descriptive-statistics" id="toc-section-1-descriptive-statistics" class="nav-link" data-scroll-target="#section-1-descriptive-statistics">Section 1: Descriptive Statistics</a>
  <ul class="collapse">
  <li><a href="#mean-average" id="toc-mean-average" class="nav-link" data-scroll-target="#mean-average">Mean (Average)</a></li>
  <li><a href="#median" id="toc-median" class="nav-link" data-scroll-target="#median">Median</a></li>
  <li><a href="#mode" id="toc-mode" class="nav-link" data-scroll-target="#mode">Mode</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance">Variance</a></li>
  <li><a href="#standard-deviation" id="toc-standard-deviation" class="nav-link" data-scroll-target="#standard-deviation">Standard Deviation</a></li>
  <li><a href="#standard-error" id="toc-standard-error" class="nav-link" data-scroll-target="#standard-error">Standard Error</a></li>
  <li><a href="#skewness" id="toc-skewness" class="nav-link" data-scroll-target="#skewness">Skewness</a></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance">Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation">Correlation</a></li>
  </ul></li>
  <li><a href="#section-2-statisticalhypothesis-testing" id="toc-section-2-statisticalhypothesis-testing" class="nav-link" data-scroll-target="#section-2-statisticalhypothesis-testing">Section 2: Statistical/Hypothesis Testing</a>
  <ul class="collapse">
  <li><a href="#t-test" id="toc-t-test" class="nav-link" data-scroll-target="#t-test">t-Test</a>
  <ul class="collapse">
  <li><a href="#probability-distribution" id="toc-probability-distribution" class="nav-link" data-scroll-target="#probability-distribution">Probability Distribution:</a></li>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose:</a></li>
  <li><a href="#uses" id="toc-uses" class="nav-link" data-scroll-target="#uses">Uses:</a></li>
  </ul></li>
  <li><a href="#chi2-test" id="toc-chi2-test" class="nav-link" data-scroll-target="#chi2-test"><span class="math inline">\(\chi^2\)</span> Test</a>
  <ul class="collapse">
  <li><a href="#purpose-1" id="toc-purpose-1" class="nav-link" data-scroll-target="#purpose-1">Purpose:</a></li>
  <li><a href="#uses-1" id="toc-uses-1" class="nav-link" data-scroll-target="#uses-1">Uses:</a></li>
  </ul></li>
  <li><a href="#analysis-of-variance-anova-test" id="toc-analysis-of-variance-anova-test" class="nav-link" data-scroll-target="#analysis-of-variance-anova-test">Analysis of Variance (ANOVA) Test</a>
  <ul class="collapse">
  <li><a href="#purpose-2" id="toc-purpose-2" class="nav-link" data-scroll-target="#purpose-2">Purpose:</a></li>
  <li><a href="#uses-2" id="toc-uses-2" class="nav-link" data-scroll-target="#uses-2">Uses:</a></li>
  </ul></li>
  <li><a href="#f-test" id="toc-f-test" class="nav-link" data-scroll-target="#f-test">F-Test</a>
  <ul class="collapse">
  <li><a href="#purpose-3" id="toc-purpose-3" class="nav-link" data-scroll-target="#purpose-3">Purpose:</a></li>
  <li><a href="#uses-3" id="toc-uses-3" class="nav-link" data-scroll-target="#uses-3">Uses:</a></li>
  </ul></li>
  <li><a href="#p-value" id="toc-p-value" class="nav-link" data-scroll-target="#p-value">p-value</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition:</a></li>
  <li><a href="#purpose-4" id="toc-purpose-4" class="nav-link" data-scroll-target="#purpose-4">Purpose:</a></li>
  <li><a href="#uses-4" id="toc-uses-4" class="nav-link" data-scroll-target="#uses-4">Uses:</a></li>
  <li><a href="#important-notes" id="toc-important-notes" class="nav-link" data-scroll-target="#important-notes">Important Notes:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-3-regression-overviewreview" id="toc-section-3-regression-overviewreview" class="nav-link" data-scroll-target="#section-3-regression-overviewreview">Section 3: Regression Overview/Review</a>
  <ul class="collapse">
  <li><a href="#assumptions-scalar-notation" id="toc-assumptions-scalar-notation" class="nav-link" data-scroll-target="#assumptions-scalar-notation">Assumptions (Scalar Notation)</a>
  <ul class="collapse">
  <li><a href="#assumption-1-epsilon_iis-normally-distributed" id="toc-assumption-1-epsilon_iis-normally-distributed" class="nav-link" data-scroll-target="#assumption-1-epsilon_iis-normally-distributed">Assumption 1: <span class="math inline">\(\epsilon_i\)</span>is normally distributed&nbsp;</a></li>
  <li><a href="#assumption-2-eepsilon_i0" id="toc-assumption-2-eepsilon_i0" class="nav-link" data-scroll-target="#assumption-2-eepsilon_i0">Assumption 2: <span class="math inline">\(E[\epsilon_i]=0\)</span></a></li>
  <li><a href="#assumption-3-varepsilon_isigma2" id="toc-assumption-3-varepsilon_isigma2" class="nav-link" data-scroll-target="#assumption-3-varepsilon_isigma2">Assumption 3: <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span></a></li>
  <li><a href="#assumption-4-covepsilon_iepsilon_j0-foralli-neq-j" id="toc-assumption-4-covepsilon_iepsilon_j0-foralli-neq-j" class="nav-link" data-scroll-target="#assumption-4-covepsilon_iepsilon_j0-foralli-neq-j">Assumption 4: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0 \:\forall\:i \neq j\)</span></a></li>
  <li><a href="#assumption-5-x_i-is-fixed-in-repeated-sampling" id="toc-assumption-5-x_i-is-fixed-in-repeated-sampling" class="nav-link" data-scroll-target="#assumption-5-x_i-is-fixed-in-repeated-sampling">Assumption 5: <span class="math inline">\(x_i\)</span> is fixed in repeated sampling</a></li>
  <li><a href="#assumption-6-sample-regression-model-correctly-specified" id="toc-assumption-6-sample-regression-model-correctly-specified" class="nav-link" data-scroll-target="#assumption-6-sample-regression-model-correctly-specified">Assumption 6: Sample regression model correctly specified&nbsp;</a></li>
  <li><a href="#assumption-7-covepsilon_ix_i0" id="toc-assumption-7-covepsilon_ix_i0" class="nav-link" data-scroll-target="#assumption-7-covepsilon_ix_i0">Assumption 7: <span class="math inline">\(Cov(\epsilon_i,x_i)=0\)</span></a></li>
  <li><a href="#assumption-8-parametric-linearity" id="toc-assumption-8-parametric-linearity" class="nav-link" data-scroll-target="#assumption-8-parametric-linearity">Assumption 8: Parametric Linearity</a></li>
  <li><a href="#assumption-9-x_i-must-vary" id="toc-assumption-9-x_i-must-vary" class="nav-link" data-scroll-target="#assumption-9-x_i-must-vary">Assumption 9: <span class="math inline">\(x_i\)</span> must vary</a></li>
  <li><a href="#assumption-10-n-k" id="toc-assumption-10-n-k" class="nav-link" data-scroll-target="#assumption-10-n-k">Assumption 10: n &gt; k</a></li>
  <li><a href="#assumption-11-no-perfect-multicollinearity" id="toc-assumption-11-no-perfect-multicollinearity" class="nav-link" data-scroll-target="#assumption-11-no-perfect-multicollinearity">Assumption 11: No perfect multicollinearity</a></li>
  </ul></li>
  <li><a href="#assumptions-matrix-notation" id="toc-assumptions-matrix-notation" class="nav-link" data-scroll-target="#assumptions-matrix-notation">Assumptions (Matrix Notation)</a>
  <ul class="collapse">
  <li><a href="#assumption-1-linearity-in-the-parameters" id="toc-assumption-1-linearity-in-the-parameters" class="nav-link" data-scroll-target="#assumption-1-linearity-in-the-parameters">Assumption 1: Linearity in the parameters</a></li>
  <li><a href="#assumption-2-full-rank" id="toc-assumption-2-full-rank" class="nav-link" data-scroll-target="#assumption-2-full-rank">Assumption 2: Full rank</a></li>
  <li><a href="#assumption-3-exogeneity-of-the-independent-variables" id="toc-assumption-3-exogeneity-of-the-independent-variables" class="nav-link" data-scroll-target="#assumption-3-exogeneity-of-the-independent-variables">Assumption 3: Exogeneity of the independent variables</a></li>
  <li><a href="#assumption-4-spherical-disturbances" id="toc-assumption-4-spherical-disturbances" class="nav-link" data-scroll-target="#assumption-4-spherical-disturbances">Assumption 4: Spherical disturbances</a></li>
  <li><a href="#assumption-5-data-generation" id="toc-assumption-5-data-generation" class="nav-link" data-scroll-target="#assumption-5-data-generation">Assumption 5: Data generation</a></li>
  <li><a href="#assumption-6-epsilon-is-normally-distributed" id="toc-assumption-6-epsilon-is-normally-distributed" class="nav-link" data-scroll-target="#assumption-6-epsilon-is-normally-distributed">Assumption 6: <span class="math inline">\(\epsilon\)</span> is normally distributed</a></li>
  </ul></li>
  <li><a href="#formula-for-deriving-beta" id="toc-formula-for-deriving-beta" class="nav-link" data-scroll-target="#formula-for-deriving-beta">Formula for deriving <span class="math inline">\(\beta\)</span></a></li>
  <li><a href="#omega-matrix" id="toc-omega-matrix" class="nav-link" data-scroll-target="#omega-matrix">Omega Matrix</a>
  <ul class="collapse">
  <li><a href="#conversation-with-andy-about-omega-matrix" id="toc-conversation-with-andy-about-omega-matrix" class="nav-link" data-scroll-target="#conversation-with-andy-about-omega-matrix">Conversation with Andy about Omega Matrix:</a></li>
  </ul></li>
  <li><a href="#standard-error-1" id="toc-standard-error-1" class="nav-link" data-scroll-target="#standard-error-1">Standard Error</a></li>
  <li><a href="#heteroscedasticity-spherical-disturbances" id="toc-heteroscedasticity-spherical-disturbances" class="nav-link" data-scroll-target="#heteroscedasticity-spherical-disturbances">Heteroscedasticity (spherical disturbances)</a></li>
  <li><a href="#autocorrelation-spherical-disturbances" id="toc-autocorrelation-spherical-disturbances" class="nav-link" data-scroll-target="#autocorrelation-spherical-disturbances">Autocorrelation (spherical disturbances)</a></li>
  <li><a href="#interactions" id="toc-interactions" class="nav-link" data-scroll-target="#interactions">Interactions:</a>
  <ul class="collapse">
  <li><a href="#interactions-increase-multicollinearity" id="toc-interactions-increase-multicollinearity" class="nav-link" data-scroll-target="#interactions-increase-multicollinearity">Interactions increase multicollinearity</a></li>
  <li><a href="#include-all-constitutive-terms" id="toc-include-all-constitutive-terms" class="nav-link" data-scroll-target="#include-all-constitutive-terms">Include all constitutive terms</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation</a></li>
  <li><a href="#interpretation-continued" id="toc-interpretation-continued" class="nav-link" data-scroll-target="#interpretation-continued">Interpretation continued</a></li>
  <li><a href="#example-case" id="toc-example-case" class="nav-link" data-scroll-target="#example-case">Example Case</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-4-basic-questions-you-are-too-afraid-to-ask" id="toc-section-4-basic-questions-you-are-too-afraid-to-ask" class="nav-link" data-scroll-target="#section-4-basic-questions-you-are-too-afraid-to-ask">Section 4: Basic Questions You Are Too Afraid To Ask</a>
  <ul class="collapse">
  <li><a href="#what-are-moments" id="toc-what-are-moments" class="nav-link" data-scroll-target="#what-are-moments">What are moments?</a>
  <ul class="collapse">
  <li><a href="#mean" id="toc-mean" class="nav-link" data-scroll-target="#mean">Mean</a></li>
  <li><a href="#median-1" id="toc-median-1" class="nav-link" data-scroll-target="#median-1">Median</a></li>
  <li><a href="#mode-1" id="toc-mode-1" class="nav-link" data-scroll-target="#mode-1">Mode</a></li>
  <li><a href="#kurtosis" id="toc-kurtosis" class="nav-link" data-scroll-target="#kurtosis">Kurtosis</a></li>
  </ul></li>
  <li><a href="#why-do-we-use-matrix-algebra-scalar-notation-seems-fine" id="toc-why-do-we-use-matrix-algebra-scalar-notation-seems-fine" class="nav-link" data-scroll-target="#why-do-we-use-matrix-algebra-scalar-notation-seems-fine">Why do we use matrix algebra? Scalar notation seems fine…</a></li>
  <li><a href="#what-is-the-difference-between-covariance-and-correlation" id="toc-what-is-the-difference-between-covariance-and-correlation" class="nav-link" data-scroll-target="#what-is-the-difference-between-covariance-and-correlation">What is the difference between covariance and correlation?</a></li>
  <li><a href="#what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept" id="toc-what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept" class="nav-link" data-scroll-target="#what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept">What do dummy variables do to the line? Why don’t they change the slope? How come they only shift the intercept?</a></li>
  <li><a href="#what-is-the-difference-between-variance-and-standard-deviation" id="toc-what-is-the-difference-between-variance-and-standard-deviation" class="nav-link" data-scroll-target="#what-is-the-difference-between-variance-and-standard-deviation">What is the difference between variance and standard deviation?</a></li>
  <li><a href="#why-is-ordinary-least-squares-ols-so-powerful" id="toc-why-is-ordinary-least-squares-ols-so-powerful" class="nav-link" data-scroll-target="#why-is-ordinary-least-squares-ols-so-powerful">Why is Ordinary Least Squares (OLS) so powerful?</a></li>
  <li><a href="#when-is-ols-not-good-why-use-other-ones" id="toc-when-is-ols-not-good-why-use-other-ones" class="nav-link" data-scroll-target="#when-is-ols-not-good-why-use-other-ones">When is OLS not good? Why use other ones?</a></li>
  <li><a href="#mediator-versus-moderator" id="toc-mediator-versus-moderator" class="nav-link" data-scroll-target="#mediator-versus-moderator">Mediator versus Moderator</a>
  <ul class="collapse">
  <li><a href="#mediator" id="toc-mediator" class="nav-link" data-scroll-target="#mediator">Mediator:</a></li>
  <li><a href="#moderator" id="toc-moderator" class="nav-link" data-scroll-target="#moderator">Moderator:</a></li>
  </ul></li>
  <li><a href="#how-is-standard-error-difference-from-standard-deviation" id="toc-how-is-standard-error-difference-from-standard-deviation" class="nav-link" data-scroll-target="#how-is-standard-error-difference-from-standard-deviation">How is standard error difference from standard deviation?</a></li>
  <li><a href="#are-the-assumptions-about-regression-related-to-the-sample-or-population" id="toc-are-the-assumptions-about-regression-related-to-the-sample-or-population" class="nav-link" data-scroll-target="#are-the-assumptions-about-regression-related-to-the-sample-or-population">Are the assumptions about regression related to the sample or population?</a></li>
  <li><a href="#why-is-it-called-ordinary-least-squares-ols" id="toc-why-is-it-called-ordinary-least-squares-ols" class="nav-link" data-scroll-target="#why-is-it-called-ordinary-least-squares-ols">Why is it called Ordinary Least Squares (OLS)?</a></li>
  <li><a href="#what-is-variance-and-why-is-it-important" id="toc-what-is-variance-and-why-is-it-important" class="nav-link" data-scroll-target="#what-is-variance-and-why-is-it-important">What is variance and why is it important?</a></li>
  <li><a href="#why-do-we-care-so-much-about-standard-errors" id="toc-why-do-we-care-so-much-about-standard-errors" class="nav-link" data-scroll-target="#why-do-we-care-so-much-about-standard-errors">Why do we care so much about standard errors?</a></li>
  <li><a href="#i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do" id="toc-i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do" class="nav-link" data-scroll-target="#i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do">I am having trouble visualizing OLS with many variables. What do I do?</a></li>
  <li><a href="#instrumental-variables-what-are-they-will-i-use-them-should-i-use-them" id="toc-instrumental-variables-what-are-they-will-i-use-them-should-i-use-them" class="nav-link" data-scroll-target="#instrumental-variables-what-are-they-will-i-use-them-should-i-use-them">Instrumental variables, what are they? Will I use them? Should I use them?</a>
  <ul class="collapse">
  <li><a href="#why-should-we-use-an-instrumental-variable" id="toc-why-should-we-use-an-instrumental-variable" class="nav-link" data-scroll-target="#why-should-we-use-an-instrumental-variable">Why should we use an instrumental variable?</a></li>
  </ul></li>
  <li><a href="#should-we-care-about-r2" id="toc-should-we-care-about-r2" class="nav-link" data-scroll-target="#should-we-care-about-r2">Should we care about <span class="math inline">\(R^2\)</span>?</a></li>
  <li><a href="#everyone-talks-about-endogeneity.-what-is-it" id="toc-everyone-talks-about-endogeneity.-what-is-it" class="nav-link" data-scroll-target="#everyone-talks-about-endogeneity.-what-is-it">Everyone talks about endogeneity. What is it?!</a></li>
  <li><a href="#what-is-orthogonal" id="toc-what-is-orthogonal" class="nav-link" data-scroll-target="#what-is-orthogonal">What is orthogonal?</a></li>
  <li><a href="#is-ols-a-causal-inference-model" id="toc-is-ols-a-causal-inference-model" class="nav-link" data-scroll-target="#is-ols-a-causal-inference-model">Is OLS a causal Inference model?</a></li>
  <li><a href="#why-do-we-use-the-normal-distribution" id="toc-why-do-we-use-the-normal-distribution" class="nav-link" data-scroll-target="#why-do-we-use-the-normal-distribution">Why do we use the normal distribution?</a></li>
  </ul></li>
  <li><a href="#section-7-interpretation-deep-dive" id="toc-section-7-interpretation-deep-dive" class="nav-link" data-scroll-target="#section-7-interpretation-deep-dive">Section 7: Interpretation Deep Dive</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#interpreting-control-variables" id="toc-interpreting-control-variables" class="nav-link" data-scroll-target="#interpreting-control-variables">Interpreting Control Variables</a></li>
  </ul></li>
  <li><a href="#section-8-thinking-deeper" id="toc-section-8-thinking-deeper" class="nav-link" data-scroll-target="#section-8-thinking-deeper">Section 8: Thinking Deeper</a>
  <ul class="collapse">
  <li><a href="#unbiased-inferences" id="toc-unbiased-inferences" class="nav-link" data-scroll-target="#unbiased-inferences">Unbiased Inferences</a></li>
  <li><a href="#efficiency" id="toc-efficiency" class="nav-link" data-scroll-target="#efficiency">Efficiency</a></li>
  <li><a href="#bias-vs.-efficiency" id="toc-bias-vs.-efficiency" class="nav-link" data-scroll-target="#bias-vs.-efficiency">Bias vs.&nbsp;Efficiency</a></li>
  </ul></li>
  <li><a href="#section-9-causal-inference" id="toc-section-9-causal-inference" class="nav-link" data-scroll-target="#section-9-causal-inference">Section 9: Causal Inference</a>
  <ul class="collapse">
  <li><a href="#readings-associated" id="toc-readings-associated" class="nav-link" data-scroll-target="#readings-associated">Readings Associated:</a></li>
  <li><a href="#what-is-causal-inference" id="toc-what-is-causal-inference" class="nav-link" data-scroll-target="#what-is-causal-inference">What is Causal Inference?</a></li>
  <li><a href="#the-fundamental-problem-of-causal-inference" id="toc-the-fundamental-problem-of-causal-inference" class="nav-link" data-scroll-target="#the-fundamental-problem-of-causal-inference">The Fundamental Problem of Causal Inference</a></li>
  <li><a href="#components-of-causal-inference" id="toc-components-of-causal-inference" class="nav-link" data-scroll-target="#components-of-causal-inference">Components of Causal Inference</a>
  <ul class="collapse">
  <li><a href="#identification" id="toc-identification" class="nav-link" data-scroll-target="#identification">Identification</a></li>
  </ul></li>
  <li><a href="#identification-strategies" id="toc-identification-strategies" class="nav-link" data-scroll-target="#identification-strategies">Identification Strategies</a></li>
  </ul></li>
  <li><a href="#section-9.5-likelihood" id="toc-section-9.5-likelihood" class="nav-link" data-scroll-target="#section-9.5-likelihood">Section 9.5: Likelihood</a>
  <ul class="collapse">
  <li><a href="#why-do-we-need-maximum-likelihood" id="toc-why-do-we-need-maximum-likelihood" class="nav-link" data-scroll-target="#why-do-we-need-maximum-likelihood">Why do we need maximum likelihood?</a></li>
  <li><a href="#the-basic-structure-of-mle" id="toc-the-basic-structure-of-mle" class="nav-link" data-scroll-target="#the-basic-structure-of-mle">The Basic Structure of MLE</a>
  <ul class="collapse">
  <li><a href="#stochastic-component" id="toc-stochastic-component" class="nav-link" data-scroll-target="#stochastic-component">Stochastic component</a></li>
  <li><a href="#systematic-component" id="toc-systematic-component" class="nav-link" data-scroll-target="#systematic-component">Systematic component</a></li>
  </ul></li>
  <li><a href="#difference-between-mle-and-ols" id="toc-difference-between-mle-and-ols" class="nav-link" data-scroll-target="#difference-between-mle-and-ols">Difference between MLE and OLS?</a></li>
  </ul></li>
  <li><a href="#section-10-generalized-linear-models-glm" id="toc-section-10-generalized-linear-models-glm" class="nav-link" data-scroll-target="#section-10-generalized-linear-models-glm">Section 10: Generalized Linear Models (GLM)</a>
  <ul class="collapse">
  <li><a href="#logit" id="toc-logit" class="nav-link" data-scroll-target="#logit">Logit</a>
  <ul class="collapse">
  <li><a href="#link-function" id="toc-link-function" class="nav-link" data-scroll-target="#link-function">Link Function:</a></li>
  <li><a href="#logit-interpretation" id="toc-logit-interpretation" class="nav-link" data-scroll-target="#logit-interpretation">Logit Interpretation</a></li>
  <li><a href="#can-we-just-use-ols" id="toc-can-we-just-use-ols" class="nav-link" data-scroll-target="#can-we-just-use-ols">Can We Just Use OLS?</a></li>
  </ul></li>
  <li><a href="#probit" id="toc-probit" class="nav-link" data-scroll-target="#probit">Probit</a>
  <ul class="collapse">
  <li><a href="#probit-interpretation" id="toc-probit-interpretation" class="nav-link" data-scroll-target="#probit-interpretation">Probit Interpretation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-11-model-selection" id="toc-section-11-model-selection" class="nav-link" data-scroll-target="#section-11-model-selection">Section 11: Model Selection</a>
  <ul class="collapse">
  <li><a href="#ordinary-least-squares-ols" id="toc-ordinary-least-squares-ols" class="nav-link" data-scroll-target="#ordinary-least-squares-ols">Ordinary Least Squares (OLS)</a></li>
  <li><a href="#stage-least-squares" id="toc-stage-least-squares" class="nav-link" data-scroll-target="#stage-least-squares">2 Stage Least Squares</a></li>
  <li><a href="#logit-1" id="toc-logit-1" class="nav-link" data-scroll-target="#logit-1">Logit</a></li>
  <li><a href="#probit-1" id="toc-probit-1" class="nav-link" data-scroll-target="#probit-1">Probit</a></li>
  <li><a href="#negative-binomial" id="toc-negative-binomial" class="nav-link" data-scroll-target="#negative-binomial">Negative Binomial</a></li>
  <li><a href="#poisson" id="toc-poisson" class="nav-link" data-scroll-target="#poisson">Poisson</a></li>
  <li><a href="#multinomial-logit" id="toc-multinomial-logit" class="nav-link" data-scroll-target="#multinomial-logit">Multinomial Logit</a></li>
  <li><a href="#ordinal-logit" id="toc-ordinal-logit" class="nav-link" data-scroll-target="#ordinal-logit">Ordinal Logit</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Data 1 &amp; 2</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Spring</div>
    <div class="quarto-category">Fall</div>
    <div class="quarto-category">2024</div>
    <div class="quarto-category">Methods</div>
    <div class="quarto-category">2023</div>
  </div>
  </div>

<div>
  <div class="description">
    Combined notes from Data 1 &amp; 2 with Anand Sokhey and Andy Phillips
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://stoneneilon.github.io/">Stone Neilon</a> <a href="https://orcid.org/0009-0006-6026-4384" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.colorado.edu/polisci/people/graduate-students/stone-neilon">
            PhD student of political science @ The University of Colorado Boulder
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 15, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="preface" class="level1">
<h1>Preface</h1>
<p>These notes were compiled in my first year of graduate school. These two classes cover simple to complex linear regression. Additionally, some other methods are discussed such as Logit/Probit, Causal Inference, and time-series. These other methods were only discussed in brief and require their own separate set of notes.</p>
<p>Reminder: These are notes and do not encompass every idea or detail associated with the concepts. They do not (and cannot) replace classes or reading the material.</p>
</section>
<section id="section-1-descriptive-statistics" class="level1">
<h1>Section 1: Descriptive Statistics</h1>
<p>While simple, it is critical you understand these. It is okay if you forget (and you will), just remember to keep reviewing. It helps to walk through the formula step by step. I have provided some commentary on the rationale behind the formula in the sections below.</p>
<p>You should always include descriptive statistics because it convinces your audience that they care. And it provides a guide to interpreting later analysis.</p>
<section id="mean-average" class="level2">
<h2 class="anchored" data-anchor-id="mean-average">Mean (Average)</h2>
<p><span class="math display">\[
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}
\]</span></p>
<p>Above is the formula for mean (average).</p>
</section>
<section id="median" class="level2">
<h2 class="anchored" data-anchor-id="median">Median</h2>
<p><span class="math display">\[
\text{Median} = \begin{cases}       x_{\frac{n+1}{2}} &amp; \text{if } n \text{ is odd} \\      \frac{1}{2}(x_{\frac{n}{2}} + x_{\frac{n}{2} + 1}) &amp; \text{if } n \text{ is even}    \end{cases}
\]</span></p>
<p>Don’t worry about knowing this formula. Arrange everything in order. Select the middle number.</p>
</section>
<section id="mode" class="level2">
<h2 class="anchored" data-anchor-id="mode">Mode</h2>
<ol type="1">
<li><p>Another measure of central tendency</p>
<ol type="1">
<li><p>Mode is simply what number appears the most in our dataset.</p>
<ol type="1">
<li><p>{4,7,3,7,8,1,7,8,9,4,7}</p>
<ol type="1">
<li><p>Our mode would be 7</p>
<ol type="1">
<li>It appears the most.</li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
<li><p>We don’t use mode that much as a measure of central tendency but it still provides some information about the distribution.</p></li>
<li><p>Don’t worry about the formula</p></li>
</ol>
</section>
<section id="variance" class="level2">
<h2 class="anchored" data-anchor-id="variance">Variance</h2>
<p><span class="math display">\[
\text{Variance} (s^2) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}
\]</span></p>
<ol type="1">
<li><p>The average of the squared differences from the Mean.&nbsp;</p></li>
<li><p>Variance is a measure of SPREAD.</p></li>
<li><p>Let’s walk through the formula step by step.</p>
<ol type="1">
<li><p>The <span class="math inline">\(\Sigma\)</span> means to sum all the values together.</p></li>
<li><p><span class="math inline">\((x_i - \bar{x})\)</span></p>
<ol type="1">
<li><p>in this part we are taking each observation and subtracting it by the mean (average).</p></li>
<li><p>Now lets add the square term. <span class="math inline">\((x_i - \bar{x})^2\)</span></p>
<ol type="1">
<li><p>Why do we square this?</p>
<ol type="1">
<li><p>Imagine a number line from 0 to 100. We have some dataset where the mean is 50. Now let’s say one of our observations is 38. 38-50 = -12. See what happens!? We have a negative number. All observations to the left of our mean are negative while all observations to the right of our mean are positive.</p>
<ol type="1">
<li>When we add these all up without the square term, we get ZERO!</li>
</ol></li>
<li><p>Thus we square to accommodate for these canceling out.</p>
<ol type="1">
<li>There are other reasons we square but they aren’t relevant here and this is the main reason.</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Now the <span class="math inline">\(n-1\)</span></p>
<ol type="1">
<li><p>N represents the number of observations.</p>
<ol type="1">
<li><p>Why are we subtracting it by 1?</p>
<ol type="1">
<li><p>If we were calculating the population variance, then we wouldn’t subtract by 1. However, we are pretty much never working with the population. We are always using some samples.&nbsp;</p></li>
<li><p>This part is not super intuitive. BUT, we are using the sample mean, NOT the population mean to calculate the variance.</p>
<ol type="1">
<li><p>We don’t know what the “true” population mean is. We have an estimate of it using our sample. Thus, there is some uncertainty around the sample mean (we don’t know if the sample mean is = to the population mean). To account for this uncertainty we add a -1 to our denominator.&nbsp;</p>
<ol type="1">
<li><p>By subtracting 1 from the denominator this makes the spread a little larger to account for that uncertainty. Think about what happens when we make our denominator smaller compared to if we don’t. Example:</p>
<ol type="1">
<li><p><span class="math inline">\(\frac{16}{4-1}\)</span> vs.&nbsp;<span class="math inline">\(\frac{16}{4}\)</span></p>
<ol type="1">
<li>the one with the <span class="math inline">\(4-1\)</span> denominator will have a larger output and thus account for the uncertainty in our measurement.</li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="standard-deviation" class="level2">
<h2 class="anchored" data-anchor-id="standard-deviation">Standard Deviation</h2>
<p><span class="math display">\[
\text{Sample Standard Deviation} (s) = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}}
\]</span></p>
<ol type="1">
<li><p>Standard deviation is denoted by <span class="math inline">\(s\)</span> or <span class="math inline">\(\sigma\)</span> (lower case sigma).</p></li>
<li><p>Standard deviation represents how far the numbers are from each other.</p>
<ol type="1">
<li><p>Look how similar this equation is compared to the variance equation.</p>
<ol type="1">
<li>The standard deviation is the square root of the variance!</li>
</ol></li>
</ol></li>
<li><p>I won’t explain the whole formula again.</p>
<ol type="1">
<li><p>I will explain why we square root the equation</p>
<ol type="1">
<li>We take the square root to put the output back into its original units. Our output is in the same units as the mean.</li>
</ol></li>
</ol></li>
<li><p>Have a good understanding of standard deviation THEN understand standard error.</p></li>
</ol>
</section>
<section id="standard-error" class="level2">
<h2 class="anchored" data-anchor-id="standard-error">Standard Error</h2>
<p><span class="math display">\[
\text{Standard Error} (\text{SE}) = \frac{s}{\sqrt{n}}
\]</span></p>
<ol type="1">
<li><p>The numerator (s) is standard deviation!</p></li>
<li><p>What is standard error?</p>
<ol type="1">
<li>It is the standard deviation of the means!</li>
</ol></li>
<li><p>OK, so what is the difference between standard deviation and standard error?</p>
<ol type="1">
<li><p>Standard deviation quantifies the variation within a <strong>set</strong> of measurements. (singular)</p></li>
<li><p>Standard error quantifies the variation in the means from <strong>multiple</strong> sets of measurements. (multiple)</p>
<ol type="1">
<li><p>What is confusing is that we can get standard error from one single measurement, even though it describes the means from multiple sets. <strong>Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error.</strong></p>
<ol type="1">
<li><p>Just watch the damn video.</p>
<ol type="1">
<li><a href="https://www.youtube.com/watch?v=A82brFpdr9g" class="uri">https://www.youtube.com/watch?v=A82brFpdr9g</a></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Why do we take the square root of observations in the denominator?&nbsp;</p>
<ol type="1">
<li><p>By dividing by the square root of the sample size, we’re essentially adjusting for the fact that the standard deviation of the sampling distribution of the mean tends to decrease as the sample size increases. This is due to the Central Limit Theorem, which states that the sampling distribution of the sample mean becomes approximately normal as the sample size increases, with a mean equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size. (chat gpt gave me this and it is a kick ass explanation)</p>
<ol type="1">
<li>It is because of this that standard error gets smaller when we have more observations!&nbsp;</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="skewness" class="level2">
<h2 class="anchored" data-anchor-id="skewness">Skewness</h2>
<p>You do not need to know the formula. You just need to know what skewness looks like and how to properly identify when your data is skewed.</p>
<p>When our distribution has no skew, the mean, median, and mode are all the same value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="skewness.jpeg" class="img-fluid figure-img"></p>
<figcaption>Skewness Visualized</figcaption>
</figure>
</div>
<p>Positive skewness is also called “right skew”. Notice where the mean/median/mode are.</p>
<p>Negative skewness is also called “left skew”. Notice where the mean/median/mode are.</p>
</section>
<section id="covariance" class="level2">
<h2 class="anchored" data-anchor-id="covariance">Covariance</h2>
<p><span class="math display">\[
\text{Covariance} (\text{cov}(X, Y)) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1}
\]</span></p>
<ol type="1">
<li><p>A measure of how much two random variables vary together</p>
<ol type="1">
<li>Similar to variance BUT covariance deals with two variables.</li>
</ol></li>
<li><p>Let’s walk through the formula.</p>
<ol type="1">
<li><p>The numerator tells us to take the sum of each observation minus its mean for both variables (x and y). Then multiply.&nbsp;</p>
<ol type="1">
<li>Remember we divide by N-1 because we are using the sample means and not the population means. Thus we have a bit of uncertainty. By subtracting 1, it makes our denominator smaller and subsequently the output larger, representing the greater uncertainty.&nbsp;</li>
</ol></li>
</ol></li>
<li><p>The output of covariance is a number that tells us the <strong>direction</strong> of how these two variables vary together.&nbsp;</p>
<ol type="1">
<li>It does not tell us the strength of the relationship between the two variables.&nbsp;</li>
</ol></li>
<li><p>Look how similar this formula is compared to the variance formula!&nbsp;</p></li>
<li><p>Covariance is influenced by the scale of the variables.&nbsp;</p>
<ol start="2" type="1">
<li>Meaning its hard to read/understand by itself.</li>
</ol></li>
</ol>
</section>
<section id="correlation" class="level2">
<h2 class="anchored" data-anchor-id="correlation">Correlation</h2>
<p><span class="math display">\[
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
\]</span></p>
<ol type="1">
<li><p>This formula looks scary! It actually is super simple!</p>
<ol type="1">
<li><p>Does the numerator look familiar? Look back at the covariance formula! The top IS covariance!</p>
<ol type="1">
<li>The denominator looks familiar too! It is the standard deviation for x and y.</li>
</ol></li>
<li><p>Correlation is similar to covariance but it is more useful.</p>
<ol type="1">
<li><p>We interpret correlation from -1 to 1.</p>
<ol type="1">
<li><p>-1 represents a perfectly negative correlation</p>
<ol type="1">
<li>This will almost never happen</li>
</ol></li>
<li><p>1 represents a perfectly positive correlation</p>
<ol type="1">
<li>this will almost never happen.</li>
</ol></li>
</ol></li>
<li><p>Correlation tell us the direction and strength of a linear relationship between two variables.</p></li>
</ol></li>
</ol></li>
<li><p>Correlation is covariance divided by the product of the two variables standard deviation. So they measure the same thing but one belongs to [-1,1] and the other (covariance) take any value.</p></li>
</ol>
</section>
</section>
<section id="section-2-statisticalhypothesis-testing" class="level1">
<h1>Section 2: Statistical/Hypothesis Testing</h1>
<p>Can we believe this finding? Is this a <em>real</em> relationship? Hypothesis testing helps us answer those questions. There are different types of tests we can do. Below are the “basic” tests. More complicated models will use these tests in some manner. For example, Ordinary Least Squares (OLS) uses a t-test to help us understand whether the relationship (the beta) is statistically significant.</p>
<p>These test correspond with a specific distribution a.k.a a probability distribution. A probability distribution TK</p>
<p>When we use these tests, we are using their respective probability distributions.</p>
<section id="t-test" class="level2">
<h2 class="anchored" data-anchor-id="t-test">t-Test</h2>
<section id="probability-distribution" class="level3">
<h3 class="anchored" data-anchor-id="probability-distribution">Probability Distribution:</h3>
</section>
<section id="purpose" class="level3">
<h3 class="anchored" data-anchor-id="purpose">Purpose:</h3>
</section>
<section id="uses" class="level3">
<h3 class="anchored" data-anchor-id="uses">Uses:</h3>
</section>
</section>
<section id="chi2-test" class="level2">
<h2 class="anchored" data-anchor-id="chi2-test"><span class="math inline">\(\chi^2\)</span> Test</h2>
<section id="purpose-1" class="level3">
<h3 class="anchored" data-anchor-id="purpose-1">Purpose:</h3>
</section>
<section id="uses-1" class="level3">
<h3 class="anchored" data-anchor-id="uses-1">Uses:</h3>
</section>
</section>
<section id="analysis-of-variance-anova-test" class="level2">
<h2 class="anchored" data-anchor-id="analysis-of-variance-anova-test">Analysis of Variance (ANOVA) Test</h2>
<section id="purpose-2" class="level3">
<h3 class="anchored" data-anchor-id="purpose-2">Purpose:</h3>
</section>
<section id="uses-2" class="level3">
<h3 class="anchored" data-anchor-id="uses-2">Uses:</h3>
</section>
</section>
<section id="f-test" class="level2">
<h2 class="anchored" data-anchor-id="f-test">F-Test</h2>
<section id="purpose-3" class="level3">
<h3 class="anchored" data-anchor-id="purpose-3">Purpose:</h3>
</section>
<section id="uses-3" class="level3">
<h3 class="anchored" data-anchor-id="uses-3">Uses:</h3>
</section>
</section>
<section id="p-value" class="level2">
<h2 class="anchored" data-anchor-id="p-value">p-value</h2>
<p>Read very closely! So many people misinterpret this concept!</p>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition:</h3>
<p>A p-value is the probability of observing a test statistic value equal to or more extreme than the value you computed if the null were true.</p>
<p>Alternatively, the p-value is the probability of making a type I/II error.</p>
</section>
<section id="purpose-4" class="level3">
<h3 class="anchored" data-anchor-id="purpose-4">Purpose:</h3>
<p>If we assume the null hypothesis is true, then we could draw the sampling distribution centered around zero. By specifing the null hypothesis we can invoke the central limit theorem.</p>
<p>P-values help us determine how statistically significant our relationship is. Remember that we are using data we have to tell us about data we do not have.</p>
<p><img src="pvalue.png" class="img-fluid" width="684"></p>
</section>
<section id="uses-4" class="level3">
<h3 class="anchored" data-anchor-id="uses-4">Uses:</h3>
<p>The p-value is decided by the researcher. Convention typically sets the p-value at .10 and below. However, .10 is still not ideal, the lower the better.</p>
</section>
<section id="important-notes" class="level3">
<h3 class="anchored" data-anchor-id="important-notes">Important Notes:</h3>
<p>the p-value does not tell us anything substantive. It simply tells us if something is statistically significant.</p>
</section>
</section>
</section>
<section id="section-3-regression-overviewreview" class="level1">
<h1>Section 3: Regression Overview/Review</h1>
<p>The big kahuna. There are other methods but regression is perhaps the most important. It is easy to understand, straightforward, and powerful. Your ability to understand it inside and out will make you smarter in every way. You are Tony Stark and regression is your Iron Man suit.</p>
<section id="assumptions-scalar-notation" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-scalar-notation">Assumptions (Scalar Notation)</h2>
<section id="assumption-1-epsilon_iis-normally-distributed" class="level3">
<h3 class="anchored" data-anchor-id="assumption-1-epsilon_iis-normally-distributed">Assumption 1: <span class="math inline">\(\epsilon_i\)</span>is normally distributed&nbsp;</h3>
<ol type="1">
<li>This assumption is for the purposes of constructing a hypothesis test.&nbsp;</li>
<li>We don’t need this to estimate our beta.&nbsp;</li>
<li>But by assuming the errors are normally distributed then we can run a hypothesis test (t-test) to see if we accept or reject the null hypothesis of beta = 0.&nbsp;</li>
</ol>
</section>
<section id="assumption-2-eepsilon_i0" class="level3">
<h3 class="anchored" data-anchor-id="assumption-2-eepsilon_i0">Assumption 2: <span class="math inline">\(E[\epsilon_i]=0\)</span></h3>
<ol type="1">
<li>The distance between the observed and fitted line is zero. (the residual is zero)&nbsp;
<ol type="1">
<li>This rarely happens <strong>BUT</strong> we estimate our <span class="math inline">\(\hat{\beta}\)</span>’s so that the error is as close to zero as possible.&nbsp;
<ol type="1">
<li>The goal is to have a line of best fit that does this for all observations.&nbsp;</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="assumption-3-varepsilon_isigma2" class="level3">
<h3 class="anchored" data-anchor-id="assumption-3-varepsilon_isigma2">Assumption 3: <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span></h3>
<ol type="1">
<li>This is homoscedasticity (or no heteroskedasticity).&nbsp;
<ol type="1">
<li><p>We want constant error variance.&nbsp;</p></li>
<li><p>Homoscedasticity visualized:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/image002.png" class="img-fluid figure-img" width="500"></p>
<figcaption>Homoscedasticity visualized (From Gujarati &amp; Porter)</figcaption>
</figure>
</div></li>
</ol></li>
</ol>
</section>
<section id="assumption-4-covepsilon_iepsilon_j0-foralli-neq-j" class="level3">
<h3 class="anchored" data-anchor-id="assumption-4-covepsilon_iepsilon_j0-foralli-neq-j">Assumption 4: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0 \:\forall\:i \neq j\)</span></h3>
<ol type="1">
<li><p>This represents no autocorrelation&nbsp;</p></li>
<li><p>The disturbances of i and j are NOT correlated</p></li>
<li><p><span class="math inline">\(\forall\)</span> means “for all”</p></li>
</ol>
</section>
<section id="assumption-5-x_i-is-fixed-in-repeated-sampling" class="level3">
<h3 class="anchored" data-anchor-id="assumption-5-x_i-is-fixed-in-repeated-sampling">Assumption 5: <span class="math inline">\(x_i\)</span> is fixed in repeated sampling</h3>
<ol type="1">
<li>X values are independent fo the error term</li>
</ol>
</section>
<section id="assumption-6-sample-regression-model-correctly-specified" class="level3">
<h3 class="anchored" data-anchor-id="assumption-6-sample-regression-model-correctly-specified">Assumption 6: Sample regression model correctly specified&nbsp;</h3>
<ol type="1">
<li><p>Our sample regression equation correctly identifies the variables in the theoretical population regression model.&nbsp;</p></li>
<li><p>We include all relevant confounding variables.&nbsp;</p></li>
<li><p>No omitted variable bias.&nbsp;</p></li>
</ol>
</section>
<section id="assumption-7-covepsilon_ix_i0" class="level3">
<h3 class="anchored" data-anchor-id="assumption-7-covepsilon_ix_i0">Assumption 7: <span class="math inline">\(Cov(\epsilon_i,x_i)=0\)</span></h3>
<ol type="1">
<li>covariance between residuals and parameters is equal to zero</li>
</ol>
</section>
<section id="assumption-8-parametric-linearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-8-parametric-linearity">Assumption 8: Parametric Linearity</h3>
<ol type="1">
<li><p>Linear in the parameters.&nbsp;</p></li>
<li><p>We do NOT raise the betas to a power.&nbsp;</p>
<ol type="1">
<li><p>We can raise the variables (x’s) to a power and it remains linear.&nbsp;</p></li>
<li><p>Note: Logit models add a “link function”. This line is not linear but it is still a linear relationship.</p></li>
</ol></li>
</ol>
</section>
<section id="assumption-9-x_i-must-vary" class="level3">
<h3 class="anchored" data-anchor-id="assumption-9-x_i-must-vary">Assumption 9: <span class="math inline">\(x_i\)</span> must vary</h3>
<ol type="1">
<li>Duh. You can’t do anything if your X variable doesn’t vary.</li>
</ol>
</section>
<section id="assumption-10-n-k" class="level3">
<h3 class="anchored" data-anchor-id="assumption-10-n-k">Assumption 10: n &gt; k</h3>
<ol type="1">
<li>This relates to degrees of freedom.</li>
<li>We need more operations than we have parameters or else we do not have enough information to test a relationship.</li>
</ol>
</section>
<section id="assumption-11-no-perfect-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-11-no-perfect-multicollinearity">Assumption 11: No perfect multicollinearity</h3>
<ol type="1">
<li>We wouldn’t include a column for male and a column for female because that would be perfect multicollinearity.&nbsp;</li>
<li>Multicollinearity is not a big issue (it is natural there will be some level of collinearity between our variables).&nbsp;
<ol type="1">
<li>BUT perfect multicollinearity is bad and we do not want it.&nbsp;</li>
</ol></li>
<li>Multicollinearity can (in some cases) disappear as we increase the number of observations.</li>
<li>This is easy to see in matrix algebra or an excel sheet.</li>
</ol>
</section>
</section>
<section id="assumptions-matrix-notation" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-matrix-notation">Assumptions (Matrix Notation)</h2>
<p>Note: These assumptions are the EXACT same assumptions listed above. The difference is in notation. Why do we do this? I answer this later, but basically its a different way to write math that is more concise and easier to understand. <strong>We use matrix algebra/notation because as our model gets bigger, scalar notation gets more complicated to read/keep track of.</strong></p>
<p>Why do we bold letters? Bold letters represent matrices.</p>
<section id="assumption-1-linearity-in-the-parameters" class="level3">
<h3 class="anchored" data-anchor-id="assumption-1-linearity-in-the-parameters">Assumption 1: Linearity in the parameters</h3>
<p><span class="math inline">\(y_i=x_{i1}\beta_1+x_{i2}\beta_2+...+X_{iK}\beta_K+\epsilon_i\)</span></p>
<ol type="1">
<li>The model specifies a linear relationship between y and <strong>X</strong></li>
<li>Do not raise the <span class="math inline">\(\beta\)</span> to a power.</li>
</ol>
</section>
<section id="assumption-2-full-rank" class="level3">
<h3 class="anchored" data-anchor-id="assumption-2-full-rank">Assumption 2: Full rank</h3>
<ol type="1">
<li><strong>X</strong> is an n x K matrix with rank K
<ol type="1">
<li>There is no exact linear relationship among variables&nbsp;</li>
<li>Sometimes called the “identification condition”</li>
</ol></li>
<li>What is “rank”?
<ol type="1">
<li><p>It is the number of linearly independent columns&nbsp;</p>
<ol type="1">
<li>If the number of independent columns is equal to the total number of columns then the matrix is full rank.</li>
</ol></li>
</ol></li>
<li>This assumption relates to the scalar assumption of <strong>no perfect multicollinearity.</strong></li>
</ol>
</section>
<section id="assumption-3-exogeneity-of-the-independent-variables" class="level3">
<h3 class="anchored" data-anchor-id="assumption-3-exogeneity-of-the-independent-variables">Assumption 3: Exogeneity of the independent variables</h3>
<ol type="1">
<li><p>The independent variables contain <strong>NO</strong> predictive information about</p></li>
<li><p>The expected value of is not a function of the independent variables at any observation (including i):</p>
<ol type="1">
<li><p><span class="math inline">\(E[\epsilon_i|x_{j1},x_{j2},...,x_{jK}=0\)</span></p>
<ol type="1">
<li><span class="math inline">\(E[\epsilon_i|\textbf{X}]=0\)</span></li>
</ol></li>
</ol></li>
<li><p>What does this mean?</p>
<ol type="1">
<li><p>The independent variables are not influenced by the error term or any other unobserved factors in the model.</p></li>
<li><p>The X variable does not depend on the Y variable (reverse causality). We can’t have the Y variable influencing our regressors (that would be endogeneity)</p></li>
</ol></li>
</ol>
</section>
<section id="assumption-4-spherical-disturbances" class="level3">
<h3 class="anchored" data-anchor-id="assumption-4-spherical-disturbances">Assumption 4: Spherical disturbances</h3>
<ol type="1">
<li><p>No autocorrelation</p>
<ol type="1">
<li><span class="math inline">\(E[\text{Cov}(i,j|\mathbf{X})] = 0\: \forall \:i=j\)</span></li>
</ol></li>
<li><p>Assumed homoscedasticity</p>
<ol type="1">
<li><p><span class="math inline">\(E[\text{Var}(i|\mathbf{X})] = 2\: \forall\: i=1,2,\ldots,n\)</span></p></li>
<li><p>Why assumed?</p>
<ol type="1">
<li><p>Some level of heteroscedasticity is not fatal</p></li>
<li><p>We can fix it. But homoscedasticity is always preferable.</p></li>
</ol></li>
</ol></li>
<li><p>These two assumptions can be written mathematically into one single assumption using matrix algebra:</p>
<p><img src="sphericalpic.png" class="img-fluid" width="347"></p>
<ol type="1">
<li><p>The off-diagonal (the zeros) represent autocorrelation</p>
<ol type="1">
<li>if these are not zero (or at least very close) we have autocorrelation</li>
</ol></li>
<li><p>The main-diagonal (the variance) represents our homoscedasticity assumption</p>
<ol type="1">
<li>If these values along the main diagonal are not the same or at least very close, then we have heteroscedasticity.</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="assumption-5-data-generation" class="level3">
<h3 class="anchored" data-anchor-id="assumption-5-data-generation">Assumption 5: Data generation</h3>
<ol type="1">
<li><p>The data generating process of <strong>X</strong> and <span class="math inline">\(\epsilon\)</span> are independent.</p></li>
<li><p><strong>X</strong> is generated by a non-stochastic process</p></li>
<li><p>this assumption allows us to say “conditional on X”</p></li>
<li><p>This assumption is a bit confusing to me.</p>
<ol type="1">
<li><p>From my understanding, we want our X values to be fixed. We then take samples to see how our y values vary based on the fixed values of X.&nbsp;</p></li>
<li><p>Let’s say you want to predict annual income based on years of experience. Your manager gave you three lists of employees with their annual income. Each list corresponds to a particular experience level — let’s say 3 years, 6 years, and 10 years of experience respectively. Each list contains data on 50 employees. As you can see, the x-values are fixed(3, 6, 10), but have repeated samples (50 data points per sample). This is what is known as Non-stochastic regressors</p></li>
</ol></li>
</ol>
</section>
<section id="assumption-6-epsilon-is-normally-distributed" class="level3">
<h3 class="anchored" data-anchor-id="assumption-6-epsilon-is-normally-distributed">Assumption 6: <span class="math inline">\(\epsilon\)</span> is normally distributed</h3>
<ol type="1">
<li><p>This is useful for constructing our hypothesis tests and test statistics&nbsp;</p></li>
<li><p>Technically, we don’t need this for estimating our beta, just uncertainty surrounding it.</p></li>
</ol>
</section>
</section>
<section id="formula-for-deriving-beta" class="level2">
<h2 class="anchored" data-anchor-id="formula-for-deriving-beta">Formula for deriving <span class="math inline">\(\beta\)</span></h2>
<ul>
<li><p>Problem: We have two missing terms, 𝜷 and 𝛆. Knowing one of these will tell us the line. But since we don’t know either of these terms, how do we find it out?&nbsp;</p></li>
<li><p>We have to solve for beta. Solving for beta in Ordinary Least Squares (OLS) requires us to find a line of best fit that minimizes the unexplained difference (the error).&nbsp;</p></li>
<li><p>To do this we take the sum of the squared residuals&nbsp;&nbsp;</p>
<ul>
<li><p>It may help to understand this through the formula of the residual.&nbsp;</p>
<ul>
<li><p>First the residual is the amount our actual observed value differs from the predicted value (This is in matrix notation).&nbsp;</p>
<ul>
<li><p><span class="math inline">\((y-\textbf{X}\beta_0)\)</span></p>
<ul>
<li>y is our observed value and the <span class="math inline">\(\textbf{X}\beta_0\)</span> is our predicted value (the line of best fit).&nbsp;</li>
</ul></li>
<li><p>WE SQUARE THIS! SO NOW:&nbsp;</p>
<ul>
<li><p><span class="math inline">\((y-\textbf{X}\beta_0)'(y-\textbf{X}\beta_0)\)</span></p>
<ul>
<li><p>The (’) means transpose. It is matrix notation that allows us to multiply these two matrices (vectors).&nbsp;</p></li>
<li><p>Why do we square?&nbsp;</p>
<ul>
<li>We square the residuals for a bunch of reasons. Mainly: if we don’t, the residuals (Both positive and negative) cancel out.&nbsp;</li>
</ul></li>
</ul></li>
<li><p>Multiplying this through, we get:</p></li>
<li><p><span class="math inline">\(y'y-y'\textbf{X}\hat{\beta_0}'\textbf{X}'y+\hat{\beta_0}'\textbf{X}'\textbf{X}\)</span></p></li>
<li><p>You collect the terms and simply.&nbsp;</p></li>
<li><p><a href="https://www.youtube.com/watch?v=K_EH2abOp00" class="uri">https://www.youtube.com/watch?v=K_EH2abOp00</a> see for more</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>SO NOW: we want to find a line <span class="math inline">\(\hat{\beta_0}\)</span> such that the derivative (the tangent) is set to 0 aka the minimum, hence LEAST squares. Remember, we do not know the Beta.&nbsp;</p>
<ul>
<li><p>We set to zero to find the critical point (the minimum)&nbsp;</p></li>
<li><p>Taking the partial derivative with respect to beta, you’re essentially finding the point where the error function is not changing with respect to changes in beta. Where the slope of the error function with respect to beta is zero.&nbsp;</p></li>
<li><p><img src="beta-1.png" class="img-fluid" width="305"></p></li>
<li><p>Figure b is a visual representation of what this looks like when we set our minimum. We are finding the tangent line of the function that is equal to zero!</p></li>
</ul></li>
<li><p>The formula is</p>
<ul>
<li><p><span class="math inline">\(\hat{\beta}=(\textbf{X'X})^{-1}\textbf{X}'y\)</span></p>
<ul>
<li>this gives us the line of best fit. This is the formula R uses to calculate the beta/line.</li>
</ul></li>
</ul></li>
<li><p><strong>Controlling for other variables:</strong></p>
<ul>
<li><p><img src="beta-2.png" class="img-fluid" width="514"></p></li>
<li><p>Compare figure 3.3 to figure 3.2. They are the same thing. However in 3.3 we have added an additional dimension because of the additional variable. What we are doing remains the same however we now just have more dimensions and we are still trying to find the minimum of that parabola(?) plane(?)</p></li>
</ul></li>
</ul>
</section>
<section id="omega-matrix" class="level2">
<h2 class="anchored" data-anchor-id="omega-matrix">Omega Matrix</h2>
<ul>
<li><p>What the hell is an omega matrix <span class="math inline">\(\Omega\)</span>?</p>
<ul>
<li><p>The omega matrix is literally 𝛆𝛆’&nbsp;</p>
<ul>
<li><p>The error times its transpose.</p></li>
<li><p>We obviously can’t solve this without knowing what the errors are.&nbsp;</p>
<ul>
<li>This produces the variance covariance matrix (VCV) AKA covariance matrix of the errors.&nbsp;</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Why do we care about this matrix?&nbsp;</p>
<ul>
<li><p>We need the residuals to get our standard errors.&nbsp;</p></li>
<li><p>Additionally, this matrix is used to test our assumptions about the model. Specifically whether our model has autocorrelation and heteroskedasticity.&nbsp;</p></li>
<li><p><img src="sphericalpic.png" class="img-fluid"></p></li>
<li><p>This is basically what the omega matrix looks like. This photo however is what we want that omega matrix to look like (ours won’t always look like that). But we want the off diagonals to be zero (or effectively zero) and we want the main diagonal to be constant.&nbsp;</p>
<ul>
<li><p>If off-diagonal values are &gt; 0&nbsp;</p>
<ul>
<li>We have autocorrelation</li>
</ul></li>
<li><p>If main-diagonal values are not the same at each value</p>
<ul>
<li>We have heteroskedasticity.&nbsp;</li>
</ul></li>
</ul></li>
<li><p>NOTE: our omega matrix will NEVER be perfectly spherical.</p></li>
</ul></li>
</ul>
<section id="conversation-with-andy-about-omega-matrix" class="level3">
<h3 class="anchored" data-anchor-id="conversation-with-andy-about-omega-matrix">Conversation with Andy about Omega Matrix:</h3>
<p>I emailed Andy about this and figured it might be beneficial to include it here.</p>
<p><strong><em>Stone:</em></strong></p>
<p><em>I am looking back on your “Roll your own standard errors.r”. I see how the residual maker is part of the variance formula.</em></p>
<p><em># the formula for s^2 is e’e/(n-K)</em></p>
<p><em>s.2&nbsp;&lt;- (t(e)%*%e)/(length(y) - ncol(X))</em></p>
<p><em>##I ran this code individually and it gave me a scalar. I assume this is the sum of the squared error (SSE)?</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>Yes, divided by degrees of freedom, so it’s a variance</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>So, autocorrelation and heteroscedasticity manifest through the variance. Then: vcv &lt;- s.2*(solve(t(X)%*%X))</em></p>
<ul>
<li><em>This is our VCV of the X’s and then we take the square root of the diagonal to get our SE.</em></li>
</ul>
<p><em>We use the omega (and the assumptions of no spherical errors) to derive the equation for the SE (equation above). However, if we have spherical disturbances and use the same equation to derive our standard errors then our standard errors are wrong.</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>Yes,if there are non-spherical disturbances than our standard VCV above isn’t technically correct anymore b/c the equation doesn’t simplify to that.</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>Then the omega matrix (and its assumptions) is related to the population error. And thus, when we get a sample with spherical disturbances that does not match our expectations of the population error of no spherical disturbances, we then must fix it. Right?</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>Right…we can’t know what the population Omega is, but we can get a good guess based off our sample Omega matrix</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>So, if we switch the order, e%*%t(e) gives us the matrix of errors (WHICH IS NOT THE OMEGA MATRIX(?)). We want our matrix of errors to look like the omega matrix. It never will but we use the various tests to figure out the level of spherical errors that are present in this matrix.</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>The matrix of the errors IS the Omega matrix, which is the variance covariance matrix of the errors (note the other VCV for our X’s above). It’ll never be spherical perfectly but our assumptions are about expectations so it just needs to be consistently a problem (e.g., 2 errors can be correlated, but it’s only a problem if&nbsp;on average there’s a correlation between errors)</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>Then when we detect spherical disturbances, we purge it or do whatever (FGLS, Robust/cluster SE), which then fixes our variance and then fixes our SE? Do I have all this right? This all feels kind of magical.</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>If you’re running FGLS you’re using the info in the residual Omega to adjust both your SE’s and coefficients. If you’re correcting just your SEs you’re basically adjusting the standard SE&nbsp;formula&nbsp;to account for the pattern you want to correct for.</em></p>
</section>
</section>
<section id="standard-error-1" class="level2">
<h2 class="anchored" data-anchor-id="standard-error-1">Standard Error</h2>
<ol type="1">
<li><p>Standard Errors are not intuitive to me…but they are important</p></li>
<li><p><strong>Standard error is the standard deviation of the means.</strong>&nbsp;</p>
<ol type="1">
<li><p>The standard error quantifies the variation in the means from multiple sets of measurements.&nbsp;</p>
<ol type="1">
<li><p>What gets often confused is that the standard error can be estimated from a SINGLE set of measurements, even though it describes the means from multiple sets. Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error.&nbsp;</p>
<ol type="1">
<li>It is an estimate!</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>It is worth discussing standard deviation and its formula.</p>
<ol type="1">
<li><p><span class="math inline">\(\sigma = \sqrt{\frac{\sum_{i=1}^{N}(x_i - \mu)^2}{N}}\)</span></p>
<ol type="1">
<li><p>Above is the formula for standard deviation.&nbsp;</p>
<ol type="1">
<li>Note: the similarity of this to variance.&nbsp;</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Standard error formula is:</p>
<ol type="1">
<li><p><span class="math inline">\(SE = \frac{\sigma}{\sqrt{n}}\)</span></p>
<ol type="1">
<li>The s is the standard deviation! So all that in the standard deviation formula above is IN the standard error formula.&nbsp;</li>
</ol></li>
</ol></li>
<li><p>Why are standard errors important?&nbsp;</p>
<ol type="1">
<li><p>Need for precision of our coefficients</p>
<ol type="1">
<li>How precise of a claim can we make?&nbsp;</li>
</ol></li>
</ol></li>
<li><p>We make assumptions about our standard errors.**TK&nbsp;&nbsp;</p>
<ol type="1">
<li><p>They are normally distributed.&nbsp;</p>
<ol type="1">
<li>Not a big deal.&nbsp;</li>
</ol></li>
<li><p>Assuming the error term is independent and identically distributed.&nbsp;</p>
<ol type="1">
<li><p>Each individual error term follows the same distribution and is uncorrelated with each other.</p>
<ol type="1">
<li>Knowing an error term does not tell you anything about another error term.</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Autocorrelation/heteroskedasticity do not bias our coefficient.&nbsp;</p></li>
<li><p>Presence of autocorrelation leads to an underestimation of the true standard errors.&nbsp;</p>
<ol type="1">
<li>Increases the possibility of making a type 1 error.&nbsp;</li>
</ol></li>
<li><p>Standard errors are useful for creating confidence intervals.&nbsp;</p></li>
</ol>
</section>
<section id="heteroscedasticity-spherical-disturbances" class="level2">
<h2 class="anchored" data-anchor-id="heteroscedasticity-spherical-disturbances">Heteroscedasticity (spherical disturbances)</h2>
<ol type="1">
<li>What is heteroskedasticity?&nbsp;
<ol type="1">
<li><p>Non-constant error variance.&nbsp;</p>
<ol type="1">
<li><p>See the picture at the beginning of the document of what homoscedasticity looks like. Heteroscedasticity is the opposite of that.&nbsp;</p>
<ol type="1">
<li><p>Think of our errors having a pattern or they “fan out”</p></li>
<li><p>Using the omega matrix again, it is when each value along the main diagonal is different.&nbsp;</p></li>
</ol></li>
</ol></li>
</ol></li>
<li>THIS AFFECTS OUR STANDARD ERROR!&nbsp;
<ol type="1">
<li>How?&nbsp;</li>
</ol></li>
<li>What does it do to our estimate?&nbsp;
<ol type="1">
<li><p>Our coefficient is unchanged.</p></li>
<li><p>However the efficiency of our model is influenced.&nbsp;</p></li>
</ol></li>
</ol>
</section>
<section id="autocorrelation-spherical-disturbances" class="level2">
<h2 class="anchored" data-anchor-id="autocorrelation-spherical-disturbances">Autocorrelation (spherical disturbances)</h2>
<ol type="1">
<li><p>What is autocorrelation?&nbsp;</p></li>
<li><p>THIS AFFECTS OUR STANDARD ERROR!&nbsp;</p>
<ol type="1">
<li>How?&nbsp;</li>
</ol></li>
</ol>
</section>
<section id="interactions" class="level2">
<h2 class="anchored" data-anchor-id="interactions">Interactions:</h2>
<p>Interactions are used when we believe the relationship is <em>conditional</em>. For example, X causes Y, only if Z is active. The effect of X on Y depends on the level of Z. In other words, the effect of one independent variable on the dependent variable is conditioned by another variable.</p>
<p>To accommodate a relationship such as this one, we multiply the two variables together rather than adding. Goal: deterime whether the function <span class="math inline">\(\textbf{E}[y|x]\)</span> is constant for different subsamples.</p>
<section id="interactions-increase-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="interactions-increase-multicollinearity">Interactions increase multicollinearity</h3>
<p>THIS IS OKAY.</p>
</section>
<section id="include-all-constitutive-terms" class="level3">
<h3 class="anchored" data-anchor-id="include-all-constitutive-terms">Include all constitutive terms</h3>
<p>It is essential that you include the constitutive terms and the interaction in the model.</p>
<p><strong>Wrong:</strong> Turnout = Age + Age*Race</p>
<p><strong>Correct:</strong> Turnout = Age + Race + Age*Race</p>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">Interpretation</h3>
<p>When interactions are dichotomous or categorical, interpretation is relatively easy. When the interaction includes a continuous variable, interpretation from the table becomes difficult.</p>
</section>
<section id="interpretation-continued" class="level3">
<h3 class="anchored" data-anchor-id="interpretation-continued">Interpretation continued</h3>
<p>Focusing on individual coefficients for interaction is not advisable. When a model includes interaction terms, the association between the focal predictor and the outcomes is no longer captured by a single coefficient, but rather by a combination of coefficients. See Rohrer &amp; Bundock Models as Prediction Machines: How to Convert Confusing Coefficients into Clear Quantities.</p>
</section>
<section id="example-case" class="level3">
<h3 class="anchored" data-anchor-id="example-case">Example Case</h3>
<p>Let’s use the example from applied econometrics.</p>
<p>Let’s look at the log of wages log(inc95) as our dependent variable and education as our main independent variable. We first restrict our analysis to only females.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># bivariate regression </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(inc95) <span class="sc">~</span> yeared, <span class="at">data =</span> <span class="fu">subset</span>(nlsy, female <span class="sc">==</span> <span class="dv">1</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(inc95) ~ yeared, data = subset(nlsy, female == 
    1))

Residuals:
    Min      1Q  Median      3Q     Max 
-5.1738 -0.3063  0.0281  0.3476  2.0734 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 8.036830   0.109053   73.70   &lt;2e-16 ***
yeared      0.145182   0.007943   18.28   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5913 on 1608 degrees of freedom
Multiple R-squared:  0.172, Adjusted R-squared:  0.1715 
F-statistic: 334.1 on 1 and 1608 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>What does the table tell us? Recall we are only focusing on women!</p>
<p><strong>Intercept: 8.036830</strong> - This is the average of log earnings of women when years of education is 0.</p>
<p><strong>yeared: 0.145182</strong> - This is our slope. A one unit increase in years of education increases earnings by 15%.</p>
<p>Now let’s run the same regression but only on males.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(inc95) <span class="sc">~</span> yeared, <span class="at">data =</span> <span class="fu">subset</span>(nlsy, female <span class="sc">==</span> <span class="dv">0</span>))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(inc95) ~ yeared, data = subset(nlsy, female == 
    0))

Residuals:
    Min      1Q  Median      3Q     Max 
-4.2828 -0.3398  0.0287  0.3482  1.6835 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 8.628191   0.090547   95.29   &lt;2e-16 ***
yeared      0.128389   0.006577   19.52   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5932 on 2002 degrees of freedom
Multiple R-squared:  0.1599,    Adjusted R-squared:  0.1595 
F-statistic:   381 on 1 and 2002 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>What does the table tell us? Recall we are only focusing on men!</p>
<p><strong>Intercept: 8.628191 -</strong> This is the average of log earnings of men when years of education is 0.</p>
<p><strong>yeared: 0.128389</strong> - This is our slope. A one unit increase in years of education increases earnings by 13%.</p>
<p>What is the point of these simple regressions? This becomes clear once we actually run an interaction. In this conception, our question becomes: Is the effect of education on wages, conditional on gender? We just showed that there are different estimates and intercepts for each. But what we don’t know is that if they are <em>statistically different</em> from each other.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(inc95) <span class="sc">~</span> yeared<span class="sc">*</span>female, <span class="at">data =</span> nlsy)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(inc95) ~ yeared * female, data = nlsy)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.1738 -0.3107  0.0281  0.3476  2.0734 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    8.628191   0.090417  95.426  &lt; 2e-16 ***
yeared         0.128389   0.006568  19.548  &lt; 2e-16 ***
female        -0.591361   0.141812  -4.170 3.12e-05 ***
yeared:female  0.016793   0.010317   1.628    0.104    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5924 on 3610 degrees of freedom
Multiple R-squared:  0.2259,    Adjusted R-squared:  0.2252 
F-statistic: 351.1 on 3 and 3610 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Let’s look at this carefully. You may see some numbers that look familiar!</p>
<p><strong>Intercept: 8.628191</strong> - This the average of log wages when education is 0 <strong><em>for MALES.</em></strong></p>
<p><strong>yeared: .128389</strong> - a one year increase in education results in a 13% change in wages, <strong><em>for MALES.</em></strong></p>
<p><strong>female: -0.591361</strong> - now hold on, this is a new number. Actually, it isn’t. Add the intercept (8.628191) to the coefficient of female (-.591361). What do you get? You get <span class="math inline">\(8.03683\)</span>! That should look familiar because it is the intercept we estimated when we just ran the simple bivariate regression when the data was subset to only female. At <code>yeared = 0</code>, women earn <strong>0.591 log points less than men</strong>.</p>
<p><strong>yeared:female: 0.016793</strong> - While it is a new number, it actually isn’t. This tells us how much the slope differs for female. The slope for male is yeared = .128389. For women, the slope is the male slope plus .016793. What does that equal? .128 + .017 = .145. Is the coefficient of .145 look familiar? It should, because that is exactly the slope coefficient we estimated in m1 earlier.</p>
<p>But interactions aren’t special for these reasons, they are special because they quantify whether these effects are different for each subsample. Of course, the coefficients are both different but we need to know if they are <em>statistically different.</em> Based off the interaction term, we see a low t-score and a high p-value, indicating that we fail to reject the null hypothesis that the effect of education on wages is the same for male and female. However, even if we have significance, it is important to plot it. Just looking at the table will not tell us <em>where</em> the slopes are statistically different from each other.</p>
</section>
</section>
</section>
<section id="section-4-basic-questions-you-are-too-afraid-to-ask" class="level1">
<h1>Section 4: Basic Questions You Are Too Afraid To Ask</h1>
<section id="what-are-moments" class="level2">
<h2 class="anchored" data-anchor-id="what-are-moments">What are moments?</h2>
<p>Moments describe the probability distribution. Think of the shape of the density plot. Technically, two unique distributions could have the same mean or median. However, we need moments to help us better understand the distribution shape.</p>
<section id="mean" class="level3">
<h3 class="anchored" data-anchor-id="mean">Mean</h3>
<p><span class="math display">\[
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}
\]</span></p>
</section>
<section id="median-1" class="level3">
<h3 class="anchored" data-anchor-id="median-1">Median</h3>
<p>asdf</p>
</section>
<section id="mode-1" class="level3">
<h3 class="anchored" data-anchor-id="mode-1">Mode</h3>
<p>fdsaf</p>
</section>
<section id="kurtosis" class="level3">
<h3 class="anchored" data-anchor-id="kurtosis">Kurtosis</h3>
<p>sadf</p>
</section>
</section>
<section id="why-do-we-use-matrix-algebra-scalar-notation-seems-fine" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-use-matrix-algebra-scalar-notation-seems-fine">Why do we use matrix algebra? Scalar notation seems fine…</h2>
<p>There are a lot of reasons. In relation to regression, matrix algebra becomes essential because doing this in scalar notation turns into hell. It is simply too hard to do all of that once you get more and more variables.&nbsp;</p>
<p>Secondly, it is how R and other coding languages calculate the coefficient. Why? Long story short, it is less taxing on your computer to do these calculations. Besides more computer sciencey explanations, your computer is doing matrix algebra all the time.&nbsp;</p>
<p>Finally, matrix algebra will be used in further methods classes. This is especially important in machine learning. You are working with an array now but in machine learning, those arrays gain more dimensions. Imagine a matrix stacked upon another matrix and another. These are called tensors. Don’t worry, you don’t have to deal with these, ever…unless you want to. TK</p>
</section>
<section id="what-is-the-difference-between-covariance-and-correlation" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-difference-between-covariance-and-correlation">What is the difference between covariance and correlation?</h2>
<p>Correlation is covariance divided by the product of the two variables standard deviation. So they measure the same thing but correlation gives an output bounded to [-1,1] and the covariance takes on the same value as the constitutive terms.</p>
<p>Correlation is a normalization of covariance. Covariance is hard to interpret because the scale depends on the variances of two inputs. If you see a covariance of 11,350 or 2,489, you don’t know what those mean or even which set of variables have a high correlation. Correlation divides variance out and rescales to the interval [-1, 1], so now you can make those comparisons. Correlation is covariance but has greater readability and usefulness.</p>
</section>
<section id="what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept" class="level2">
<h2 class="anchored" data-anchor-id="what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept">What do dummy variables do to the line? Why don’t they change the slope? How come they only shift the intercept?</h2>
<p>A dummy variable is a variable coded in binary (0 or 1). Dummy variables can be a factor (0, 1, 2, 3, etc.)</p>
</section>
<section id="what-is-the-difference-between-variance-and-standard-deviation" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-difference-between-variance-and-standard-deviation">What is the difference between variance and standard deviation?</h2>
</section>
<section id="why-is-ordinary-least-squares-ols-so-powerful" class="level2">
<h2 class="anchored" data-anchor-id="why-is-ordinary-least-squares-ols-so-powerful">Why is Ordinary Least Squares (OLS) so powerful?</h2>
<p>The power of OLS becomes somewhat clearer as you learn about different methods. OLS is powerful because it is extremely easy to interpret. The interpretation of OLS is easy because we are specifying a linear relationship.</p>
<p>OLS power comes from the popularly known Gauss-Markov assumptions. If these assumptions are met, OLS is BLUE - Best Unbiased Linear Estimator.</p>
<p>Despite its power, OLS has shortfalls. However, it is still important to know OLS, as many methods serve as <em>extensions</em> of OLS and adapt it to better fit the data.</p>
</section>
<section id="when-is-ols-not-good-why-use-other-ones" class="level2">
<h2 class="anchored" data-anchor-id="when-is-ols-not-good-why-use-other-ones">When is OLS not good? Why use other ones?</h2>
<p>OLS has numerous advantages. However, OLS has shortfalls that other methods can fix/correct.</p>
<ol type="1">
<li>OLS is not good with a categorical dependent variable.</li>
</ol>
</section>
<section id="mediator-versus-moderator" class="level2">
<h2 class="anchored" data-anchor-id="mediator-versus-moderator">Mediator versus Moderator</h2>
<section id="mediator" class="level3">
<h3 class="anchored" data-anchor-id="mediator">Mediator:</h3>
<p>A mediator explains how an independent variable predicts an outcome, outlining the causal mechanism. A mediator tells us why or how something works.</p>
</section>
<section id="moderator" class="level3">
<h3 class="anchored" data-anchor-id="moderator">Moderator:</h3>
<p>A moderator variable changes the strength or direction of a relationship between two variables. Moderators tell us when or for whom something works. Influences the <em>strength</em> of a relationship. The built environment is a moderator for how people interact, it influences the nature and strength of interaction. But it does not explain how people interact.</p>
</section>
</section>
<section id="how-is-standard-error-difference-from-standard-deviation" class="level2">
<h2 class="anchored" data-anchor-id="how-is-standard-error-difference-from-standard-deviation">How is standard error difference from standard deviation?</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=A82brFpdr9g" class="uri">https://www.youtube.com/watch?v=A82brFpdr9g</a>
<ul>
<li>Watch the video.</li>
</ul></li>
<li>Standard deviation quantifies the variation within a set of measurements. Standard error quantifies the variation in the MEANS from multiple sets of measurements.</li>
<li>This gets confusing because we can estimate standard error off of one measurement.&nbsp;</li>
<li>Watch the video. Seriously, just watch the damn video.</li>
</ul>
</section>
<section id="are-the-assumptions-about-regression-related-to-the-sample-or-population" class="level2">
<h2 class="anchored" data-anchor-id="are-the-assumptions-about-regression-related-to-the-sample-or-population">Are the assumptions about regression related to the sample or population?</h2>
<ul>
<li><p>This was originally a question on Andy’s midterm. I got it wrong. :(</p></li>
<li><p>The assumptions relate to the population.</p>
<ul>
<li><p>We test these assumptions using our sample.</p></li>
<li><p>We use samples to tell us what we think the true (population) relationship is.</p>
<ul>
<li>Using data we have to tell us about data we do not have.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="why-is-it-called-ordinary-least-squares-ols" class="level2">
<h2 class="anchored" data-anchor-id="why-is-it-called-ordinary-least-squares-ols">Why is it called Ordinary Least Squares (OLS)?</h2>
</section>
<section id="what-is-variance-and-why-is-it-important" class="level2">
<h2 class="anchored" data-anchor-id="what-is-variance-and-why-is-it-important">What is variance and why is it important?</h2>
<p><span class="math display">\[
\text{Variance} (s^2) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}
\]</span></p>
<p>Variance is a measure of spread. Variance helps us understand the <em>dispersion</em> or <em>variability</em> in a data set. Variance estimates how far a set of numbers are spread out from the mean value. It can be difficult to interpret based on the output alone. This is because these values are squared, so we can’t really tell based on the number alone whether the value is relatively high or low.</p>
<p>Understanding variance is critical in statistics. Variance is integral to the efficiency of our estimators. That is, how accurate our model is.</p>
</section>
<section id="why-do-we-care-so-much-about-standard-errors" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-care-so-much-about-standard-errors">Why do we care so much about standard errors?</h2>
</section>
<section id="i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do" class="level2">
<h2 class="anchored" data-anchor-id="i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do">I am having trouble visualizing OLS with many variables. What do I do?</h2>
<p>Not much. We are pretty limited to understanding things in three dimensions. Imagine you have 8 variables in your OLS model. Try to draw an 8 dimensional model that shows the relationship. It is impossible.</p>
</section>
<section id="instrumental-variables-what-are-they-will-i-use-them-should-i-use-them" class="level2">
<h2 class="anchored" data-anchor-id="instrumental-variables-what-are-they-will-i-use-them-should-i-use-them">Instrumental variables, what are they? Will I use them? Should I use them?</h2>
<p>Instrumental variables are somewhat rare and frowned upon (?) in political science. To be a good instrumental variable, instrumental variables must satisfy two conditions:</p>
<ol type="1">
<li><p>The instrumental variable is theoretically relevant to x.</p></li>
<li><p>The instrumental variable must satisfy the exclusion restriction.</p></li>
</ol>
<p>The first point requires that our instrument (z) must be endogenous to our independent variable (x)</p>
<p>The exclusion restriction is typically where instrumental variables get attacked. The instrumental variable (<em>z</em> in this case) must only affect X. Z-&gt; X -&gt; Y. The difficulty to this condition is that there is no statistical test. The exclusion restriction must be defended by theory.</p>
<p>It is very difficult to find an instrument that is both related to X and does not affect Y. An example of a good instrument is provided below (thank you ChrisP from StackExchange):</p>
<p><em>“For example, suppose we want to estimate the effect of police (</em>𝑥<em>) on crime (</em>𝑦<em>) in a cross-section of cities. One issue is that places with lots of crime will hire more police. We therefore seek an instrument</em> 𝑧<em>𝑧 that is correlated with the size of the police force, but unrelated to crime.</em></p>
<p><em>One possible</em> 𝑧 <em>is number of firefighters. The assumptions are that cities with lots of firefighters also have large police forces (relevance) and that firefighters do not affect crime (exclusion). Relevance can be checked with the reduced form regressions, but whether firefighters also affect crime is something to be argued for. Theoretically, they do not and are therefore a valid instrument.”</em></p>
<section id="why-should-we-use-an-instrumental-variable" class="level3">
<h3 class="anchored" data-anchor-id="why-should-we-use-an-instrumental-variable">Why should we use an instrumental variable?</h3>
<p>The need for an instrumental variable arises when we are concerned for confounding variables or measurement error.</p>
<p>Most of the time these suck. You need to defend them rigorously with theory and even then are hard. Rainfall is one that is used in many cases with success. If asked to use one, the classic rainfall instrumental variable might work.</p>
</section>
</section>
<section id="should-we-care-about-r2" class="level2">
<h2 class="anchored" data-anchor-id="should-we-care-about-r2">Should we care about <span class="math inline">\(R^2\)</span>?</h2>
<p>It depends on your question. Chances are you want to find some variable (X) that causes another variable (Y). In this instance, your <span class="math inline">\(R^2\)</span> is mostly irrelevant. You want to see whether that X variable is statistically having an effect on your Y variable. For example, my data 1 project was on the relationship between walkability and voter turnout. I wanted to see if the walkability of an area had an impact on voter turnout in 2016, 2018, and 2020 general elections. Once I accounted for confounding variables, all I cared about was the significance of my variable of interest (walkability). <span class="math inline">\(R^2\)</span> told me nothing that helped me answer this question.&nbsp;</p>
<p>However, R^2 is very important for questions surrounding prediction. TK</p>
<p>Also we should focus on adjusted R^2</p>
<p>We are in the business of beta hats not y hats.</p>
</section>
<section id="everyone-talks-about-endogeneity.-what-is-it" class="level2">
<h2 class="anchored" data-anchor-id="everyone-talks-about-endogeneity.-what-is-it">Everyone talks about endogeneity. What is it?!</h2>
<p>Endogeneity is when the error term is correlated with the X. Remember that the error term contains everything <strong>not</strong> in our model (everything that determines Y but is NOT X, will be in our error term). If any of those things <strong>not</strong> in our model (the error) are related to our X and affect Y, then we have endogeneity. Endogeneity relates to confounders.</p>
<p>Endogeneity leads to bias in our coefficient.</p>
</section>
<section id="what-is-orthogonal" class="level2">
<h2 class="anchored" data-anchor-id="what-is-orthogonal">What is orthogonal?</h2>
<p>This concept was always a bit confusing as it can have different meaning in different contexts.</p>
<p>ORTHOGONAL MEANS INDEPENDENT</p>
<p>Simply put, orthogonality means “uncorrelated”. An orthogonal model means that all independent variables in that model are uncorrelated. If one or more independent variables are correlated, then that model is <strong>non-orthogonal (</strong>statisticshowto.com)</p>
</section>
<section id="is-ols-a-causal-inference-model" class="level2">
<h2 class="anchored" data-anchor-id="is-ols-a-causal-inference-model">Is OLS a causal Inference model?</h2>
</section>
<section id="why-do-we-use-the-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-use-the-normal-distribution">Why do we use the normal distribution?</h2>
</section>
</section>
<section id="section-7-interpretation-deep-dive" class="level1">
<h1>Section 7: Interpretation Deep Dive</h1>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
</section>
<section id="interpreting-control-variables" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-control-variables">Interpreting Control Variables</h2>
<p>Our model, of whatever specification, will likely include numerous control variables. Each of these variables included will have their own coefficient estimated. Of course, we have our main IV, but should we bother to interpret the other variables in our model? No.&nbsp;This is known as the “Table 2 fallacy”.</p>
</section>
</section>
<section id="section-8-thinking-deeper" class="level1">
<h1>Section 8: Thinking Deeper</h1>
<section id="unbiased-inferences" class="level2">
<h2 class="anchored" data-anchor-id="unbiased-inferences">Unbiased Inferences</h2>
<p>“If we apply a method of inference again and again, we will get estimates that are sometimes too small. Across a large number of applications, do we get the right answer <em>on average</em>? If yes, then this method, or estimator, is said to be <strong><em>unbiased.</em></strong>” - KKV</p>
<p>Bias occurs when there is a systematic error in the measure that shifts the estimate more in on e direction than another over a set of replications. Note there is <em>statistical bias</em> and <em>substantive bias.</em></p>
<p>An estimator of <span class="math inline">\(\mu\)</span> is said to be unbiased if it equals <span class="math inline">\(\mu\)</span> on average over many hypothetical replications of the same experiment.</p>
</section>
<section id="efficiency" class="level2">
<h2 class="anchored" data-anchor-id="efficiency">Efficiency</h2>
<p>We generally only get to estimate once. We want confidence that the one estimate we get is close to the right one.</p>
<p>Efficiency is measured by calculating the variance of the estimator across hypothetical replications. For unbiased estimators, the smaller the variance, the more efficient (the better) the estimator.</p>
</section>
<section id="bias-vs.-efficiency" class="level2">
<h2 class="anchored" data-anchor-id="bias-vs.-efficiency">Bias vs.&nbsp;Efficiency</h2>
<p>It is a tradeoff!</p>
</section>
</section>
<section id="section-9-causal-inference" class="level1">
<h1>Section 9: Causal Inference</h1>
<section id="readings-associated" class="level2">
<h2 class="anchored" data-anchor-id="readings-associated">Readings Associated:</h2>
<ol type="1">
<li>Keele, Luke. 2015. “The Statistics of Causal Inference: A View from Political Methodology.” Political Analysis 23(3): 313–335</li>
<li></li>
</ol>
</section>
<section id="what-is-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="what-is-causal-inference">What is Causal Inference?</h2>
<p>Causality refers to the realtional concept where on set of events causes another. Causal inference is the process by which we make claims about causal relationships. Testing causality is done udner the counterfactual model, that is, causation is defined in terms of observable and unobservable events.</p>
<p>Why should we are about the counterfactual model and how is this different from statistics broadly?</p>
<ol type="1">
<li>the counterfactual approach has provided new insighs in the assumptions needed for data to be informative about causality.</li>
<li>a renewed emphasis on research design and design-based appraoch.</li>
</ol>
</section>
<section id="the-fundamental-problem-of-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="the-fundamental-problem-of-causal-inference">The Fundamental Problem of Causal Inference</h2>
<p>It is actually very difficult to say on thing actually caused another. Why? Because we can never observe the real counterfactual. We live in one universe. You cannot observe someone taking both the treatment and placebo pill. You only see the universe in which they took one or the other.</p>
</section>
<section id="components-of-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="components-of-causal-inference">Components of Causal Inference</h2>
<section id="identification" class="level3">
<h3 class="anchored" data-anchor-id="identification">Identification</h3>
<p>We say a parameter in a model is identified if it is theoretically possible to learn the true value of that parameter with an infinite number of observations. We seek to describe the <em>conclusions</em> that can be drawn with an infinite sample. Inference requires two components: identification and statistical. Studies of statistical inference on the other hand focus on what can be learned with finite samples. The causal identification step is important to see if it’s possible to estimate the effect of <em>Treatment</em> on <em>Outcome</em>.</p>
<section id="estimands" class="level4">
<h4 class="anchored" data-anchor-id="estimands">Estimands</h4>
<p>What is an estimand. You can think of them as an inquiry. However, the inquiry is the function that operates on events generated by the real world or a simulated world. The estimand is the value of that function. We use “inquiry” to refer to the question and “estimand” to refer to the answer to the question.</p>
<p>Average Treatment Effect (ATE): <span class="math inline">\(ATE=\mathbb{E}[Y_{i1} -Y_{i0}]\)</span></p>
<ul>
<li>The ATE is the average difference in the pair of potential outcomes average over the entire population of interest.</li>
</ul>
<p>Average Treatment Effect of the Treated (ATT): <span class="math inline">\(ATT=\mathbb{E}[Y_{i1} -Y_{i0} |D_i=1]\)</span></p>
<ul>
<li><p>The ATE defined for the subpopulation exposed to the treatment</p>
<ul>
<li>We might average over subpopulations defined by pretreatment covariates such as sex and estimate the ATE for females only = ATT.</li>
</ul></li>
</ul>
</section>
<section id="threats-to-identification" class="level4">
<h4 class="anchored" data-anchor-id="threats-to-identification">Threats to Identification</h4>
<p>Confounding due to a common cause</p>
<p>Confounding due to a common effect</p>
</section>
<section id="assumptions" class="level4">
<h4 class="anchored" data-anchor-id="assumptions">Assumptions</h4>
<ol type="1">
<li>Assumption of independence between treatment and potential outcomes.</li>
<li>The stable unit treatment value assumption (SUTVA) - Rubin 1986
<ol type="1">
<li>there are no hidden forms of treatment, which implies that for for unit <span class="math inline">\(i\)</span> under <span class="math inline">\(D_i=d\)</span>, we assume that <span class="math inline">\(Y_{id}=Y_i\)</span>
<ol type="1">
<li>AKA the consistency assumption</li>
</ol></li>
<li>a subject’s potential outcome is not affected by other subjects’ exposure to the treatment.
<ol type="1">
<li>This is a big problem and must be avoided - think of it as contamination. Your no longer looking at treated vs.&nbsp;control but now between a treated unit and partially treated unit.
<ol type="1">
<li>if we have no knowledge of these spillovers, causal parameters cannot be identified.</li>
</ol></li>
</ol></li>
</ol></li>
</ol>
</section>
</section>
</section>
<section id="identification-strategies" class="level2">
<h2 class="anchored" data-anchor-id="identification-strategies">Identification Strategies</h2>
<p>A research design intended to solve the causal inference identification problem (Angrist and Pischke 2010). To ask what is your identification strategy IS to ask what research design (and assumptions) one intends to use for the indemnification of a causal effect.</p>
<section id="random-experiments" class="level4">
<h4 class="anchored" data-anchor-id="random-experiments">Random Experiments</h4>
<p>Randomize recipients of treatment. The typical estimand is the ATE (equivalent in this to the ATT).</p>
<ul>
<li><p>Critical strength of experiments is that the researcher has the ability to impose independence between treatment status and potential outcomes on a set of units because they can impose a particular type of assignment process.</p>
<ul>
<li>The researcher can assert that the treated and control groups will be identical in all respects, observable and unobserervable. EXCEPT for who got the treatment which is known.</li>
</ul></li>
</ul>
</section>
<section id="natural-experiments" class="level4">
<h4 class="anchored" data-anchor-id="natural-experiments">Natural Experiments</h4>
</section>
<section id="instrumental-variables" class="level4">
<h4 class="anchored" data-anchor-id="instrumental-variables">Instrumental Variables</h4>
</section>
<section id="regression-discontinuity-designs" class="level4">
<h4 class="anchored" data-anchor-id="regression-discontinuity-designs">Regression Discontinuity Designs</h4>
</section>
<section id="selection-on-observables" class="level4">
<h4 class="anchored" data-anchor-id="selection-on-observables">Selection on Observables</h4>
</section>
<section id="selection-on-observables-with-temporal-data" class="level4">
<h4 class="anchored" data-anchor-id="selection-on-observables-with-temporal-data">Selection on Observables with Temporal Data</h4>
</section>
<section id="partial-identification" class="level4">
<h4 class="anchored" data-anchor-id="partial-identification">Partial Identification</h4>
</section>
<section id="mediation-analysis" class="level4">
<h4 class="anchored" data-anchor-id="mediation-analysis">Mediation Analysis</h4>
</section>
<section id="reasoning-about-assumptions" class="level4">
<h4 class="anchored" data-anchor-id="reasoning-about-assumptions">Reasoning About Assumptions</h4>
</section>
</section>
</section>
<section id="section-9.5-likelihood" class="level1">
<h1>Section 9.5: Likelihood</h1>
<p>The principle of maximum likelihood is based on the idea that the observed data (even if it is not a random sample) are more likely to have come about as a result of a particular set of parameters. Thus, we flip the problem on its head. <em>Rather than consider the data as random and the parameters as fixed, the principle of maximum likelihood treats the observed data as fixed and asks: “What parameter values are most likely to have generated the data?”</em> Thus, the parameters are random variables.</p>
<p>The key innovation in the likelihood framework is treating the observed data as fixed and asking what combination of probability model and parameter values are the most likely to have generated these specific data.</p>
<section id="why-do-we-need-maximum-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-need-maximum-likelihood">Why do we need maximum likelihood?</h2>
<p>We use it anytime our dependent variables has a distribution that was not generated by a Gaussian (normal) process. We are trying to estimate the <span class="math inline">\(\theta\)</span> (parameter) values based on the data we have been given.</p>
</section>
<section id="the-basic-structure-of-mle" class="level2">
<h2 class="anchored" data-anchor-id="the-basic-structure-of-mle">The Basic Structure of MLE</h2>
<section id="stochastic-component" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-component">Stochastic component</h3>
<p>Stochastic component: <span class="math inline">\(Y_i\sim f(\theta_i)\)</span></p>
<p>Here, <span class="math inline">\(f\)</span> is some probability distribution or mass function. These can be Gaussian, binomial, Poisson, Weibull, etc. The stochastic statement describes our assumptions about the probability distributions that govern our data-generating process.</p>
</section>
<section id="systematic-component" class="level3">
<h3 class="anchored" data-anchor-id="systematic-component">Systematic component</h3>
<p>Systematic component: <span class="math inline">\(\theta_i = \beta_0+ \beta_1x_i\)</span></p>
<p>The systematic statement describes our model for the parameters of the assumed probability distribution.</p>
</section>
</section>
<section id="difference-between-mle-and-ols" class="level2">
<h2 class="anchored" data-anchor-id="difference-between-mle-and-ols">Difference between MLE and OLS?</h2>
<p>They are equivalent, and the OLS estimator can be derived under weaker assumptions. But the OLS approach assumes a linear model and unbounded, continuous outcome. The linear model is a good one, but the world around can be both nonlinear and bounded. The ML approach permits us to specify nonlinear models quite easily if warranted by either our theory or data. The flexibility to model categorical and vounded variables is a benefit of the maximum likelihood approach.</p>
</section>
</section>
<section id="section-10-generalized-linear-models-glm" class="level1">
<h1>Section 10: Generalized Linear Models (GLM)</h1>
<section id="logit" class="level2">
<h2 class="anchored" data-anchor-id="logit">Logit</h2>
<p>Models the probability of an event. Dependent variable is binary (0 or 1). The logit is a cumulative density function</p>
<section id="link-function" class="level3">
<h3 class="anchored" data-anchor-id="link-function">Link Function:</h3>
<p><span class="math display">\[
f(x) = \frac{1}{1+e^{-X_i\beta}}
\]</span></p>
</section>
<section id="logit-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="logit-interpretation">Logit Interpretation</h3>
</section>
<section id="can-we-just-use-ols" class="level3">
<h3 class="anchored" data-anchor-id="can-we-just-use-ols">Can We Just Use OLS?</h3>
<ul>
<li><p>You could. In fact, many (mostly economists) argue that this is economical and won’t be too much different.</p>
<ul>
<li><p>Linear interpretations of betas</p></li>
<li><p>simple</p></li>
<li><p>Works well if <span class="math inline">\(X_i\)</span> is also distributed Bernoulli</p></li>
</ul></li>
<li><p>Problems:</p>
<ul>
<li><p>impossible predictions</p></li>
<li><p>TK</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="probit" class="level2">
<h2 class="anchored" data-anchor-id="probit">Probit</h2>
<section id="probit-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="probit-interpretation">Probit Interpretation</h3>
</section>
</section>
</section>
<section id="section-11-model-selection" class="level1">
<h1>Section 11: Model Selection</h1>
<p>This section is intended for application purposes. What model do we used based on our data and application purpose? The bulk of this decision is driven by the structure of our data, specifically the dependent variable.</p>
<section id="ordinary-least-squares-ols" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-least-squares-ols">Ordinary Least Squares (OLS)</h2>
</section>
<section id="stage-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="stage-least-squares">2 Stage Least Squares</h2>
</section>
<section id="logit-1" class="level2">
<h2 class="anchored" data-anchor-id="logit-1">Logit</h2>
</section>
<section id="probit-1" class="level2">
<h2 class="anchored" data-anchor-id="probit-1">Probit</h2>
</section>
<section id="negative-binomial" class="level2">
<h2 class="anchored" data-anchor-id="negative-binomial">Negative Binomial</h2>
</section>
<section id="poisson" class="level2">
<h2 class="anchored" data-anchor-id="poisson">Poisson</h2>
</section>
<section id="multinomial-logit" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logit">Multinomial Logit</h2>
</section>
<section id="ordinal-logit" class="level2">
<h2 class="anchored" data-anchor-id="ordinal-logit">Ordinal Logit</h2>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{neilon2024,
  author = {Neilon, Stone},
  title = {Data 1 \&amp; 2},
  date = {2024-05-15},
  url = {https://stoneneilon.github.io/notes/Data/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-neilon2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Neilon, Stone. 2024. <span>“Data 1 &amp; 2.”</span> May 15, 2024. <a href="https://stoneneilon.github.io/notes/Data/">https://stoneneilon.github.io/notes/Data/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>