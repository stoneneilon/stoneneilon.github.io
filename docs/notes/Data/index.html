<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stone Neilon">
<meta name="dcterms.date" content="2024-05-15">
<meta name="description" content="Combined notes from Data 1 &amp; 2 with Anand Sokhey and Andy Phillips">

<title>Stone Neilon - Data 1 &amp; 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stone Neilon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/file/d/1wwwiFFiVu-k76M6oixKugPFjESA0g4Is/view?usp=sharing"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface">Preface</a></li>
  <li><a href="#section-1-descriptive-statistics" id="toc-section-1-descriptive-statistics" class="nav-link" data-scroll-target="#section-1-descriptive-statistics">Section 1: Descriptive Statistics</a>
  <ul class="collapse">
  <li><a href="#mean-average" id="toc-mean-average" class="nav-link" data-scroll-target="#mean-average">Mean (Average)</a></li>
  <li><a href="#median" id="toc-median" class="nav-link" data-scroll-target="#median">Median</a></li>
  <li><a href="#mode" id="toc-mode" class="nav-link" data-scroll-target="#mode">Mode</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance">Variance</a></li>
  <li><a href="#standard-deviation" id="toc-standard-deviation" class="nav-link" data-scroll-target="#standard-deviation">Standard Deviation</a></li>
  <li><a href="#standard-error" id="toc-standard-error" class="nav-link" data-scroll-target="#standard-error">Standard Error</a></li>
  <li><a href="#skewness" id="toc-skewness" class="nav-link" data-scroll-target="#skewness">Skewness</a></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance">Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation">Correlation</a></li>
  </ul></li>
  <li><a href="#section-2-statisticalhypothesis-testing" id="toc-section-2-statisticalhypothesis-testing" class="nav-link" data-scroll-target="#section-2-statisticalhypothesis-testing">Section 2: Statistical/Hypothesis Testing</a>
  <ul class="collapse">
  <li><a href="#t-test" id="toc-t-test" class="nav-link" data-scroll-target="#t-test">t-Test</a>
  <ul class="collapse">
  <li><a href="#probability-distribution" id="toc-probability-distribution" class="nav-link" data-scroll-target="#probability-distribution">Probability Distribution:</a></li>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose:</a></li>
  <li><a href="#uses" id="toc-uses" class="nav-link" data-scroll-target="#uses">Uses:</a></li>
  </ul></li>
  <li><a href="#chi2-test" id="toc-chi2-test" class="nav-link" data-scroll-target="#chi2-test"><span class="math inline">\(\chi^2\)</span> Test</a>
  <ul class="collapse">
  <li><a href="#purpose-1" id="toc-purpose-1" class="nav-link" data-scroll-target="#purpose-1">Purpose:</a></li>
  <li><a href="#uses-1" id="toc-uses-1" class="nav-link" data-scroll-target="#uses-1">Uses:</a></li>
  </ul></li>
  <li><a href="#analysis-of-variance-anova-test" id="toc-analysis-of-variance-anova-test" class="nav-link" data-scroll-target="#analysis-of-variance-anova-test">Analysis of Variance (ANOVA) Test</a>
  <ul class="collapse">
  <li><a href="#purpose-2" id="toc-purpose-2" class="nav-link" data-scroll-target="#purpose-2">Purpose:</a></li>
  <li><a href="#uses-2" id="toc-uses-2" class="nav-link" data-scroll-target="#uses-2">Uses:</a></li>
  </ul></li>
  <li><a href="#f-test" id="toc-f-test" class="nav-link" data-scroll-target="#f-test">F-Test</a>
  <ul class="collapse">
  <li><a href="#purpose-3" id="toc-purpose-3" class="nav-link" data-scroll-target="#purpose-3">Purpose:</a></li>
  <li><a href="#uses-3" id="toc-uses-3" class="nav-link" data-scroll-target="#uses-3">Uses:</a></li>
  </ul></li>
  <li><a href="#p-value" id="toc-p-value" class="nav-link" data-scroll-target="#p-value">p-value</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition:</a></li>
  <li><a href="#purpose-4" id="toc-purpose-4" class="nav-link" data-scroll-target="#purpose-4">Purpose:</a></li>
  <li><a href="#uses-4" id="toc-uses-4" class="nav-link" data-scroll-target="#uses-4">Uses:</a></li>
  <li><a href="#important-notes" id="toc-important-notes" class="nav-link" data-scroll-target="#important-notes">Important Notes:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-3-regression-overviewreview" id="toc-section-3-regression-overviewreview" class="nav-link" data-scroll-target="#section-3-regression-overviewreview">Section 3: Regression Overview/Review</a>
  <ul class="collapse">
  <li><a href="#assumptions-scalar-notation" id="toc-assumptions-scalar-notation" class="nav-link" data-scroll-target="#assumptions-scalar-notation">Assumptions (Scalar Notation)</a>
  <ul class="collapse">
  <li><a href="#assumption-1-epsilon_iis-normally-distributed" id="toc-assumption-1-epsilon_iis-normally-distributed" class="nav-link" data-scroll-target="#assumption-1-epsilon_iis-normally-distributed">Assumption 1: <span class="math inline">\(\epsilon_i\)</span>is normally distributed&nbsp;</a></li>
  <li><a href="#assumption-2-eepsilon_i0" id="toc-assumption-2-eepsilon_i0" class="nav-link" data-scroll-target="#assumption-2-eepsilon_i0">Assumption 2: <span class="math inline">\(E[\epsilon_i]=0\)</span></a></li>
  <li><a href="#assumption-3-varepsilon_isigma2" id="toc-assumption-3-varepsilon_isigma2" class="nav-link" data-scroll-target="#assumption-3-varepsilon_isigma2">Assumption 3: <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span></a></li>
  <li><a href="#assumption-4-covepsilon_iepsilon_j0-foralli-neq-j" id="toc-assumption-4-covepsilon_iepsilon_j0-foralli-neq-j" class="nav-link" data-scroll-target="#assumption-4-covepsilon_iepsilon_j0-foralli-neq-j">Assumption 4: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0 \:\forall\:i \neq j\)</span></a></li>
  <li><a href="#assumption-5-x_i-is-fixed-in-repeated-sampling" id="toc-assumption-5-x_i-is-fixed-in-repeated-sampling" class="nav-link" data-scroll-target="#assumption-5-x_i-is-fixed-in-repeated-sampling">Assumption 5: <span class="math inline">\(x_i\)</span> is fixed in repeated sampling</a></li>
  <li><a href="#assumption-6-sample-regression-model-correctly-specified" id="toc-assumption-6-sample-regression-model-correctly-specified" class="nav-link" data-scroll-target="#assumption-6-sample-regression-model-correctly-specified">Assumption 6: Sample regression model correctly specified&nbsp;</a></li>
  <li><a href="#assumption-7-covepsilon_ix_i0" id="toc-assumption-7-covepsilon_ix_i0" class="nav-link" data-scroll-target="#assumption-7-covepsilon_ix_i0">Assumption 7: <span class="math inline">\(Cov(\epsilon_i,x_i)=0\)</span></a></li>
  <li><a href="#assumption-8-parametric-linearity" id="toc-assumption-8-parametric-linearity" class="nav-link" data-scroll-target="#assumption-8-parametric-linearity">Assumption 8: Parametric Linearity</a></li>
  <li><a href="#assumption-9-x_i-must-vary" id="toc-assumption-9-x_i-must-vary" class="nav-link" data-scroll-target="#assumption-9-x_i-must-vary">Assumption 9: <span class="math inline">\(x_i\)</span> must vary</a></li>
  <li><a href="#assumption-10-n-k" id="toc-assumption-10-n-k" class="nav-link" data-scroll-target="#assumption-10-n-k">Assumption 10: n &gt; k</a></li>
  <li><a href="#assumption-11-no-perfect-multicollinearity" id="toc-assumption-11-no-perfect-multicollinearity" class="nav-link" data-scroll-target="#assumption-11-no-perfect-multicollinearity">Assumption 11: No perfect multicollinearity</a></li>
  </ul></li>
  <li><a href="#assumptions-matrix-notation" id="toc-assumptions-matrix-notation" class="nav-link" data-scroll-target="#assumptions-matrix-notation">Assumptions (Matrix Notation)</a>
  <ul class="collapse">
  <li><a href="#assumption-1-linearity-in-the-parameters" id="toc-assumption-1-linearity-in-the-parameters" class="nav-link" data-scroll-target="#assumption-1-linearity-in-the-parameters">Assumption 1: Linearity in the parameters</a></li>
  <li><a href="#assumption-2-full-rank" id="toc-assumption-2-full-rank" class="nav-link" data-scroll-target="#assumption-2-full-rank">Assumption 2: Full rank</a></li>
  <li><a href="#assumption-3-exogeneity-of-the-independent-variables" id="toc-assumption-3-exogeneity-of-the-independent-variables" class="nav-link" data-scroll-target="#assumption-3-exogeneity-of-the-independent-variables">Assumption 3: Exogeneity of the independent variables</a></li>
  <li><a href="#assumption-4-spherical-disturbances" id="toc-assumption-4-spherical-disturbances" class="nav-link" data-scroll-target="#assumption-4-spherical-disturbances">Assumption 4: Spherical disturbances</a></li>
  <li><a href="#assumption-5-data-generation" id="toc-assumption-5-data-generation" class="nav-link" data-scroll-target="#assumption-5-data-generation">Assumption 5: Data generation</a></li>
  <li><a href="#assumption-6-epsilon-is-normally-distributed" id="toc-assumption-6-epsilon-is-normally-distributed" class="nav-link" data-scroll-target="#assumption-6-epsilon-is-normally-distributed">Assumption 6: <span class="math inline">\(\epsilon\)</span> is normally distributed</a></li>
  </ul></li>
  <li><a href="#formula-for-deriving-beta" id="toc-formula-for-deriving-beta" class="nav-link" data-scroll-target="#formula-for-deriving-beta">Formula for deriving <span class="math inline">\(\beta\)</span></a></li>
  <li><a href="#omega-matrix" id="toc-omega-matrix" class="nav-link" data-scroll-target="#omega-matrix">Omega Matrix</a>
  <ul class="collapse">
  <li><a href="#conversation-with-andy-about-omega-matrix" id="toc-conversation-with-andy-about-omega-matrix" class="nav-link" data-scroll-target="#conversation-with-andy-about-omega-matrix">Conversation with Andy about Omega Matrix:</a></li>
  </ul></li>
  <li><a href="#standard-error-1" id="toc-standard-error-1" class="nav-link" data-scroll-target="#standard-error-1">Standard Error</a></li>
  <li><a href="#heteroscedasticity-spherical-disturbances" id="toc-heteroscedasticity-spherical-disturbances" class="nav-link" data-scroll-target="#heteroscedasticity-spherical-disturbances">Heteroscedasticity (spherical disturbances)</a></li>
  <li><a href="#autocorrelation-spherical-disturbances" id="toc-autocorrelation-spherical-disturbances" class="nav-link" data-scroll-target="#autocorrelation-spherical-disturbances">Autocorrelation (spherical disturbances)</a></li>
  <li><a href="#interactions" id="toc-interactions" class="nav-link" data-scroll-target="#interactions">Interactions:</a>
  <ul class="collapse">
  <li><a href="#interactions-increase-multicollinearity" id="toc-interactions-increase-multicollinearity" class="nav-link" data-scroll-target="#interactions-increase-multicollinearity">Interactions increase multicollinearity</a></li>
  <li><a href="#include-all-constitutive-terms" id="toc-include-all-constitutive-terms" class="nav-link" data-scroll-target="#include-all-constitutive-terms">Include all constitutive terms</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-4-basic-questions-you-are-too-afraid-to-ask" id="toc-section-4-basic-questions-you-are-too-afraid-to-ask" class="nav-link" data-scroll-target="#section-4-basic-questions-you-are-too-afraid-to-ask">Section 4: Basic Questions You Are Too Afraid To Ask</a>
  <ul class="collapse">
  <li><a href="#what-are-moments" id="toc-what-are-moments" class="nav-link" data-scroll-target="#what-are-moments">What are moments?</a>
  <ul class="collapse">
  <li><a href="#mean" id="toc-mean" class="nav-link" data-scroll-target="#mean">Mean</a></li>
  <li><a href="#median-1" id="toc-median-1" class="nav-link" data-scroll-target="#median-1">Median</a></li>
  <li><a href="#mode-1" id="toc-mode-1" class="nav-link" data-scroll-target="#mode-1">Mode</a></li>
  <li><a href="#kurtosis" id="toc-kurtosis" class="nav-link" data-scroll-target="#kurtosis">Kurtosis</a></li>
  </ul></li>
  <li><a href="#why-do-we-use-matrix-algebra-scalar-notation-seems-fine" id="toc-why-do-we-use-matrix-algebra-scalar-notation-seems-fine" class="nav-link" data-scroll-target="#why-do-we-use-matrix-algebra-scalar-notation-seems-fine">Why do we use matrix algebra? Scalar notation seems fine…</a></li>
  <li><a href="#what-is-the-difference-between-covariance-and-correlation" id="toc-what-is-the-difference-between-covariance-and-correlation" class="nav-link" data-scroll-target="#what-is-the-difference-between-covariance-and-correlation">What is the difference between covariance and correlation?</a></li>
  <li><a href="#what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept" id="toc-what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept" class="nav-link" data-scroll-target="#what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept">What do dummy variables do to the line? Why don’t they change the slope? How come they only shift the intercept?</a></li>
  <li><a href="#what-is-the-difference-between-variance-and-standard-deviation" id="toc-what-is-the-difference-between-variance-and-standard-deviation" class="nav-link" data-scroll-target="#what-is-the-difference-between-variance-and-standard-deviation">What is the difference between variance and standard deviation?</a></li>
  <li><a href="#why-is-ordinary-least-squares-ols-so-powerful" id="toc-why-is-ordinary-least-squares-ols-so-powerful" class="nav-link" data-scroll-target="#why-is-ordinary-least-squares-ols-so-powerful">Why is Ordinary Least Squares (OLS) so powerful?</a></li>
  <li><a href="#when-is-ols-not-good-why-use-other-ones" id="toc-when-is-ols-not-good-why-use-other-ones" class="nav-link" data-scroll-target="#when-is-ols-not-good-why-use-other-ones">When is OLS not good? Why use other ones?</a></li>
  <li><a href="#how-is-standard-error-difference-from-standard-deviation" id="toc-how-is-standard-error-difference-from-standard-deviation" class="nav-link" data-scroll-target="#how-is-standard-error-difference-from-standard-deviation">How is standard error difference from standard deviation?</a></li>
  <li><a href="#are-the-assumptions-about-regression-related-to-the-sample-or-population" id="toc-are-the-assumptions-about-regression-related-to-the-sample-or-population" class="nav-link" data-scroll-target="#are-the-assumptions-about-regression-related-to-the-sample-or-population">Are the assumptions about regression related to the sample or population?</a></li>
  <li><a href="#why-is-it-called-ordinary-least-squares-ols" id="toc-why-is-it-called-ordinary-least-squares-ols" class="nav-link" data-scroll-target="#why-is-it-called-ordinary-least-squares-ols">Why is it called Ordinary Least Squares (OLS)?</a></li>
  <li><a href="#what-is-variance-and-why-is-it-important" id="toc-what-is-variance-and-why-is-it-important" class="nav-link" data-scroll-target="#what-is-variance-and-why-is-it-important">What is variance and why is it important?</a></li>
  <li><a href="#why-do-we-care-so-much-about-standard-errors" id="toc-why-do-we-care-so-much-about-standard-errors" class="nav-link" data-scroll-target="#why-do-we-care-so-much-about-standard-errors">Why do we care so much about standard errors?</a></li>
  <li><a href="#i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do" id="toc-i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do" class="nav-link" data-scroll-target="#i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do">I am having trouble visualizing OLS with many variables. What do I do?</a></li>
  <li><a href="#instrumental-variables-what-are-they-will-i-use-them-should-i-use-them" id="toc-instrumental-variables-what-are-they-will-i-use-them-should-i-use-them" class="nav-link" data-scroll-target="#instrumental-variables-what-are-they-will-i-use-them-should-i-use-them">Instrumental variables, what are they? Will I use them? Should I use them?</a>
  <ul class="collapse">
  <li><a href="#why-should-we-use-an-instrumental-variable" id="toc-why-should-we-use-an-instrumental-variable" class="nav-link" data-scroll-target="#why-should-we-use-an-instrumental-variable">Why should we use an instrumental variable?</a></li>
  </ul></li>
  <li><a href="#should-we-care-about-r2" id="toc-should-we-care-about-r2" class="nav-link" data-scroll-target="#should-we-care-about-r2">Should we care about <span class="math inline">\(R^2\)</span>?</a></li>
  <li><a href="#everyone-talks-about-endogeneity.-what-is-it" id="toc-everyone-talks-about-endogeneity.-what-is-it" class="nav-link" data-scroll-target="#everyone-talks-about-endogeneity.-what-is-it">Everyone talks about endogeneity. What is it?!</a></li>
  <li><a href="#what-is-orthogonal" id="toc-what-is-orthogonal" class="nav-link" data-scroll-target="#what-is-orthogonal">What is orthogonal?</a></li>
  <li><a href="#is-ols-a-causal-inference-model" id="toc-is-ols-a-causal-inference-model" class="nav-link" data-scroll-target="#is-ols-a-causal-inference-model">Is OLS a causal Inference model?</a></li>
  <li><a href="#why-do-we-use-the-normal-distribution" id="toc-why-do-we-use-the-normal-distribution" class="nav-link" data-scroll-target="#why-do-we-use-the-normal-distribution">Why do we use the normal distribution?</a></li>
  </ul></li>
  <li><a href="#section-5-notation" id="toc-section-5-notation" class="nav-link" data-scroll-target="#section-5-notation">Section 5: Notation</a></li>
  <li><a href="#section-6-list-of-resources-that-helped-me" id="toc-section-6-list-of-resources-that-helped-me" class="nav-link" data-scroll-target="#section-6-list-of-resources-that-helped-me">Section 6: List of Resources That Helped Me</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Data 1 &amp; 2</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Spring</div>
    <div class="quarto-category">Fall</div>
    <div class="quarto-category">2024</div>
    <div class="quarto-category">Methods</div>
    <div class="quarto-category">2023</div>
  </div>
  </div>

<div>
  <div class="description">
    Combined notes from Data 1 &amp; 2 with Anand Sokhey and Andy Phillips
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://stoneneilon.github.io/">Stone Neilon</a> <a href="https://orcid.org/0009-0006-6026-4384" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.colorado.edu/polisci/people/graduate-students/stone-neilon">
            PhD student of political science @ The University of Colorado Boulder
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 15, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="preface" class="level1">
<h1>Preface</h1>
<p>These notes were compiled in my first year of graduate school. These two classes cover simple to complex linear regression. Additionally, some other methods are discussed such as Logit/Probit, Causal Inference, and time-series. These other methods were only discussed in brief and require their own separate set of notes.</p>
<p>Reminder: These are notes and do not encompass every idea or detail associated with the concepts. They do not (and cannot) replace classes or reading the material.</p>
</section>
<section id="section-1-descriptive-statistics" class="level1">
<h1>Section 1: Descriptive Statistics</h1>
<p>While simple, it is critical you understand these. It is okay if you forget (and you will), just remember to keep reviewing. It helps to walk through the formula step by step. I have provided some commentary on the rationale behind the formula in the sections below.</p>
<section id="mean-average" class="level2">
<h2 class="anchored" data-anchor-id="mean-average">Mean (Average)</h2>
<p><span class="math display">\[
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}
\]</span></p>
<p>Above is the formula for mean (average).</p>
</section>
<section id="median" class="level2">
<h2 class="anchored" data-anchor-id="median">Median</h2>
<p><span class="math display">\[
\text{Median} = \begin{cases}       x_{\frac{n+1}{2}} &amp; \text{if } n \text{ is odd} \\      \frac{1}{2}(x_{\frac{n}{2}} + x_{\frac{n}{2} + 1}) &amp; \text{if } n \text{ is even}    \end{cases}
\]</span></p>
<p>Don’t worry about knowing this formula. Arrange everything in order. Select the middle number.</p>
</section>
<section id="mode" class="level2">
<h2 class="anchored" data-anchor-id="mode">Mode</h2>
<ol type="1">
<li><p>Another measure of central tendency</p>
<ol type="1">
<li><p>Mode is simply what number appears the most in our dataset.</p>
<ol type="1">
<li><p>{4,7,3,7,8,1,7,8,9,4,7}</p>
<ol type="1">
<li><p>Our mode would be 7</p>
<ol type="1">
<li>It appears the most.</li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
<li><p>We don’t use mode that much as a measure of central tendency but it still provides some information about the distribution.</p></li>
<li><p>Don’t worry about the formula</p></li>
</ol>
</section>
<section id="variance" class="level2">
<h2 class="anchored" data-anchor-id="variance">Variance</h2>
<p><span class="math display">\[
\text{Variance} (s^2) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}
\]</span></p>
<ol type="1">
<li><p>The average of the squared differences from the Mean.&nbsp;</p></li>
<li><p>Variance is a measure of SPREAD.</p></li>
<li><p>Let’s walk through the formula step by step.</p>
<ol type="1">
<li><p>The <span class="math inline">\(\Sigma\)</span> means to sum all the values together.</p></li>
<li><p><span class="math inline">\((x_i - \bar{x})\)</span></p>
<ol type="1">
<li><p>in this part we are taking each observation and subtracting it by the mean (average).</p></li>
<li><p>Now lets add the square term. <span class="math inline">\((x_i - \bar{x})^2\)</span></p>
<ol type="1">
<li><p>Why do we square this?</p>
<ol type="1">
<li><p>Imagine a number line from 0 to 100. We have some dataset where the mean is 50. Now let’s say one of our observations is 38. 38-50 = -12. See what happens!? We have a negative number. All observations to the left of our mean are negative while all observations to the right of our mean are positive.</p>
<ol type="1">
<li>When we add these all up without the square term, we get ZERO!</li>
</ol></li>
<li><p>Thus we square to accommodate for these canceling out.</p>
<ol type="1">
<li>There are other reasons we square but they aren’t relevant here and this is the main reason.</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Now the <span class="math inline">\(n-1\)</span></p>
<ol type="1">
<li><p>N represents the number of observations.</p>
<ol type="1">
<li><p>Why are we subtracting it by 1?</p>
<ol type="1">
<li><p>If we were calculating the population variance, then we wouldn’t subtract by 1. However, we are pretty much never working with the population. We are always using some samples.&nbsp;</p></li>
<li><p>This part is not super intuitive. BUT, we are using the sample mean, NOT the population mean to calculate the variance.</p>
<ol type="1">
<li><p>We don’t know what the “true” population mean is. We have an estimate of it using our sample. Thus, there is some uncertainty around the sample mean (we don’t know if the sample mean is = to the population mean). To account for this uncertainty we add a -1 to our denominator.&nbsp;</p>
<ol type="1">
<li><p>By subtracting 1 from the denominator this makes the spread a little larger to account for that uncertainty. Think about what happens when we make our denominator smaller compared to if we don’t. Example:</p>
<ol type="1">
<li><p><span class="math inline">\(\frac{16}{4-1}\)</span> vs.&nbsp;<span class="math inline">\(\frac{16}{4}\)</span></p>
<ol type="1">
<li>the one with the <span class="math inline">\(4-1\)</span> denominator will have a larger output and thus account for the uncertainty in our measurement.</li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="standard-deviation" class="level2">
<h2 class="anchored" data-anchor-id="standard-deviation">Standard Deviation</h2>
<p><span class="math display">\[
\text{Sample Standard Deviation} (s) = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}}
\]</span></p>
<ol type="1">
<li><p>Standard deviation is denoted by <span class="math inline">\(s\)</span> or <span class="math inline">\(\sigma\)</span> (lower case sigma).</p></li>
<li><p>Standard deviation represents how far the numbers are from each other.</p>
<ol type="1">
<li><p>Look how similar this equation is compared to the variance equation.</p>
<ol type="1">
<li>The standard deviation is the square root of the variance!</li>
</ol></li>
</ol></li>
<li><p>I won’t explain the whole formula again.</p>
<ol type="1">
<li><p>I will explain why we square root the equation</p>
<ol type="1">
<li>We take the square root to put the output back into its original units. Our output is in the same units as the mean.</li>
</ol></li>
</ol></li>
<li><p>Have a good understanding of standard deviation THEN understand standard error.</p></li>
</ol>
</section>
<section id="standard-error" class="level2">
<h2 class="anchored" data-anchor-id="standard-error">Standard Error</h2>
<p><span class="math display">\[
\text{Standard Error} (\text{SE}) = \frac{s}{\sqrt{n}}
\]</span></p>
<ol type="1">
<li><p>The numerator (s) is standard deviation!</p></li>
<li><p>What is standard error?</p>
<ol type="1">
<li>It is the standard deviation of the means!</li>
</ol></li>
<li><p>OK, so what is the difference between standard deviation and standard error?</p>
<ol type="1">
<li><p>Standard deviation quantifies the variation within a <strong>set</strong> of measurements. (singular)</p></li>
<li><p>Standard error quantifies the variation in the means from <strong>multiple</strong> sets of measurements. (multiple)</p>
<ol type="1">
<li><p>What is confusing is that we can get standard error from one single measurement, even though it describes the means from multiple sets. <strong>Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error.</strong></p>
<ol type="1">
<li><p>Just watch the damn video.</p>
<ol type="1">
<li><a href="https://www.youtube.com/watch?v=A82brFpdr9g" class="uri">https://www.youtube.com/watch?v=A82brFpdr9g</a></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Why do we take the square root of observations in the denominator?&nbsp;</p>
<ol type="1">
<li><p>By dividing by the square root of the sample size, we’re essentially adjusting for the fact that the standard deviation of the sampling distribution of the mean tends to decrease as the sample size increases. This is due to the Central Limit Theorem, which states that the sampling distribution of the sample mean becomes approximately normal as the sample size increases, with a mean equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size. (chat gpt gave me this and it is a kick ass explanation)</p>
<ol type="1">
<li>It is because of this that standard error gets smaller when we have more observations!&nbsp;</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="skewness" class="level2">
<h2 class="anchored" data-anchor-id="skewness">Skewness</h2>
<p>You do not need to know the formula. You just need to know what skewness looks like and how to properly identify when your data is skewed.</p>
<p>When our distribution has no skew, the mean, median, and mode are all the same value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="skewness.jpeg" class="img-fluid figure-img"></p>
<figcaption>Skewness Visualized</figcaption>
</figure>
</div>
<p>Positive skewness is also called “right skew”. Notice where the mean/median/mode are.</p>
<p>Negative skewness is also called “left skew”. Notice where the mean/median/mode are.</p>
</section>
<section id="covariance" class="level2">
<h2 class="anchored" data-anchor-id="covariance">Covariance</h2>
<p><span class="math display">\[
\text{Covariance} (\text{cov}(X, Y)) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1}
\]</span></p>
<ol type="1">
<li><p>A measure of how much two random variables vary together</p>
<ol type="1">
<li>Similar to variance BUT covariance deals with two variables.</li>
</ol></li>
<li><p>Let’s walk through the formula.</p>
<ol type="1">
<li><p>The numerator tells us to take the sum of each observation minus its mean for both variables (x and y). Then multiply.&nbsp;</p>
<ol type="1">
<li>Remember we divide by N-1 because we are using the sample means and not the population means. Thus we have a bit of uncertainty. By subtracting 1, it makes our denominator smaller and subsequently the output larger, representing the greater uncertainty.&nbsp;</li>
</ol></li>
</ol></li>
<li><p>The output of covariance is a number that tells us the <strong>direction</strong> of how these two variables vary together.&nbsp;</p>
<ol type="1">
<li>It does not tell us the strength of the relationship between the two variables.&nbsp;</li>
</ol></li>
<li><p>Look how similar this formula is compared to the variance formula!&nbsp;</p></li>
<li><p>Covariance is influenced by the scale of the variables.&nbsp;</p>
<ol start="2" type="1">
<li>Meaning its hard to read/understand by itself.</li>
</ol></li>
</ol>
</section>
<section id="correlation" class="level2">
<h2 class="anchored" data-anchor-id="correlation">Correlation</h2>
<p><span class="math display">\[
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
\]</span></p>
<ol type="1">
<li><p>This formula looks scary! It actually is super simple!</p>
<ol type="1">
<li><p>Does the numerator look familiar? Look back at the covariance formula! The top IS covariance!</p>
<ol type="1">
<li>The denominator looks familiar too! It is the standard deviation for x and y.</li>
</ol></li>
<li><p>Correlation is similar to covariance but it is more useful.</p>
<ol type="1">
<li><p>We interpret correlation from -1 to 1.</p>
<ol type="1">
<li><p>-1 represents a perfectly negative correlation</p>
<ol type="1">
<li>This will almost never happen</li>
</ol></li>
<li><p>1 represents a perfectly positive correlation</p>
<ol type="1">
<li>this will almost never happen.</li>
</ol></li>
</ol></li>
<li><p>Correlation tell us the direction and strength of a linear relationship between two variables.</p></li>
</ol></li>
</ol></li>
<li><p>Correlation is covariance divided by the product of the two variables standard deviation. So they measure the same thing but one belongs to [-1,1] and the other (covariance) take any value.</p></li>
</ol>
</section>
</section>
<section id="section-2-statisticalhypothesis-testing" class="level1">
<h1>Section 2: Statistical/Hypothesis Testing</h1>
<p>Can we believe this finding? Is this a <em>real</em> relationship? Hypothesis testing helps us answer those questions. There are different types of tests we can do. Below are the “basic” tests. More complicated models will use these tests in some manner. For example, Ordinary Least Squares (OLS) uses a t-test to help us understand whether the relationship (the beta) is statistically significant.</p>
<p>These test correspond with a specific distribution a.k.a a probability distribution. A probability distribution TK</p>
<p>When we use these tests, we are using their respective probability distributions.</p>
<section id="t-test" class="level2">
<h2 class="anchored" data-anchor-id="t-test">t-Test</h2>
<section id="probability-distribution" class="level3">
<h3 class="anchored" data-anchor-id="probability-distribution">Probability Distribution:</h3>
</section>
<section id="purpose" class="level3">
<h3 class="anchored" data-anchor-id="purpose">Purpose:</h3>
</section>
<section id="uses" class="level3">
<h3 class="anchored" data-anchor-id="uses">Uses:</h3>
</section>
</section>
<section id="chi2-test" class="level2">
<h2 class="anchored" data-anchor-id="chi2-test"><span class="math inline">\(\chi^2\)</span> Test</h2>
<section id="purpose-1" class="level3">
<h3 class="anchored" data-anchor-id="purpose-1">Purpose:</h3>
</section>
<section id="uses-1" class="level3">
<h3 class="anchored" data-anchor-id="uses-1">Uses:</h3>
</section>
</section>
<section id="analysis-of-variance-anova-test" class="level2">
<h2 class="anchored" data-anchor-id="analysis-of-variance-anova-test">Analysis of Variance (ANOVA) Test</h2>
<section id="purpose-2" class="level3">
<h3 class="anchored" data-anchor-id="purpose-2">Purpose:</h3>
</section>
<section id="uses-2" class="level3">
<h3 class="anchored" data-anchor-id="uses-2">Uses:</h3>
</section>
</section>
<section id="f-test" class="level2">
<h2 class="anchored" data-anchor-id="f-test">F-Test</h2>
<section id="purpose-3" class="level3">
<h3 class="anchored" data-anchor-id="purpose-3">Purpose:</h3>
</section>
<section id="uses-3" class="level3">
<h3 class="anchored" data-anchor-id="uses-3">Uses:</h3>
</section>
</section>
<section id="p-value" class="level2">
<h2 class="anchored" data-anchor-id="p-value">p-value</h2>
<p>Read very closely! So many people misinterpret this concept!</p>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition:</h3>
<p>A p-value is the probability of observing a test statistic value equal to or more extreme than the value you computed if the null were true.</p>
<p>Alternatively, the p-value is the probability of making a type I/II error.</p>
</section>
<section id="purpose-4" class="level3">
<h3 class="anchored" data-anchor-id="purpose-4">Purpose:</h3>
<p>If we assume the null hypothesis is true, then we could draw the sampling distribution centered around zero. By specifing the null hypothesis we can invoke the central limit theorem.</p>
<p>P-values help us determine how statistically significant our relationship is. Remember that we are using data we have to tell us about data we do not have.</p>
<p><img src="pvalue.png" class="img-fluid" width="684"></p>
</section>
<section id="uses-4" class="level3">
<h3 class="anchored" data-anchor-id="uses-4">Uses:</h3>
<p>The p-value is decided by the researcher. Convention typically sets the p-value at .10 and below. However, .10 is still not ideal, the lower the better.</p>
</section>
<section id="important-notes" class="level3">
<h3 class="anchored" data-anchor-id="important-notes">Important Notes:</h3>
<p>the p-value does not tell us anything substantive. It simply tells us</p>
</section>
</section>
</section>
<section id="section-3-regression-overviewreview" class="level1">
<h1>Section 3: Regression Overview/Review</h1>
<p>The big kahuna. There are other methods but regression is perhaps the most important. It is easy to understand, straightforward, and powerful. Your ability to understand it inside and out will make you smarter in every way. You are Tony Stark and regression is your Iron Man suit.</p>
<section id="assumptions-scalar-notation" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-scalar-notation">Assumptions (Scalar Notation)</h2>
<section id="assumption-1-epsilon_iis-normally-distributed" class="level3">
<h3 class="anchored" data-anchor-id="assumption-1-epsilon_iis-normally-distributed">Assumption 1: <span class="math inline">\(\epsilon_i\)</span>is normally distributed&nbsp;</h3>
<ol type="1">
<li>This assumption is for the purposes of constructing a hypothesis test.&nbsp;</li>
<li>We don’t need this to estimate our beta.&nbsp;</li>
<li>But by assuming the errors are normally distributed then we can run a hypothesis test (t-test) to see if we accept or reject the null hypothesis of beta = 0.&nbsp;</li>
</ol>
</section>
<section id="assumption-2-eepsilon_i0" class="level3">
<h3 class="anchored" data-anchor-id="assumption-2-eepsilon_i0">Assumption 2: <span class="math inline">\(E[\epsilon_i]=0\)</span></h3>
<ol type="1">
<li>The distance between the observed and fitted line is zero. (the residual is zero)&nbsp;
<ol type="1">
<li>This rarely happens <strong>BUT</strong> we estimate our <span class="math inline">\(\hat{\beta}\)</span>’s so that the error is as close to zero as possible.&nbsp;
<ol type="1">
<li>The goal is to have a line of best fit that does this for all observations.&nbsp;</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="assumption-3-varepsilon_isigma2" class="level3">
<h3 class="anchored" data-anchor-id="assumption-3-varepsilon_isigma2">Assumption 3: <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span></h3>
<ol type="1">
<li>This is homoscedasticity (or no heteroskedasticity).&nbsp;
<ol type="1">
<li><p>We want constant error variance.&nbsp;</p></li>
<li><p>Homoscedasticity visualized:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/image002.png" class="img-fluid figure-img" width="500"></p>
<figcaption>Homoscedasticity visualized (From Gujarati &amp; Porter)</figcaption>
</figure>
</div></li>
</ol></li>
</ol>
</section>
<section id="assumption-4-covepsilon_iepsilon_j0-foralli-neq-j" class="level3">
<h3 class="anchored" data-anchor-id="assumption-4-covepsilon_iepsilon_j0-foralli-neq-j">Assumption 4: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j)=0 \:\forall\:i \neq j\)</span></h3>
<ol type="1">
<li><p>This represents no autocorrelation&nbsp;</p></li>
<li><p>The disturbances of i and j are NOT correlated</p></li>
<li><p><span class="math inline">\(\forall\)</span> means “for all”</p></li>
</ol>
</section>
<section id="assumption-5-x_i-is-fixed-in-repeated-sampling" class="level3">
<h3 class="anchored" data-anchor-id="assumption-5-x_i-is-fixed-in-repeated-sampling">Assumption 5: <span class="math inline">\(x_i\)</span> is fixed in repeated sampling</h3>
<ol type="1">
<li>X values are independent fo the error term</li>
</ol>
</section>
<section id="assumption-6-sample-regression-model-correctly-specified" class="level3">
<h3 class="anchored" data-anchor-id="assumption-6-sample-regression-model-correctly-specified">Assumption 6: Sample regression model correctly specified&nbsp;</h3>
<ol type="1">
<li><p>Our sample regression equation correctly identifies the variables in the theoretical population regression model.&nbsp;</p></li>
<li><p>We include all relevant confounding variables.&nbsp;</p></li>
<li><p>No omitted variable bias.&nbsp;</p></li>
</ol>
</section>
<section id="assumption-7-covepsilon_ix_i0" class="level3">
<h3 class="anchored" data-anchor-id="assumption-7-covepsilon_ix_i0">Assumption 7: <span class="math inline">\(Cov(\epsilon_i,x_i)=0\)</span></h3>
<ol type="1">
<li>covariance between residuals and parameters is equal to zero</li>
</ol>
</section>
<section id="assumption-8-parametric-linearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-8-parametric-linearity">Assumption 8: Parametric Linearity</h3>
<ol type="1">
<li><p>Linear in the parameters.&nbsp;</p></li>
<li><p>We do NOT raise the betas to a power.&nbsp;</p>
<ol type="1">
<li><p>We can raise the variables (x’s) to a power and it remains linear.&nbsp;</p></li>
<li><p>Note: Logit models add a “link function”. This line is not linear but it is still a linear relationship.</p></li>
</ol></li>
</ol>
</section>
<section id="assumption-9-x_i-must-vary" class="level3">
<h3 class="anchored" data-anchor-id="assumption-9-x_i-must-vary">Assumption 9: <span class="math inline">\(x_i\)</span> must vary</h3>
<ol type="1">
<li>Duh. You can’t do anything if your X variable doesn’t vary.</li>
</ol>
</section>
<section id="assumption-10-n-k" class="level3">
<h3 class="anchored" data-anchor-id="assumption-10-n-k">Assumption 10: n &gt; k</h3>
<ol type="1">
<li>This relates to degrees of freedom.</li>
<li>We need more operations than we have parameters or else we do not have enough information to test a relationship.</li>
</ol>
</section>
<section id="assumption-11-no-perfect-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="assumption-11-no-perfect-multicollinearity">Assumption 11: No perfect multicollinearity</h3>
<ol type="1">
<li>We wouldn’t include a column for male and a column for female because that would be perfect multicollinearity.&nbsp;</li>
<li>Multicollinearity is not a big issue (it is natural there will be some level of collinearity between our variables).&nbsp;
<ol type="1">
<li>BUT perfect multicollinearity is bad and we do not want it.&nbsp;</li>
</ol></li>
<li>Multicollinearity can (in some cases) disappear as we increase the number of observations.</li>
<li>This is easy to see in matrix algebra or an excel sheet.</li>
</ol>
</section>
</section>
<section id="assumptions-matrix-notation" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-matrix-notation">Assumptions (Matrix Notation)</h2>
<p>Note: These assumptions are the EXACT same assumptions listed above. The difference is in notation. Why do we do this? I answer this later, but basically its a different way to write math that is more concise and easier to understand. <strong>We use matrix algebra/notation because as our model gets bigger, scalar notation gets more complicated to read/keep track of.</strong></p>
<p>Why do we bold letters? Bold letters represent matrices.</p>
<section id="assumption-1-linearity-in-the-parameters" class="level3">
<h3 class="anchored" data-anchor-id="assumption-1-linearity-in-the-parameters">Assumption 1: Linearity in the parameters</h3>
<p><span class="math inline">\(y_i=x_{i1}\beta_1+x_{i2}\beta_2+...+X_{iK}\beta_K+\epsilon_i\)</span></p>
<ol type="1">
<li>The model specifies a linear relationship between y and <strong>X</strong></li>
<li>Do not raise the <span class="math inline">\(\beta\)</span> to a power.</li>
</ol>
</section>
<section id="assumption-2-full-rank" class="level3">
<h3 class="anchored" data-anchor-id="assumption-2-full-rank">Assumption 2: Full rank</h3>
<ol type="1">
<li><strong>X</strong> is an n x K matrix with rank K
<ol type="1">
<li>There is no exact linear relationship among variables&nbsp;</li>
<li>Sometimes called the “identification condition”</li>
</ol></li>
<li>What is “rank”?
<ol type="1">
<li><p>It is the number of linearly independent columns&nbsp;</p>
<ol type="1">
<li>If the number of independent columns is equal to the total number of columns then the matrix is full rank.</li>
</ol></li>
</ol></li>
<li>This assumption relates to the scalar assumption of <strong>no perfect multicollinearity.</strong></li>
</ol>
</section>
<section id="assumption-3-exogeneity-of-the-independent-variables" class="level3">
<h3 class="anchored" data-anchor-id="assumption-3-exogeneity-of-the-independent-variables">Assumption 3: Exogeneity of the independent variables</h3>
<ol type="1">
<li><p>The independent variables contain <strong>NO</strong> predictive information about</p></li>
<li><p>The expected value of is not a function of the independent variables at any observation (including i):</p>
<ol type="1">
<li><p><span class="math inline">\(E[\epsilon_i|x_{j1},x_{j2},...,x_{jK}=0\)</span></p>
<ol type="1">
<li><span class="math inline">\(E[\epsilon_i|\textbf{X}]=0\)</span></li>
</ol></li>
</ol></li>
<li><p>What does this mean?</p>
<ol type="1">
<li><p>The independent variables are not influenced by the error term or any other unobserved factors in the model.</p></li>
<li><p>The X variable does not depend on the Y variable (reverse causality). We can’t have the Y variable influencing our regressors (that would be endogeneity)</p></li>
</ol></li>
</ol>
</section>
<section id="assumption-4-spherical-disturbances" class="level3">
<h3 class="anchored" data-anchor-id="assumption-4-spherical-disturbances">Assumption 4: Spherical disturbances</h3>
<ol type="1">
<li><p>No autocorrelation</p>
<ol type="1">
<li><span class="math inline">\(E[\text{Cov}(i,j|\mathbf{X})] = 0\: \forall \:i=j\)</span></li>
</ol></li>
<li><p>Assumed homoscedasticity</p>
<ol type="1">
<li><p><span class="math inline">\(E[\text{Var}(i|\mathbf{X})] = 2\: \forall\: i=1,2,\ldots,n\)</span></p></li>
<li><p>Why assumed?</p>
<ol type="1">
<li><p>Some level of heteroscedasticity is not fatal</p></li>
<li><p>We can fix it. But homoscedasticity is always preferable.</p></li>
</ol></li>
</ol></li>
<li><p>These two assumptions can be written mathematically into one single assumption using matrix algebra:</p>
<p><img src="sphericalpic.png" class="img-fluid" width="347"></p>
<ol type="1">
<li><p>The off-diagonal (the zeros) represent autocorrelation</p>
<ol type="1">
<li>if these are not zero (or at least very close) we have autocorrelation</li>
</ol></li>
<li><p>The main-diagonal (the variance) represents our homoscedasticity assumption</p>
<ol type="1">
<li>If these values along the main diagonal are not the same or at least very close, then we have heteroscedasticity.</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="assumption-5-data-generation" class="level3">
<h3 class="anchored" data-anchor-id="assumption-5-data-generation">Assumption 5: Data generation</h3>
<ol type="1">
<li><p>The data generating process of <strong>X</strong> and <span class="math inline">\(\epsilon\)</span> are independent.</p></li>
<li><p><strong>X</strong> is generated by a non-stochastic process</p></li>
<li><p>this assumption allows us to say “conditional on X”</p></li>
<li><p>This assumption is a bit confusing to me.</p>
<ol type="1">
<li><p>From my understanding, we want our X values to be fixed. We then take samples to see how our y values vary based on the fixed values of X.&nbsp;</p></li>
<li><p>Let’s say you want to predict annual income based on years of experience. Your manager gave you three lists of employees with their annual income. Each list corresponds to a particular experience level — let’s say 3 years, 6 years, and 10 years of experience respectively. Each list contains data on 50 employees. As you can see, the x-values are fixed(3, 6, 10), but have repeated samples (50 data points per sample). This is what is known as Non-stochastic regressors</p></li>
</ol></li>
</ol>
</section>
<section id="assumption-6-epsilon-is-normally-distributed" class="level3">
<h3 class="anchored" data-anchor-id="assumption-6-epsilon-is-normally-distributed">Assumption 6: <span class="math inline">\(\epsilon\)</span> is normally distributed</h3>
<ol type="1">
<li><p>This is useful for constructing our hypothesis tests and test statistics&nbsp;</p></li>
<li><p>Technically, we don’t need this for estimating our beta, just uncertainty surrounding it.</p></li>
</ol>
</section>
</section>
<section id="formula-for-deriving-beta" class="level2">
<h2 class="anchored" data-anchor-id="formula-for-deriving-beta">Formula for deriving <span class="math inline">\(\beta\)</span></h2>
<ul>
<li><p>Problem: We have two missing terms, 𝜷 and 𝛆. Knowing one of these will tell us the line. But since we don’t know either of these terms, how do we find it out?&nbsp;</p></li>
<li><p>We have to solve for beta. Solving for beta in Ordinary Least Squares (OLS) requires us to find a line of best fit that minimizes the unexplained difference (the error).&nbsp;</p></li>
<li><p>To do this we take the sum of the squared residuals&nbsp;&nbsp;</p>
<ul>
<li><p>It may help to understand this through the formula of the residual.&nbsp;</p>
<ul>
<li><p>First the residual is the amount our actual observed value differs from the predicted value (This is in matrix notation).&nbsp;</p>
<ul>
<li><p><span class="math inline">\((y-\textbf{X}\beta_0)\)</span></p>
<ul>
<li>y is our observed value and the <span class="math inline">\(\textbf{X}\beta_0\)</span> is our predicted value (the line of best fit).&nbsp;</li>
</ul></li>
<li><p>WE SQUARE THIS! SO NOW:&nbsp;</p>
<ul>
<li><p><span class="math inline">\((y-\textbf{X}\beta_0)'(y-\textbf{X}\beta_0)\)</span></p>
<ul>
<li><p>The (’) means transpose. It is matrix notation that allows us to multiply these two matrices (vectors).&nbsp;</p></li>
<li><p>Why do we square?&nbsp;</p>
<ul>
<li>We square the residuals for a bunch of reasons. Mainly: if we don’t, the residuals (Both positive and negative) cancel out.&nbsp;</li>
</ul></li>
</ul></li>
<li><p>Multiplying this through, we get:</p></li>
<li><p><span class="math inline">\(y'y-y'\textbf{X}\hat{\beta_0}'\textbf{X}'y+\hat{\beta_0}'\textbf{X}'\textbf{X}\)</span></p></li>
<li><p>You collect the terms and simply.&nbsp;</p></li>
<li><p><a href="https://www.youtube.com/watch?v=K_EH2abOp00" class="uri">https://www.youtube.com/watch?v=K_EH2abOp00</a> see for more</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>SO NOW: we want to find a line <span class="math inline">\(\hat{\beta_0}\)</span> such that the derivative (the tangent) is set to 0 aka the minimum, hence LEAST squares. Remember, we do not know the Beta.&nbsp;</p>
<ul>
<li><p>We set to zero to find the critical point (the minimum)&nbsp;</p></li>
<li><p>Taking the partial derivative with respect to beta, you’re essentially finding the point where the error function is not changing with respect to changes in beta. Where the slope of the error function with respect to beta is zero.&nbsp;</p></li>
<li><p><img src="beta-1.png" class="img-fluid" width="305"></p></li>
<li><p>Figure b is a visual representation of what this looks like when we set our minimum. We are finding the tangent line of the function that is equal to zero!</p></li>
</ul></li>
<li><p>The formula is</p>
<ul>
<li><p><span class="math inline">\(\hat{\beta}=(\textbf{X'X})^{-1}\textbf{X}'y\)</span></p>
<ul>
<li>this gives us the line of best fit. This is the formula R uses to calculate the beta/line.</li>
</ul></li>
</ul></li>
<li><p><strong>Controlling for other variables:</strong></p>
<ul>
<li><p><img src="beta-2.png" class="img-fluid" width="514"></p></li>
<li><p>Compare figure 3.3 to figure 3.2. They are the same thing. However in 3.3 we have added an additional dimension because of the additional variable. What we are doing remains the same however we now just have more dimensions and we are still trying to find the minimum of that parabola(?) plane(?)</p></li>
</ul></li>
</ul>
</section>
<section id="omega-matrix" class="level2">
<h2 class="anchored" data-anchor-id="omega-matrix">Omega Matrix</h2>
<ul>
<li><p>What the hell is an omega matrix <span class="math inline">\(\Omega\)</span>?</p>
<ul>
<li><p>The omega matrix is literally 𝛆𝛆’&nbsp;</p>
<ul>
<li><p>The error times its transpose.</p></li>
<li><p>We obviously can’t solve this without knowing what the errors are.&nbsp;</p>
<ul>
<li>This produces the variance covariance matrix (VCV) AKA covariance matrix of the errors.&nbsp;</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Why do we care about this matrix?&nbsp;</p>
<ul>
<li><p>We need the residuals to get our standard errors.&nbsp;</p></li>
<li><p>Additionally, this matrix is used to test our assumptions about the model. Specifically whether our model has autocorrelation and heteroskedasticity.&nbsp;</p></li>
<li><p><img src="sphericalpic.png" class="img-fluid"></p></li>
<li><p>This is basically what the omega matrix looks like. This photo however is what we want that omega matrix to look like (ours won’t always look like that). But we want the off diagonals to be zero (or effectively zero) and we want the main diagonal to be constant.&nbsp;</p>
<ul>
<li><p>If off-diagonal values are &gt; 0&nbsp;</p>
<ul>
<li>We have autocorrelation</li>
</ul></li>
<li><p>If main-diagonal values are not the same at each value</p>
<ul>
<li>We have heteroskedasticity.&nbsp;</li>
</ul></li>
</ul></li>
<li><p>NOTE: our omega matrix will NEVER be perfectly spherical.</p></li>
</ul></li>
</ul>
<section id="conversation-with-andy-about-omega-matrix" class="level3">
<h3 class="anchored" data-anchor-id="conversation-with-andy-about-omega-matrix">Conversation with Andy about Omega Matrix:</h3>
<p>I emailed Andy about this and figured it might be beneficial to include it here.</p>
<p><strong><em>Stone:</em></strong></p>
<p><em>I am looking back on your “Roll your own standard errors.r”. I see how the residual maker is part of the variance formula.</em></p>
<p><em># the formula for s^2 is e’e/(n-K)</em></p>
<p><em>s.2&nbsp;&lt;- (t(e)%*%e)/(length(y) - ncol(X))</em></p>
<p><em>##I ran this code individually and it gave me a scalar. I assume this is the sum of the squared error (SSE)?</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>Yes, divided by degrees of freedom, so it’s a variance</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>So, autocorrelation and heteroscedasticity manifest through the variance. Then: vcv &lt;- s.2*(solve(t(X)%*%X))</em></p>
<ul>
<li><em>This is our VCV of the X’s and then we take the square root of the diagonal to get our SE.</em></li>
</ul>
<p><em>We use the omega (and the assumptions of no spherical errors) to derive the equation for the SE (equation above). However, if we have spherical disturbances and use the same equation to derive our standard errors then our standard errors are wrong.</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>Yes,if there are non-spherical disturbances than our standard VCV above isn’t technically correct anymore b/c the equation doesn’t simplify to that.</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>Then the omega matrix (and its assumptions) is related to the population error. And thus, when we get a sample with spherical disturbances that does not match our expectations of the population error of no spherical disturbances, we then must fix it. Right?</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>Right…we can’t know what the population Omega is, but we can get a good guess based off our sample Omega matrix</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>So, if we switch the order, e%*%t(e) gives us the matrix of errors (WHICH IS NOT THE OMEGA MATRIX(?)). We want our matrix of errors to look like the omega matrix. It never will but we use the various tests to figure out the level of spherical errors that are present in this matrix.</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>The matrix of the errors IS the Omega matrix, which is the variance covariance matrix of the errors (note the other VCV for our X’s above). It’ll never be spherical perfectly but our assumptions are about expectations so it just needs to be consistently a problem (e.g., 2 errors can be correlated, but it’s only a problem if&nbsp;on average there’s a correlation between errors)</em></p>
<p><strong><em>Stone:</em></strong></p>
<p><em>Then when we detect spherical disturbances, we purge it or do whatever (FGLS, Robust/cluster SE), which then fixes our variance and then fixes our SE? Do I have all this right? This all feels kind of magical.</em></p>
<p><strong><em>Andy:</em></strong></p>
<p><em>If you’re running FGLS you’re using the info in the residual Omega to adjust both your SE’s and coefficients. If you’re correcting just your SEs you’re basically adjusting the standard SE&nbsp;formula&nbsp;to account for the pattern you want to correct for.</em></p>
</section>
</section>
<section id="standard-error-1" class="level2">
<h2 class="anchored" data-anchor-id="standard-error-1">Standard Error</h2>
<ol type="1">
<li><p>Standard Errors are not intuitive to me…but they are important</p></li>
<li><p><strong>Standard error is the standard deviation of the means.</strong>&nbsp;</p>
<ol type="1">
<li><p>The standard error quantifies the variation in the means from multiple sets of measurements.&nbsp;</p>
<ol type="1">
<li><p>What gets often confused is that the standard error can be estimated from a SINGLE set of measurements, even though it describes the means from multiple sets. Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error.&nbsp;</p>
<ol type="1">
<li>It is an estimate!</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>It is worth discussing standard deviation and its formula.</p>
<ol type="1">
<li><p><span class="math inline">\(\sigma = \sqrt{\frac{\sum_{i=1}^{N}(x_i - \mu)^2}{N}}\)</span></p>
<ol type="1">
<li><p>Above is the formula for standard deviation.&nbsp;</p>
<ol type="1">
<li>Note: the similarity of this to variance.&nbsp;</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Standard error formula is:</p>
<ol type="1">
<li><p><span class="math inline">\(SE = \frac{\sigma}{\sqrt{n}}\)</span></p>
<ol type="1">
<li>The s is the standard deviation! So all that in the standard deviation formula above is IN the standard error formula.&nbsp;</li>
</ol></li>
</ol></li>
<li><p>Why are standard errors important?&nbsp;</p>
<ol type="1">
<li><p>Need for precision of our coefficients</p>
<ol type="1">
<li>How precise of a claim can we make?&nbsp;</li>
</ol></li>
</ol></li>
<li><p>We make assumptions about our standard errors.**TK&nbsp;&nbsp;</p>
<ol type="1">
<li><p>They are normally distributed.&nbsp;</p>
<ol type="1">
<li>Not a big deal.&nbsp;</li>
</ol></li>
<li><p>Assuming the error term is independent and identically distributed.&nbsp;</p>
<ol type="1">
<li><p>Each individual error term follows the same distribution and is uncorrelated with each other.</p>
<ol type="1">
<li>Knowing an error term does not tell you anything about another error term.</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>Autocorrelation/heteroskedasticity do not bias our coefficient.&nbsp;</p></li>
<li><p>Presence of autocorrelation leads to an underestimation of the true standard errors.&nbsp;</p>
<ol type="1">
<li>Increases the possibility of making a type 1 error.&nbsp;</li>
</ol></li>
<li><p>Standard errors are useful for creating confidence intervals.&nbsp;</p></li>
</ol>
</section>
<section id="heteroscedasticity-spherical-disturbances" class="level2">
<h2 class="anchored" data-anchor-id="heteroscedasticity-spherical-disturbances">Heteroscedasticity (spherical disturbances)</h2>
<ol type="1">
<li>What is heteroskedasticity?&nbsp;
<ol type="1">
<li><p>Non-constant error variance.&nbsp;</p>
<ol type="1">
<li><p>See the picture at the beginning of the document of what homoscedasticity looks like. Heteroscedasticity is the opposite of that.&nbsp;</p>
<ol type="1">
<li><p>Think of our errors having a pattern or they “fan out”</p></li>
<li><p>Using the omega matrix again, it is when each value along the main diagonal is different.&nbsp;</p></li>
</ol></li>
</ol></li>
</ol></li>
<li>THIS AFFECTS OUR STANDARD ERROR!&nbsp;
<ol type="1">
<li>How?&nbsp;</li>
</ol></li>
<li>What does it do to our estimate?&nbsp;
<ol type="1">
<li><p>Our coefficient is unchanged.</p></li>
<li><p>However the efficiency of our model is influenced.&nbsp;</p></li>
</ol></li>
</ol>
</section>
<section id="autocorrelation-spherical-disturbances" class="level2">
<h2 class="anchored" data-anchor-id="autocorrelation-spherical-disturbances">Autocorrelation (spherical disturbances)</h2>
<ol type="1">
<li><p>What is autocorrelation?&nbsp;</p></li>
<li><p>THIS AFFECTS OUR STANDARD ERROR!&nbsp;</p>
<ol type="1">
<li>How?&nbsp;</li>
</ol></li>
</ol>
</section>
<section id="interactions" class="level2">
<h2 class="anchored" data-anchor-id="interactions">Interactions:</h2>
<p>Interactions are used when we believe the relationship is <em>conditional</em>. For example, X causes Y, only if Z is active. The effect of X on Y depends on the level of Z. In other words, the effect of one independent variable on the dependent variable is conditioned by another variable.</p>
<p>To accommodate a relationship such as this one, we multiply the two variables together rather than adding.</p>
<section id="interactions-increase-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="interactions-increase-multicollinearity">Interactions increase multicollinearity</h3>
<p>THIS IS OKAY.</p>
</section>
<section id="include-all-constitutive-terms" class="level3">
<h3 class="anchored" data-anchor-id="include-all-constitutive-terms">Include all constitutive terms</h3>
<p>It is essential that you include the constitutive terms and the interaction in the model.</p>
<p><strong>Wrong:</strong> Turnout = Age + Age*Race</p>
<p><strong>Correct:</strong> Turnout = Age + Race + Age*Race</p>
</section>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">Interpretation</h3>
<p>When interactions are dichotomous or categorical, interpretation is relatively easy. When the interaction includes a continuous variable, interpretation from the table becomes difficult.</p>
</section>
</section>
</section>
<section id="section-4-basic-questions-you-are-too-afraid-to-ask" class="level1">
<h1>Section 4: Basic Questions You Are Too Afraid To Ask</h1>
<section id="what-are-moments" class="level2">
<h2 class="anchored" data-anchor-id="what-are-moments">What are moments?</h2>
<p>Moments describe the probability distribution. Think of the shape of the density plot. Technically, two unique distributions could have the same mean or median. However, we need moments to help us better understand the distribution shape.</p>
<section id="mean" class="level3">
<h3 class="anchored" data-anchor-id="mean">Mean</h3>
<p><span class="math display">\[
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}
\]</span></p>
</section>
<section id="median-1" class="level3">
<h3 class="anchored" data-anchor-id="median-1">Median</h3>
<p>asdf</p>
</section>
<section id="mode-1" class="level3">
<h3 class="anchored" data-anchor-id="mode-1">Mode</h3>
<p>fdsaf</p>
</section>
<section id="kurtosis" class="level3">
<h3 class="anchored" data-anchor-id="kurtosis">Kurtosis</h3>
<p>sadf</p>
</section>
</section>
<section id="why-do-we-use-matrix-algebra-scalar-notation-seems-fine" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-use-matrix-algebra-scalar-notation-seems-fine">Why do we use matrix algebra? Scalar notation seems fine…</h2>
<p>There are a lot of reasons. In relation to regression, matrix algebra becomes essential because doing this in scalar notation turns into hell. It is simply too hard to do all of that once you get more and more variables.&nbsp;</p>
<p>Secondly, it is how R and other coding languages calculate the coefficient. Why? Long story short, it is less taxing on your computer to do these calculations. Besides more computer sciencey explanations, your computer is doing matrix algebra all the time.&nbsp;</p>
<p>Finally, matrix algebra will be used in further methods classes. This is especially important in machine learning. You are working with an array now but in machine learning, those arrays gain more dimensions. Imagine a matrix stacked upon another matrix and another. These are called tensors. Don’t worry, you don’t have to deal with these, ever…unless you want to. TK</p>
</section>
<section id="what-is-the-difference-between-covariance-and-correlation" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-difference-between-covariance-and-correlation">What is the difference between covariance and correlation?</h2>
<p>Correlation is covariance divided by the product of the two variables standard deviation. So they measure the same thing but correlation gives an output bounded to [-1,1] and the covariance takes on the same value as the constitutive terms.</p>
<p>Correlation is a normalization of covariance. Covariance is hard to interpret because the scale depends on the variances of two inputs. If you see a covariance of 11,350 or 2,489, you don’t know what those mean or even which set of variables have a high correlation. Correlation divides variance out and rescales to the interval [-1, 1], so now you can make those comparisons. Correlation is covariance but has greater readability and usefulness.</p>
</section>
<section id="what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept" class="level2">
<h2 class="anchored" data-anchor-id="what-do-dummy-variables-do-to-the-line-why-dont-they-change-the-slope-how-come-they-only-shift-the-intercept">What do dummy variables do to the line? Why don’t they change the slope? How come they only shift the intercept?</h2>
<p>A dummy variable is a variable coded in binary (0 or 1). Dummy variables can be a factor (0, 1, 2, 3, etc.)</p>
</section>
<section id="what-is-the-difference-between-variance-and-standard-deviation" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-difference-between-variance-and-standard-deviation">What is the difference between variance and standard deviation?</h2>
</section>
<section id="why-is-ordinary-least-squares-ols-so-powerful" class="level2">
<h2 class="anchored" data-anchor-id="why-is-ordinary-least-squares-ols-so-powerful">Why is Ordinary Least Squares (OLS) so powerful?</h2>
<p>The power of OLS becomes somewhat clearer as you learn about different methods. OLS is powerful because it is extremely easy to interpret. The interpretation of OLS is easy because we are specifying a linear relationship.</p>
<p>OLS power comes from the popularly known Gauss-Markov assumptions. If these assumptions are met, OLS is BLUE - Best Unbiased Linear Estimator.</p>
<p>Despite its power, OLS has shortfalls. However, it is still important to know OLS, as many methods serve as <em>extensions</em> of OLS and adapt it to better fit the data.</p>
</section>
<section id="when-is-ols-not-good-why-use-other-ones" class="level2">
<h2 class="anchored" data-anchor-id="when-is-ols-not-good-why-use-other-ones">When is OLS not good? Why use other ones?</h2>
<p>OLS has numerous advantages. However, OLS has shortfalls that other methods can fix/correct.</p>
<ol type="1">
<li>OLS is not good with a categorical dependent variable.</li>
</ol>
</section>
<section id="how-is-standard-error-difference-from-standard-deviation" class="level2">
<h2 class="anchored" data-anchor-id="how-is-standard-error-difference-from-standard-deviation">How is standard error difference from standard deviation?</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=A82brFpdr9g" class="uri">https://www.youtube.com/watch?v=A82brFpdr9g</a>
<ul>
<li>Watch the video.</li>
</ul></li>
<li>Standard deviation quantifies the variation within a set of measurements. Standard error quantifies the variation in the MEANS from multiple sets of measurements.</li>
<li>This gets confusing because we can estimate standard error off of one measurement.&nbsp;</li>
<li>Watch the video. Seriously, just watch the damn video.</li>
</ul>
</section>
<section id="are-the-assumptions-about-regression-related-to-the-sample-or-population" class="level2">
<h2 class="anchored" data-anchor-id="are-the-assumptions-about-regression-related-to-the-sample-or-population">Are the assumptions about regression related to the sample or population?</h2>
<ul>
<li><p>This was originally a question on Andy’s midterm. I got it wrong. :(</p></li>
<li><p>The assumptions relate to the population.</p>
<ul>
<li><p>We test these assumptions using our sample.</p></li>
<li><p>We use samples to tell us what we think the true (population) relationship is.</p>
<ul>
<li>Using data we have to tell us about data we do not have.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="why-is-it-called-ordinary-least-squares-ols" class="level2">
<h2 class="anchored" data-anchor-id="why-is-it-called-ordinary-least-squares-ols">Why is it called Ordinary Least Squares (OLS)?</h2>
</section>
<section id="what-is-variance-and-why-is-it-important" class="level2">
<h2 class="anchored" data-anchor-id="what-is-variance-and-why-is-it-important">What is variance and why is it important?</h2>
<p><span class="math display">\[
\text{Variance} (s^2) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}
\]</span></p>
<p>Variance is a measure of spread. Variance helps us understand the <em>dispersion</em> or <em>variability</em> in a data set. Variance estimates how far a set of numbers are spread out from the mean value. It can be difficult to interpret based on the output alone. This is because these values are squared, so we can’t really tell based on the number alone whether the value is relatively high or low.</p>
<p>Understanding variance is critical in statistics. Variance is integral to the efficiency of our estimators. That is, how accurate our model is.</p>
</section>
<section id="why-do-we-care-so-much-about-standard-errors" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-care-so-much-about-standard-errors">Why do we care so much about standard errors?</h2>
</section>
<section id="i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do" class="level2">
<h2 class="anchored" data-anchor-id="i-am-having-trouble-visualizing-ols-with-many-variables.-what-do-i-do">I am having trouble visualizing OLS with many variables. What do I do?</h2>
<p>Not much. We are pretty limited to understanding things in three dimensions. Imagine you have 8 variables in your OLS model. Try to draw an 8 dimensional model that shows the relationship. It is impossible.</p>
</section>
<section id="instrumental-variables-what-are-they-will-i-use-them-should-i-use-them" class="level2">
<h2 class="anchored" data-anchor-id="instrumental-variables-what-are-they-will-i-use-them-should-i-use-them">Instrumental variables, what are they? Will I use them? Should I use them?</h2>
<p>Instrumental variables are somewhat rare and frowned upon (?) in political science. To be a good instrumental variable, instrumental variables must satisfy two conditions:</p>
<ol type="1">
<li><p>The instrumental variable is theoretically relevant to x.</p></li>
<li><p>The instrumental variable must satisfy the exclusion restriction.</p></li>
</ol>
<p>The first point requires that our instrument (z) must be endogenous to our independent variable (x)</p>
<p>The exclusion restriction is typically where instrumental variables get attacked. The instrumental variable (<em>z</em> in this case) must only affect X. Z-&gt; X -&gt; Y. The difficulty to this condition is that there is no statistical test. The exclusion restriction must be defended by theory.</p>
<p>It is very difficult to find an instrument that is both related to X and does not affect Y. An example of a good instrument is provided below (thank you ChrisP from StackExchange):</p>
<p><em>“For example, suppose we want to estimate the effect of police (</em>𝑥<em>) on crime (</em>𝑦<em>) in a cross-section of cities. One issue is that places with lots of crime will hire more police. We therefore seek an instrument</em> 𝑧<em>𝑧 that is correlated with the size of the police force, but unrelated to crime.</em></p>
<p><em>One possible</em> 𝑧 <em>is number of firefighters. The assumptions are that cities with lots of firefighters also have large police forces (relevance) and that firefighters do not affect crime (exclusion). Relevance can be checked with the reduced form regressions, but whether firefighters also affect crime is something to be argued for. Theoretically, they do not and are therefore a valid instrument.”</em></p>
<section id="why-should-we-use-an-instrumental-variable" class="level3">
<h3 class="anchored" data-anchor-id="why-should-we-use-an-instrumental-variable">Why should we use an instrumental variable?</h3>
<p>The need for an instrumental variable arises when we are concerned for confounding variables or measurement error.</p>
</section>
</section>
<section id="should-we-care-about-r2" class="level2">
<h2 class="anchored" data-anchor-id="should-we-care-about-r2">Should we care about <span class="math inline">\(R^2\)</span>?</h2>
<p>It depends on your question. Chances are you want to find some variable (X) that causes another variable (Y). In this instance, your <span class="math inline">\(R^2\)</span> is mostly irrelevant. You want to see whether that X variable is statistically having an effect on your Y variable. For example, my data 1 project was on the relationship between walkability and voter turnout. I wanted to see if the walkability of an area had an impact on voter turnout in 2016, 2018, and 2020 general elections. Once I accounted for confounding variables, all I cared about was the significance of my variable of interest (walkability). <span class="math inline">\(R^2\)</span> told me nothing that helped me answer this question.&nbsp;</p>
<p>However, R^2 is very important for questions surrounding prediction. TK</p>
<p>Also we should focus on adjusted R^2</p>
</section>
<section id="everyone-talks-about-endogeneity.-what-is-it" class="level2">
<h2 class="anchored" data-anchor-id="everyone-talks-about-endogeneity.-what-is-it">Everyone talks about endogeneity. What is it?!</h2>
<p>Endogeneity is when the error term is correlated with the X. Remember that the error term contains everything <strong>not</strong> in our model (everything that determines Y but is NOT X, will be in our error term). If any of those things <strong>not</strong> in our model (the error) are related to our X and affect Y, then we have endogeneity. Endogeneity relates to confounders.</p>
<p>Endogeneity leads to bias in our coefficient.</p>
</section>
<section id="what-is-orthogonal" class="level2">
<h2 class="anchored" data-anchor-id="what-is-orthogonal">What is orthogonal?</h2>
<p>This concept was always a bit confusing as it can have different meaning in different contexts.</p>
<p>ORTHOGONAL MEANS INDEPENDENT</p>
<p>Simply put, orthogonality means “uncorrelated”. An orthogonal model means that all independent variables in that model are uncorrelated. If one or more independent variables are correlated, then that model is <strong>non-orthogonal (</strong>statisticshowto.com)</p>
</section>
<section id="is-ols-a-causal-inference-model" class="level2">
<h2 class="anchored" data-anchor-id="is-ols-a-causal-inference-model">Is OLS a causal Inference model?</h2>
</section>
<section id="why-do-we-use-the-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-use-the-normal-distribution">Why do we use the normal distribution?</h2>
</section>
</section>
<section id="section-5-notation" class="level1">
<h1>Section 5: Notation</h1>
</section>
<section id="section-6-list-of-resources-that-helped-me" class="level1">
<h1>Section 6: List of Resources That Helped Me</h1>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{neilon2024,
  author = {Neilon, Stone},
  title = {Data 1 \&amp; 2},
  date = {2024-05-15},
  url = {https://stoneneilon.github.io/notes/Data/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-neilon2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Neilon, Stone. 2024. <span>“Data 1 &amp; 2.”</span> May 15, 2024. <a href="https://stoneneilon.github.io/notes/Data/">https://stoneneilon.github.io/notes/Data/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>