<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stone Neilon">
<meta name="dcterms.date" content="2024-05-15">
<meta name="description" content="Data Comps Prep">

<title>Stone Neilon - Data Comps Prep</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stone Neilon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/file/d/1RgKR73KnxS3K_AP2nqiTiZZzAL-zJZ2N/view?usp=sharing"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preface" id="toc-preface" class="nav-link active" data-scroll-target="#preface">Preface</a></li>
  <li><a href="#section-1-notation" id="toc-section-1-notation" class="nav-link" data-scroll-target="#section-1-notation">Section 1: Notation</a></li>
  <li><a href="#section-x-abused-words-in-the-discipline" id="toc-section-x-abused-words-in-the-discipline" class="nav-link" data-scroll-target="#section-x-abused-words-in-the-discipline">Section X: Abused Words in the Discipline</a></li>
  <li><a href="#section-x-ordinary-least-squares-ols" id="toc-section-x-ordinary-least-squares-ols" class="nav-link" data-scroll-target="#section-x-ordinary-least-squares-ols">Section X: Ordinary Least Squares (OLS)</a>
  <ul class="collapse">
  <li><a href="#the-gaus-markov-assumptions-of-ols" id="toc-the-gaus-markov-assumptions-of-ols" class="nav-link" data-scroll-target="#the-gaus-markov-assumptions-of-ols">The (Gaus-Markov) assumptions of OLS</a>
  <ul class="collapse">
  <li><a href="#assumption-1-linearity-in-the-parameters" id="toc-assumption-1-linearity-in-the-parameters" class="nav-link" data-scroll-target="#assumption-1-linearity-in-the-parameters">Assumption 1: Linearity in the parameters</a></li>
  <li><a href="#assumption-2-full-rank" id="toc-assumption-2-full-rank" class="nav-link" data-scroll-target="#assumption-2-full-rank">Assumption 2: Full rank</a></li>
  <li><a href="#assumption-3-exogeneity-of-the-independent-variables" id="toc-assumption-3-exogeneity-of-the-independent-variables" class="nav-link" data-scroll-target="#assumption-3-exogeneity-of-the-independent-variables">Assumption 3: Exogeneity of the independent variables</a></li>
  <li><a href="#assumption-4-spherical-disturbances" id="toc-assumption-4-spherical-disturbances" class="nav-link" data-scroll-target="#assumption-4-spherical-disturbances">Assumption 4: Spherical disturbances</a></li>
  <li><a href="#assumption-5-data-generation" id="toc-assumption-5-data-generation" class="nav-link" data-scroll-target="#assumption-5-data-generation">Assumption 5: Data generation</a></li>
  <li><a href="#assumption-6-epsilon-is-normally-distributed" id="toc-assumption-6-epsilon-is-normally-distributed" class="nav-link" data-scroll-target="#assumption-6-epsilon-is-normally-distributed">Assumption 6: <span class="math inline">\(\epsilon\)</span> is normally distributed</a></li>
  </ul></li>
  <li><a href="#residuals-the-error" id="toc-residuals-the-error" class="nav-link" data-scroll-target="#residuals-the-error">Residuals (the error)</a></li>
  <li><a href="#controlling-for-other-variables" id="toc-controlling-for-other-variables" class="nav-link" data-scroll-target="#controlling-for-other-variables">Controlling for other variables</a>
  <ul class="collapse">
  <li><a href="#frisch-waugh-lovell-fwl-theorem" id="toc-frisch-waugh-lovell-fwl-theorem" class="nav-link" data-scroll-target="#frisch-waugh-lovell-fwl-theorem">Frisch-Waugh-Lovell (FWL) Theorem</a></li>
  </ul></li>
  <li><a href="#the-interaction" id="toc-the-interaction" class="nav-link" data-scroll-target="#the-interaction">The interaction</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals">Confidence Intervals</a></li>
  <li><a href="#p-values" id="toc-p-values" class="nav-link" data-scroll-target="#p-values">P-Values</a>
  <ul class="collapse">
  <li><a href="#why-is-the-p-value-over-rated" id="toc-why-is-the-p-value-over-rated" class="nav-link" data-scroll-target="#why-is-the-p-value-over-rated">Why is the p-value over-rated?</a></li>
  <li><a href="#what-the-p-value-is-not" id="toc-what-the-p-value-is-not" class="nav-link" data-scroll-target="#what-the-p-value-is-not">What the p-value is NOT</a></li>
  </ul></li>
  <li><a href="#the-pitfalls-of-ols" id="toc-the-pitfalls-of-ols" class="nav-link" data-scroll-target="#the-pitfalls-of-ols">The pitfalls of OLS</a></li>
  </ul></li>
  <li><a href="#section-x-functional-form" id="toc-section-x-functional-form" class="nav-link" data-scroll-target="#section-x-functional-form">Section X: Functional Form</a>
  <ul class="collapse">
  <li><a href="#which-functional-form" id="toc-which-functional-form" class="nav-link" data-scroll-target="#which-functional-form">Which Functional Form?</a></li>
  <li><a href="#log-transformation-interpretations" id="toc-log-transformation-interpretations" class="nav-link" data-scroll-target="#log-transformation-interpretations">Log Transformation Interpretations:</a></li>
  <li><a href="#polynomials-of-x" id="toc-polynomials-of-x" class="nav-link" data-scroll-target="#polynomials-of-x">Polynomials of x</a></li>
  <li><a href="#our-models-are-still-linear" id="toc-our-models-are-still-linear" class="nav-link" data-scroll-target="#our-models-are-still-linear">Our Models are Still Linear</a></li>
  </ul></li>
  <li><a href="#section-x-likelihood" id="toc-section-x-likelihood" class="nav-link" data-scroll-target="#section-x-likelihood">Section X: Likelihood</a></li>
  <li><a href="#section-x-maximum-likelihood-estimation-mle" id="toc-section-x-maximum-likelihood-estimation-mle" class="nav-link" data-scroll-target="#section-x-maximum-likelihood-estimation-mle">Section X: Maximum Likelihood Estimation (MLE)</a>
  <ul class="collapse">
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a></li>
  <li><a href="#the-structure-of-mle" id="toc-the-structure-of-mle" class="nav-link" data-scroll-target="#the-structure-of-mle">The Structure of MLE</a>
  <ul class="collapse">
  <li><a href="#stochastic-component" id="toc-stochastic-component" class="nav-link" data-scroll-target="#stochastic-component">Stochastic Component</a></li>
  <li><a href="#systematic-component" id="toc-systematic-component" class="nav-link" data-scroll-target="#systematic-component">Systematic Component</a></li>
  <li><a href="#the-link-function" id="toc-the-link-function" class="nav-link" data-scroll-target="#the-link-function">The Link Function</a></li>
  </ul></li>
  <li><a href="#estimating-beta-in-mle" id="toc-estimating-beta-in-mle" class="nav-link" data-scroll-target="#estimating-beta-in-mle">Estimating <span class="math inline">\(\beta\)</span> in MLE</a>
  <ul class="collapse">
  <li><a href="#mle-uses-logarithms" id="toc-mle-uses-logarithms" class="nav-link" data-scroll-target="#mle-uses-logarithms">MLE uses Logarithms</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-x-selecting-what-model-to-use" id="toc-section-x-selecting-what-model-to-use" class="nav-link" data-scroll-target="#section-x-selecting-what-model-to-use">Section X: Selecting What Model To Use</a>
  <ul class="collapse">
  <li><a href="#a-not-so-quick-aside" id="toc-a-not-so-quick-aside" class="nav-link" data-scroll-target="#a-not-so-quick-aside">A Not-So Quick Aside</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#dichotomousbinary-dependent-variable" id="toc-dichotomousbinary-dependent-variable" class="nav-link" data-scroll-target="#dichotomousbinary-dependent-variable">Dichotomous/Binary Dependent Variable</a>
  <ul class="collapse">
  <li><a href="#linear-probability-model-lpm" id="toc-linear-probability-model-lpm" class="nav-link" data-scroll-target="#linear-probability-model-lpm">Linear Probability Model (LPM)</a></li>
  <li><a href="#logistic-regression-logit" id="toc-logistic-regression-logit" class="nav-link" data-scroll-target="#logistic-regression-logit">Logistic Regression (Logit)</a></li>
  <li><a href="#probit" id="toc-probit" class="nav-link" data-scroll-target="#probit">Probit</a></li>
  <li><a href="#logit-or-probit" id="toc-logit-or-probit" class="nav-link" data-scroll-target="#logit-or-probit">Logit or Probit?</a></li>
  </ul></li>
  <li><a href="#multiple-categoricalordinal-variable-choice-models" id="toc-multiple-categoricalordinal-variable-choice-models" class="nav-link" data-scroll-target="#multiple-categoricalordinal-variable-choice-models">Multiple Categorical/Ordinal Variable (Choice Models)</a>
  <ul class="collapse">
  <li><a href="#ordered-logit" id="toc-ordered-logit" class="nav-link" data-scroll-target="#ordered-logit">Ordered Logit</a></li>
  <li><a href="#multinomial-logit" id="toc-multinomial-logit" class="nav-link" data-scroll-target="#multinomial-logit">Multinomial Logit</a></li>
  </ul></li>
  <li><a href="#selection-models" id="toc-selection-models" class="nav-link" data-scroll-target="#selection-models">Selection Models</a>
  <ul class="collapse">
  <li><a href="#heckman-selection-model" id="toc-heckman-selection-model" class="nav-link" data-scroll-target="#heckman-selection-model">Heckman Selection Model</a></li>
  </ul></li>
  <li><a href="#count-models" id="toc-count-models" class="nav-link" data-scroll-target="#count-models">Count Models</a>
  <ul class="collapse">
  <li><a href="#poisson" id="toc-poisson" class="nav-link" data-scroll-target="#poisson">Poisson</a></li>
  <li><a href="#negative-binomial" id="toc-negative-binomial" class="nav-link" data-scroll-target="#negative-binomial">Negative Binomial</a></li>
  <li><a href="#poisson-or-negative-binomial-model" id="toc-poisson-or-negative-binomial-model" class="nav-link" data-scroll-target="#poisson-or-negative-binomial-model">Poisson or Negative Binomial Model?</a></li>
  </ul></li>
  <li><a href="#extensions-of-count-models" id="toc-extensions-of-count-models" class="nav-link" data-scroll-target="#extensions-of-count-models">Extensions of Count Models</a>
  <ul class="collapse">
  <li><a href="#zero-inflated-models" id="toc-zero-inflated-models" class="nav-link" data-scroll-target="#zero-inflated-models">Zero-inflated models</a></li>
  <li><a href="#truncated-count-models" id="toc-truncated-count-models" class="nav-link" data-scroll-target="#truncated-count-models">Truncated count models</a></li>
  <li><a href="#hurdle-models" id="toc-hurdle-models" class="nav-link" data-scroll-target="#hurdle-models">Hurdle Models</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-x-panel-data" id="toc-section-x-panel-data" class="nav-link" data-scroll-target="#section-x-panel-data">Section X: Panel Data</a>
  <ul class="collapse">
  <li><a href="#fixed-effects" id="toc-fixed-effects" class="nav-link" data-scroll-target="#fixed-effects">Fixed Effects</a></li>
  <li><a href="#fixed-effects-does-not-fix-everything" id="toc-fixed-effects-does-not-fix-everything" class="nav-link" data-scroll-target="#fixed-effects-does-not-fix-everything">Fixed effects does not fix everything</a>
  <ul class="collapse">
  <li><a href="#example-of-fixed-effects-not-being-a-causal-estimate" id="toc-example-of-fixed-effects-not-being-a-causal-estimate" class="nav-link" data-scroll-target="#example-of-fixed-effects-not-being-a-causal-estimate">Example of fixed effects not being a causal estimate</a></li>
  <li><a href="#clustered-standard-errors" id="toc-clustered-standard-errors" class="nav-link" data-scroll-target="#clustered-standard-errors">Clustered Standard Errors</a></li>
  </ul></li>
  <li><a href="#random-effects" id="toc-random-effects" class="nav-link" data-scroll-target="#random-effects">Random Effects</a></li>
  </ul></li>
  <li><a href="#section-x-standard-errors-and-their-fixes" id="toc-section-x-standard-errors-and-their-fixes" class="nav-link" data-scroll-target="#section-x-standard-errors-and-their-fixes">Section X: Standard Errors and their Fixes</a>
  <ul class="collapse">
  <li><a href="#back-to-basics-what-is-standard-error" id="toc-back-to-basics-what-is-standard-error" class="nav-link" data-scroll-target="#back-to-basics-what-is-standard-error">Back to Basics: What is standard error?</a></li>
  <li><a href="#why-should-we-care-about-our-standard-errors" id="toc-why-should-we-care-about-our-standard-errors" class="nav-link" data-scroll-target="#why-should-we-care-about-our-standard-errors">Why should we care about our standard errors?</a></li>
  <li><a href="#variance-co-variance-matrix-vcv" id="toc-variance-co-variance-matrix-vcv" class="nav-link" data-scroll-target="#variance-co-variance-matrix-vcv">Variance Co-Variance Matrix (VCV)</a>
  <ul class="collapse">
  <li><a href="#vcv-example" id="toc-vcv-example" class="nav-link" data-scroll-target="#vcv-example">VCV Example</a></li>
  <li><a href="#how-do-we-get-standard-errors-using-maximum-likelihood-estimation-mle." id="toc-how-do-we-get-standard-errors-using-maximum-likelihood-estimation-mle." class="nav-link" data-scroll-target="#how-do-we-get-standard-errors-using-maximum-likelihood-estimation-mle.">How do we get standard errors using Maximum Likelihood Estimation (MLE).</a></li>
  </ul></li>
  <li><a href="#what-can-influence-our-standard-errors" id="toc-what-can-influence-our-standard-errors" class="nav-link" data-scroll-target="#what-can-influence-our-standard-errors">What can influence our standard errors?</a>
  <ul class="collapse">
  <li><a href="#does-including-lots-of-control-variables-influence-our-standard-errors" id="toc-does-including-lots-of-control-variables-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-including-lots-of-control-variables-influence-our-standard-errors">Does including lots of control variables influence our standard errors?</a></li>
  <li><a href="#does-transforming-a-nonlinear-relationship-by-logging-x-influence-our-standard-errors" id="toc-does-transforming-a-nonlinear-relationship-by-logging-x-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-transforming-a-nonlinear-relationship-by-logging-x-influence-our-standard-errors">Does transforming a nonlinear relationship by logging x influence our standard errors?</a></li>
  <li><a href="#does-positive-spatial-autocorrelation-influence-our-standard-errors" id="toc-does-positive-spatial-autocorrelation-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-positive-spatial-autocorrelation-influence-our-standard-errors">Does positive spatial autocorrelation influence our standard errors?</a></li>
  <li><a href="#does-the-error-term-not-being-normally-distributed-influence-our-standard-errors" id="toc-does-the-error-term-not-being-normally-distributed-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-the-error-term-not-being-normally-distributed-influence-our-standard-errors">Does the error term not being normally distributed influence our standard errors?</a></li>
  <li><a href="#does-measurement-error-associated-with-the-dependent-variable-influence-our-standard-errors" id="toc-does-measurement-error-associated-with-the-dependent-variable-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-measurement-error-associated-with-the-dependent-variable-influence-our-standard-errors">Does measurement error associated with the dependent variable influence our standard errors?</a></li>
  <li><a href="#does-a-predictor-being-correlated-with-the-error-term-influence-our-standard-errors" id="toc-does-a-predictor-being-correlated-with-the-error-term-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-a-predictor-being-correlated-with-the-error-term-influence-our-standard-errors">Does a predictor being correlated with the error term influence our standard errors?</a></li>
  <li><a href="#does-modeling-country-differences-through-the-inclusion-of-fixed-effects-influence-our-standard-errors" id="toc-does-modeling-country-differences-through-the-inclusion-of-fixed-effects-influence-our-standard-errors" class="nav-link" data-scroll-target="#does-modeling-country-differences-through-the-inclusion-of-fixed-effects-influence-our-standard-errors">Does modeling country differences through the inclusion of fixed effects influence our standard errors?</a></li>
  </ul></li>
  <li><a href="#the-omega-matrix" id="toc-the-omega-matrix" class="nav-link" data-scroll-target="#the-omega-matrix">The Omega Matrix</a></li>
  <li><a href="#the-sandwich-estimator" id="toc-the-sandwich-estimator" class="nav-link" data-scroll-target="#the-sandwich-estimator">The Sandwich Estimator</a></li>
  <li><a href="#robust-standard-errors" id="toc-robust-standard-errors" class="nav-link" data-scroll-target="#robust-standard-errors">Robust Standard Errors</a>
  <ul class="collapse">
  <li><a href="#the-math-of-robust-standard-errors" id="toc-the-math-of-robust-standard-errors" class="nav-link" data-scroll-target="#the-math-of-robust-standard-errors">The Math of Robust Standard Errors</a></li>
  <li><a href="#the-code-for-robust-standard-errors" id="toc-the-code-for-robust-standard-errors" class="nav-link" data-scroll-target="#the-code-for-robust-standard-errors">The code for robust standard errors</a></li>
  </ul></li>
  <li><a href="#clustered-standard-errors-1" id="toc-clustered-standard-errors-1" class="nav-link" data-scroll-target="#clustered-standard-errors-1">Clustered Standard Errors</a>
  <ul class="collapse">
  <li><a href="#the-math-of-clustered-standard-errors" id="toc-the-math-of-clustered-standard-errors" class="nav-link" data-scroll-target="#the-math-of-clustered-standard-errors">The Math of Clustered Standard Errors</a></li>
  <li><a href="#the-code-for-clustered-standard-errors" id="toc-the-code-for-clustered-standard-errors" class="nav-link" data-scroll-target="#the-code-for-clustered-standard-errors">The code for clustered standard errors</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-x-various-diagnostic-tests-and-other-important-considerations" id="toc-section-x-various-diagnostic-tests-and-other-important-considerations" class="nav-link" data-scroll-target="#section-x-various-diagnostic-tests-and-other-important-considerations">Section X: Various Diagnostic Tests and Other Important Considerations</a>
  <ul class="collapse">
  <li><a href="#checking-for-multicollinearity" id="toc-checking-for-multicollinearity" class="nav-link" data-scroll-target="#checking-for-multicollinearity">Checking for Multicollinearity</a></li>
  <li><a href="#checking-for-heteroskedasticity" id="toc-checking-for-heteroskedasticity" class="nav-link" data-scroll-target="#checking-for-heteroskedasticity">Checking for Heteroskedasticity</a>
  <ul class="collapse">
  <li><a href="#breush-pagan-godfrey-test" id="toc-breush-pagan-godfrey-test" class="nav-link" data-scroll-target="#breush-pagan-godfrey-test">Breush-Pagan-Godfrey Test</a></li>
  <li><a href="#whites-general-heteroscedasticity" id="toc-whites-general-heteroscedasticity" class="nav-link" data-scroll-target="#whites-general-heteroscedasticity">White’s General Heteroscedasticity</a></li>
  </ul></li>
  <li><a href="#checking-for-autocorrelation" id="toc-checking-for-autocorrelation" class="nav-link" data-scroll-target="#checking-for-autocorrelation">Checking for Autocorrelation</a>
  <ul class="collapse">
  <li><a href="#durbin-watson-test" id="toc-durbin-watson-test" class="nav-link" data-scroll-target="#durbin-watson-test">Durbin-Watson Test</a></li>
  <li><a href="#breusch-godfrey-test" id="toc-breusch-godfrey-test" class="nav-link" data-scroll-target="#breusch-godfrey-test">Breusch-Godfrey Test</a></li>
  </ul></li>
  <li><a href="#common-support" id="toc-common-support" class="nav-link" data-scroll-target="#common-support">Common Support</a></li>
  </ul></li>
  <li><a href="#section-x-miscellaneous-other-stuff-that-might-be-useful-to-know" id="toc-section-x-miscellaneous-other-stuff-that-might-be-useful-to-know" class="nav-link" data-scroll-target="#section-x-miscellaneous-other-stuff-that-might-be-useful-to-know">Section X: Miscellaneous Other Stuff That Might Be Useful To Know</a>
  <ul class="collapse">
  <li><a href="#bootstrapping" id="toc-bootstrapping" class="nav-link" data-scroll-target="#bootstrapping">Bootstrapping</a>
  <ul class="collapse">
  <li><a href="#why-use-a-bootstrap-sample" id="toc-why-use-a-bootstrap-sample" class="nav-link" data-scroll-target="#why-use-a-bootstrap-sample">Why use a bootstrap sample?</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Data Comps Prep</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Spring</div>
    <div class="quarto-category">Fall</div>
    <div class="quarto-category">2024</div>
    <div class="quarto-category">Methods</div>
    <div class="quarto-category">2023</div>
  </div>
  </div>

<div>
  <div class="description">
    Data Comps Prep
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://stoneneilon.github.io/">Stone Neilon</a> <a href="https://orcid.org/0009-0006-6026-4384" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.colorado.edu/polisci/people/graduate-students/stone-neilon">
            PhD student of political science @ The University of Colorado Boulder
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 15, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="preface" class="level1">
<h1>Preface</h1>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>This document is incomplete and may contain errors. I will continually make updates to this document when I have available time (almost never).</p>
</div>
</div>
<p>These are my notes I compiled through the data sequence. They come from a variety of sources and were created to help me study for the methods comprehensive exams. These notes are by no means comprehensive nor should be substituted for your own notes/material. Special thanks to Anand Sokhey, Andy Philips, Alex Siegel, Josh Strayhorn, Brian Cadena (Econ), the various (and disgustingly extensive) YouTube videos I watched explaining this material, Reddit comments, Stack-exchange comments, and God knows who else.</p>
</section>
<section id="section-1-notation" class="level1">
<h1>Section 1: Notation</h1>
<p>I apologize in advance as this section will seem like you are pledging for a fraternity or sorority. My insistence on being familiar with math notation is due to my own struggles. I think many students struggle with math not because of the “math” but because it is literally in a different language. You get mad and upset because you will look at an equation and you cannot understand the information it is conveying. I am a firm believer that if you can translate math notation into English, it becomes much easier to understand. Since math notation loves Greek letters, let us begin there:</p>
</section>
<section id="section-x-abused-words-in-the-discipline" class="level1">
<h1>Section X: Abused Words in the Discipline</h1>
<p>Starting a PhD is daunting. It is especially so when people start throwing around words that you do not know or never encountered in your undergraduate education. Much like math notation, the verbiage in the PhD world can feel foreign. To shore this up, I provide a list of some common words that I heard a lot when starting but didn’t fully grasp until much later.</p>
<table class="table">
<colgroup>
<col style="width: 27%">
<col style="width: 39%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Word</th>
<th>Definition</th>
<th>Can you use it in a sentence?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Endogenous</td>
<td>Reverse causality (This word will haunt you).</td>
<td><em>You have selection/omitted variable bias and your relationship is thus not causal/incorrect.</em></td>
</tr>
<tr class="even">
<td>Orthogonal</td>
<td>Independent/perpendicular (math symbol is <span class="math inline">\(\perp\)</span>)</td>
<td><em>Your treatment is orthogonal to observable and unobservable variables.</em></td>
</tr>
<tr class="odd">
<td>Parametric</td>
<td>Related to specific probability distributions. Parametric models are defined by some parameter that describes the shape of the probability distribution.</td>
<td><em>Your regression is a parametric model, meaning the shape of your distribution is defined by two parameters.</em></td>
</tr>
<tr class="even">
<td>Non-parametric</td>
<td>Your data-generating process is not defined parametrically – not a probability distribution. Relates to machine learning methods (neural networks and stuff like that)</td>
<td><em>You may be better off using a non-parametric model like a Random Forest.</em></td>
</tr>
<tr class="odd">
<td>Causal</td>
<td></td>
<td><em>Your treatment better be random or (as-if random) it is not a causal effect.</em></td>
</tr>
<tr class="even">
<td>Robust(ness)</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Probability</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Likelihood</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Ambivalence</td>
<td>Struggling between competing considerations. <strong>It does not mean indifferent.</strong></td>
<td></td>
</tr>
<tr class="even">
<td>Panel Data</td>
<td>Observations in our data observed at different time periods.</td>
<td></td>
</tr>
<tr class="odd">
<td>Systematic</td>
<td>Predictable</td>
<td></td>
</tr>
<tr class="even">
<td>Observables</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Unobservables</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="section-x-ordinary-least-squares-ols" class="level1">
<h1>Section X: Ordinary Least Squares (OLS)</h1>
<p>Ordinary Least Squares, like other statistical models, is simply a model we use to understand our data and what variables relate to what. So why have it in a separate section? Because OLS is the workhorse of the statistical world. Specifically, OLS is the <strong>B</strong>est <strong>L</strong>inear <strong>U</strong>nbiased <strong>E</strong>stimator (<strong>BLUE</strong>). OLS is the <strong>best</strong> statistical model out of all the other models, <strong>if you can meet the assumption requirements.</strong> I think the power of OLS does not really manifest until you learn about other models, which we will later on.</p>
<section id="the-gaus-markov-assumptions-of-ols" class="level2">
<h2 class="anchored" data-anchor-id="the-gaus-markov-assumptions-of-ols">The (Gaus-Markov) assumptions of OLS</h2>
<p>What are these assumptions and why do we need them? We are using sample data to draw inferences about the true population parameter. A subtle but important point was just made, we do not need these assumptions to draw a regression line, we need these assumptions to make <em>inferences.</em> The assumptions relate to the population. We test these assumptions using our sample. We use samples to tell us what we think the true (population) relationship is. Below I formally list these assumptions in their matrix notation form.</p>
<section id="assumption-1-linearity-in-the-parameters" class="level3">
<h3 class="anchored" data-anchor-id="assumption-1-linearity-in-the-parameters">Assumption 1: Linearity in the parameters</h3>
<p><span class="math inline">\(y_i=x_{i1}\beta_1+x_{i2}\beta_2+...+X_{iK}\beta_K+\epsilon_i\)</span></p>
<p>The model specifies a linear relationship between y and <strong>X.</strong> Do not raise the <span class="math inline">\(\beta\)</span> to a power OR transform it. We can manipulate the X’s but not the <span class="math inline">\(\beta\)</span>’s! Doing so will no longer mean we are estimating a linear relationship.</p>
</section>
<section id="assumption-2-full-rank" class="level3">
<h3 class="anchored" data-anchor-id="assumption-2-full-rank">Assumption 2: Full rank</h3>
<ol type="1">
<li><strong>X</strong> is an n x K matrix with rank K (n x k describes the dimensions of a matrix)
<ol type="1">
<li>n denotes the rows (horizontal lines) in a matrix</li>
<li>k denotes the columns (vertical lines) in a matrix</li>
</ol></li>
<li>What is “rank”?
<ol type="1">
<li><p>It is the number of linearly independent columns&nbsp;</p>
<ol type="1">
<li>If the number of independent columns is equal to the total number of columns then the matrix is full rank.</li>
</ol></li>
</ol></li>
<li>This assumption relates to the scalar assumption of <strong>no perfect multicollinearity.</strong>
<ol type="1">
<li>There is no exact linear relationship among variables&nbsp;</li>
<li>Sometimes called the “identification condition”</li>
</ol></li>
</ol>
</section>
<section id="assumption-3-exogeneity-of-the-independent-variables" class="level3">
<h3 class="anchored" data-anchor-id="assumption-3-exogeneity-of-the-independent-variables">Assumption 3: Exogeneity of the independent variables</h3>
<p>The independent variables are not influenced by the dependent variable or the model’s error term. <em>The independent variables influence the outcome.</em> This assumption in plain terms says that we should have no <em>endogeneity</em>.</p>
<p><span class="math display">\[
E[\epsilon_i|x_{j1},x_{j2},...,x_{jK}=0
\]</span></p>
<p>is matrix form this is written as:</p>
<p><span class="math display">\[
E[\epsilon_i|\textbf{X}]=0
\]</span></p>
</section>
<section id="assumption-4-spherical-disturbances" class="level3">
<h3 class="anchored" data-anchor-id="assumption-4-spherical-disturbances">Assumption 4: Spherical disturbances</h3>
<p>No autocorrelation or no heteroskedasticity (assumed homoskedasticity). Autocorrelation refers to the concept that observations do not influence each other. Heteroskedasticity refers to non-constant error variance.</p>
<p>No autocorrelation in matrix notation:</p>
<p><span class="math display">\[
E[\text{Cov}(i,j|\mathbf{X})] = 0\: \forall \:i=j
\]</span></p>
<p>Assumed homoskedasticity in matrix notation:</p>
<p><span class="math display">\[
E[\text{Var}(i|\mathbf{X})] = 2\: \forall\: i=1,2,\ldots,n
\]</span></p>
<p>Why is it that we have assumed homoskedasticity and not full homoskedasticity? It turns out that some heteroskedasticity is not fatal and can be accounted for. Generally though, we should strive for complete homoskedasticity.</p>
<p>Note that the assumption of no spherical disturbances can actually be encompassed into one single assumption in matrix algebra:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sphericalpic.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>The off-diagonal (the zeros) represent autocorrelation if these are not zero (or at least very close) we have autocorrelation. The main-diagonal (the variance) represents our homoscedasticity assumption. If these values along the main diagonal are not the same or at least very close, then we have heteroscedasticity.</p>
</section>
<section id="assumption-5-data-generation" class="level3">
<h3 class="anchored" data-anchor-id="assumption-5-data-generation">Assumption 5: Data generation</h3>
</section>
<section id="assumption-6-epsilon-is-normally-distributed" class="level3">
<h3 class="anchored" data-anchor-id="assumption-6-epsilon-is-normally-distributed">Assumption 6: <span class="math inline">\(\epsilon\)</span> is normally distributed</h3>
<p>This is useful for constructing our hypothesis tests and test statistics. Technically, we don’t need this for estimating our beta, just uncertainty surrounding it.</p>
</section>
</section>
<section id="residuals-the-error" class="level2">
<h2 class="anchored" data-anchor-id="residuals-the-error">Residuals (the error)</h2>
<p>Residuals, also known as the ‘error term’, is a critical component of OLS (and all other models). Residuals are simply the difference between out predicted value and our actual value. Mathematically this looks like:</p>
<p><span class="math display">\[
\hat{\epsilon}_i = Y_i-\hat{Y}_i
\]</span></p>
<p>It is the difference between observation and predicted (regression) line. It is the measure of how much we do not explain with our model.</p>
</section>
<section id="controlling-for-other-variables" class="level2">
<h2 class="anchored" data-anchor-id="controlling-for-other-variables">Controlling for other variables</h2>
<p>A bivariate regression is not a causal model. If we want to know the effect of some variable on <span class="math inline">\(Y\)</span>, then we need to know what other factors might be confounding this relationship. Thus, we move from a bivariate to a multi-variate regression. How do we decide what control variables to include? We will expand on this in another section related to omitted variable bias, but you should select control variables if they influence the dependent variable <em>and</em> correlate with your main explanatory variable.</p>
<section id="frisch-waugh-lovell-fwl-theorem" class="level3">
<h3 class="anchored" data-anchor-id="frisch-waugh-lovell-fwl-theorem">Frisch-Waugh-Lovell (FWL) Theorem</h3>
<p>The FWL theorem can help explain some of the intuition of what is going on when we have a multi-variate regression. Imagine we have some multivariate regression:</p>
<p><span class="math display">\[
Y=β1​X1​+β2​X2​+⋯+βk​Xk​+u
\]</span></p>
<p>We can simply run the full regression, but the FWL states we can get the same coefficient for <span class="math inline">\(\beta_1\)</span> using the FWL. Again, the FWL is not something you use as an alternative estimation method to regression (you could but that’s not the point of it), but it shows the intuition behind what is happening with our main variable and other controls.</p>
<p>The FWL shows we can derive <span class="math inline">\(\beta_1\)</span> through three smaller steps:</p>
<ol type="1">
<li>23 TK</li>
<li>dsa TK</li>
<li>sda TK</li>
</ol>
<section id="fwl-step-1" class="level4">
<h4 class="anchored" data-anchor-id="fwl-step-1">FWL: Step 1</h4>
<p>Regress Y on all other X’s <strong>except</strong> <span class="math inline">\(X_1\)</span> and collect the residuals. Recall residuals are the difference between our line of best fit (the predicted value) and our actual observation. Basically, how much does our prediction not explain.</p>
<p><span class="math display">\[
Y=β2​X2​+⋯+βk​Xk​+u
\]</span></p>
<p>We run that regression then we take the residuals of that regression:</p>
<p><span class="math display">\[
Y - \hat{Y} = \bar{Y}
\]</span></p>
<p>These are the parts of Y <strong>NOT EXPLAINED</strong> by the other controls.</p>
</section>
<section id="fwl-step-2" class="level4">
<h4 class="anchored" data-anchor-id="fwl-step-2">FWL: Step 2</h4>
<p>Regress <span class="math inline">\(X_1\)</span> on all other X’s (except itself) and get the residuals. In other words, run the same regression as above but have <span class="math inline">\(X_1\)</span> as your <em>dependent variable.</em></p>
<p><span class="math display">\[
X_1=β2​X2​+⋯+βk​Xk​+u
\]</span></p>
<p>We run that regression then we take the residuals of that regression:</p>
<p><span class="math display">\[
X_1 - \hat{X_1}=\bar{X}
\]</span></p>
<p>These are the parts of <span class="math inline">\(X_1\)</span> <strong>NOT EXPLAINED</strong> by the other controls</p>
</section>
<section id="fwl-step-3" class="level4">
<h4 class="anchored" data-anchor-id="fwl-step-3">FWL: Step 3</h4>
<p>Now have the residuals from both <strong>Step 1</strong> and <strong>Step 2.</strong> We take these residuals and run a regression with them. We have the residuals from step 1: <span class="math inline">\(\bar{Y}\)</span> and the residuals from step 2: <span class="math inline">\(\bar{X}\)</span>. Thus we run the following regression:</p>
<p><span class="math display">\[
\bar{Y}=\bar{X} +\bar{\mu}
\]</span></p>
<p>When we run this regression, our coefficient for <span class="math inline">\(X_1\)</span> will be the <em>exact same</em> as the coefficient in our normal multivariate regression.</p>
</section>
<section id="fwl-intuition" class="level4">
<h4 class="anchored" data-anchor-id="fwl-intuition">FWL: Intuition</h4>
<p>Step by step, what we just did was:</p>
<ul>
<li><p>purge Y of the influence of other regressors</p></li>
<li><p>purge <span class="math inline">\(X_1\)</span> of the influence of other regressors</p></li>
<li><p>Then we see how those “cleaned” up versions of <span class="math inline">\(X_1\)</span> and Y move together.</p></li>
</ul>
<p>This is the partial effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> aka the multiple regression coefficient. This does a good job of showing you what controlling for variables actually does.</p>
</section>
</section>
</section>
<section id="the-interaction" class="level2">
<h2 class="anchored" data-anchor-id="the-interaction">The interaction</h2>
<p>When we run an interaction, we are specifying a conditional relationship. For example, X causes Y, only if Z is active. The effect of X on Y depends on the level of Z. In other words, the effect of one independent variable on the dependent variable is conditioned by another variable.To accommodate a relationship such as this one, we multiply the two variables together rather than adding. Goal: determine whether the function <span class="math inline">\(\textbf{E}[y|x]\)</span> is constant for different sub-samples.</p>
<p>You could split the same and run regressions separately. So imagine you have some interaction, lets say (income*gender), rather than running the interaction, you could do a separate regression on one sample of only males and other regression on only a sample of women. This is fine, BUT, you aren’t able to statistically show the difference between the groups.</p>
<section id="always-include-constitutive-terms" class="level4">
<h4 class="anchored" data-anchor-id="always-include-constitutive-terms">Always include constitutive terms</h4>
<p>Our regression with an interaction specification will obviously include the interaction term <span class="math inline">\(X_1*X_2\)</span>. It is important that you do not leave out these individual terms! When you do not include the constitutive terms, you are actually imposing a restriction on the coefficient on that variable is zero. I highly doubt you mean to do this. Intuitively, when you are saying the coefficient is zero, you are arguing there is no effect. This interpretation of your interaction will be the same <em>only if that restriction is actually true.</em></p>
<p><strong>Wrong:</strong> <span class="math inline">\(Turnout = Age + Age*Race\)</span></p>
<p><strong>Correct:</strong> <span class="math inline">\(Turnout = Age + Race + Age*Race\)</span></p>
<p>There are statistical reasons for doing AND interpretation benefits for this.</p>
</section>
<section id="interpretation" class="level4">
<h4 class="anchored" data-anchor-id="interpretation">Interpretation:</h4>
<p>A few points about interpreting an interaction. Sometimes you’ll hear <span class="math inline">\(\beta_1\)</span> interpreted as the “main effect” of <span class="math inline">\(x_i\)</span>, and <span class="math inline">\(\beta_3\)</span> being the “interactive effect”. This is <em>incorrect.</em> <span class="math inline">\(\beta_1\)</span> is simply the marginal effect of <span class="math inline">\(x_i\)</span> on <span class="math inline">\(y_i\)</span>, when <span class="math inline">\(z_i=0\)</span>! Note it is the exact same logic for interpreting <span class="math inline">\(\beta_2\)</span>. If we just keep the <span class="math inline">\(z\)</span> as dichotomous, these are actually very simple to interpret.</p>
</section>
<section id="interactions-increase-multicollinearity" class="level4">
<h4 class="anchored" data-anchor-id="interactions-increase-multicollinearity">Interactions increase multicollinearity</h4>
<p>Interactions increase multicollinearity; this is OKAY. <strong>Why do interactions increase multicollinearity?</strong> This is because our interaction is inherently correlated with our constitutive terms that are included in the model. This is by construction! Interactions increase multicollinearity because the interaction term is <strong>linearly related to the constitutive terms</strong>, making it harder for OLS to separately identify the effect of each variable. <strong>What do we do?</strong> Nothing. No one gives a shit about this.</p>
</section>
</section>
<section id="confidence-intervals" class="level2">
<h2 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h2>
<p>Confidence intervals are an important part of the inferences we make. All else equal, we want our confidence intervals to be as small as possible. A confidence interval estimates a range of values that contains/captures the true population value for a variable of interest. Implicit in this concept, is a level of confidence that this actual/true population value is contained within the estimated range of values. A confidence interval is a range of values; it consists of an upper and lower bound. This range is designed to capture the population parameter.</p>
<p><strong>The confidence interval gives us the set of all null hypothesis that we would have been unable to reject.</strong> The best guess for the population mean value is the sample mean value, plus or minus two standard errors. To get the confidence interval (let’s say 95%), we simply add/subtract 1.96 times the standard error to your estimated coefficient. For the 99% confidence interval, just add/subtract 2.33 times the standard error on either side. <strong>We cannot create a confidence interval or make inferences about the population as whole if our sample is a convenience (non-random) sample.</strong></p>
<p>As the sample size increases, the standard error decreases, giving us more precision of the confidence interval.</p>
</section>
<section id="p-values" class="level2">
<h2 class="anchored" data-anchor-id="p-values">P-Values</h2>
<p>The most overrated, over-discussed, and over-valued metric ever. Before I explain why, let’s first walk through what a p-value actually is. The formal definition:</p>
<p><em>A p-value is:</em></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
p-value definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>A p-value is the probability of observing a test statistic value, equal to or more extreme than the value you observed, <em>if the null were true.</em></p>
</div>
</div>
<p>Is this relationship (my coefficient I observed) real or due to chance? Remember, we are using a sample to tell us about the population. Did we just get a weird sample? A p-value is going to help us gain leverage over this! The intuition behind a p-value: <em>How weird is it for me to have observed the result I got, if there is truly no effect.</em> One more time, if the true effect of <span class="math inline">\(\beta\)</span> is ZERO, the p-value will tell us HOW WEIRD it was for us to have observed the coefficient we got from our sample. If the p-value is low (decided by us) then we are saying that is is weird for us to have observed this effect <em>due to chance.</em> Confusingly, we decide a p-value for which to compare our calculated p-value to. Thus, we pick a threshold p-value and then figure out what the actual p-value is from our results and then compare. If the p-value we calculated is higher than our threshold, we fail to reject the null hypothesis, meaning we cannot statistically differentiate our coefficient from zero (no effect).</p>
<p>You will also here the p-value you described as the “probability of making a Type 1 error (false positive). <strong>When you reject the null hypothesis, you are saying it is highly unlikely to have observed this coefficient by chance, if the true effect was zero.</strong></p>
<section id="why-is-the-p-value-over-rated" class="level3">
<h3 class="anchored" data-anchor-id="why-is-the-p-value-over-rated">Why is the p-value over-rated?</h3>
<p>For one, R or Stata will calculate the value; however, as researchers we select a p-value threshold to compare the to. If the p-value we got is lower than that threshold, we believe we have a significant relationship. That is, it is weird for us to have observed the value we got, if the true <span class="math inline">\(\beta=0\)</span>. Thus, we reject the null hypothesis. As researchers, we choose these thresholds. There is no law that states we must select this p-value.</p>
<p>More importantly, you don’t need to report the p-value or even the t/z-stat. Why? Because your coefficient and standard error provides that information already. To get those other values, all you need is the coefficient and the SE. This is why you hardly see econ journals report the t-stat or p-value; because they can calculate themselves and figure out the confidence intervals.</p>
</section>
<section id="what-the-p-value-is-not" class="level3">
<h3 class="anchored" data-anchor-id="what-the-p-value-is-not">What the p-value is NOT</h3>
<ul>
<li><p>It does not tell you if your relationship is causal</p></li>
<li><p>It says nothing about the strength of the relationship</p></li>
<li><p>Does not take into account measurement or if the sample is truly random</p></li>
</ul>
</section>
</section>
<section id="the-pitfalls-of-ols" class="level2">
<h2 class="anchored" data-anchor-id="the-pitfalls-of-ols">The pitfalls of OLS</h2>
</section>
</section>
<section id="section-x-functional-form" class="level1">
<h1>Section X: Functional Form</h1>
<p>In any statistical model, you may see changes to the “functional form”. In simple terms, this means we are changing how our model is estimated. ‘Functional form’ and ‘specification’ get used interchangeably, in short, they both relate to how our regression (or any class of model we choose) is formed/constructed. Transformations used to achieve parametric linearity are called functional form adjustments.</p>
<p>Transformations maintain monotinicity - a fancy way to maintain the “ordering” of the data.</p>
<section id="which-functional-form" class="level3">
<h3 class="anchored" data-anchor-id="which-functional-form">Which Functional Form?</h3>
</section>
<section id="log-transformation-interpretations" class="level2">
<h2 class="anchored" data-anchor-id="log-transformation-interpretations">Log Transformation Interpretations:</h2>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 38%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Dependent Variable</th>
<th>Independent Variable</th>
<th>Interpretation of <span class="math inline">\(\beta\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>level-level</td>
<td><span class="math inline">\(y\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\Delta y = \beta \Delta x\)</span></td>
</tr>
<tr class="even">
<td>level-log</td>
<td><span class="math inline">\(y\)</span></td>
<td><span class="math inline">\(log(x)\)</span></td>
<td><span class="math inline">\(\Delta y = (\beta/100)\% \Delta x\)</span></td>
</tr>
<tr class="odd">
<td>log-level</td>
<td><span class="math inline">\(log(y)\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\% \Delta y = (100 \beta) \Delta x\)</span></td>
</tr>
<tr class="even">
<td>log-log</td>
<td><span class="math inline">\(log(y)\)</span></td>
<td><span class="math inline">\(log(x)\)</span></td>
<td><span class="math inline">\(\% \Delta y = \beta \% \Delta x\)</span></td>
</tr>
</tbody>
</table>
<p>Why log? Couple of reasons:</p>
<ol type="1">
<li>Interpretation might make more sense</li>
<li>We have skewed data and is not normal.</li>
</ol>
</section>
<section id="polynomials-of-x" class="level2">
<h2 class="anchored" data-anchor-id="polynomials-of-x">Polynomials of x</h2>
<p>This is super easy! We just add an extra square to our variable in our regression equation.</p>
<p>Why a square term? What does the square term? This goes back to your high school math. When you add a square term to a line it transforms into a parabola (think of a “U” shape).</p>
<p>Be careful with asymptotic data and polynomials.</p>
</section>
<section id="our-models-are-still-linear" class="level2">
<h2 class="anchored" data-anchor-id="our-models-are-still-linear">Our Models are Still Linear</h2>
<p>At this point, you might be a little confused since we describe these models as linear in the parameters. And you have probably been told that you cannot change these parameters, <strong><em>and that is correct.</em></strong> But you still might be confused, since we are transforming our <span class="math inline">\(X\)</span>’s to allow for a non-linear relationship. What is going on? Linear in the parameters refers to linear combination. The <span class="math inline">\(\beta\)</span>s are constants (a scalar) transforming vectors on a hyperspace. This gets math-y and you’ll probably be kicking yourself for not paying attention more in high school linear algebra.</p>
<p>The <span class="math inline">\(x\)</span> can be thought of as a vector, the beta is a transformation of that vector - it does this LINEARLY.</p>
<p>These transformations do not transform the parameters! Think of it like this, your betas just see a number next to it, the combination of that number is still a linear transformation.</p>
<p>The relationship between x and y can be non-linear. When we are talking about linearity, we are talking about the <span class="math inline">\(\beta\)</span> being linear.</p>
</section>
</section>
<section id="section-x-likelihood" class="level1">
<h1>Section X: Likelihood</h1>
<p>Likelihood can be thought of as as the join probability of the data as a function of parameter values for a particular density or mass function.The key innovation in the likelihood framework is treating the observed data as fixed and asking what combination of probability model and parameter values are the most likely to have generated these specific data. The principle of maximum likelihood is based on the idea that the observed data (even if it is not a random sample) are more likely to have come about as a result of a particular set of parameters. Thus, we flip the problem on its head. <em>Rather than consider the data as random and the parameters as fixed, the principle of maximum likelihood treats the observed data as fixed and asks: “What parameter values are most likely to have generated the data?”</em> Thus, the parameters are random variables <span class="citation" data-cites="ward2018maximum">@ward2018maximum</span>.</p>
<p>Let’s take a moment here to unpack what we mean by <em>parameters.</em> Parameters are random ‘variables’ (not to be confused with our variables that we include in our models) that define the shape of a distribution. There are different parameters for different distributions; when we are discussing parameters across all models, we will use theta <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\theta\)</span> is the symbol we will use to describe parameters, but within that <span class="math inline">\(\theta\)</span> are different types of parameters that are associated with the specific distribution and receive a different symbol to denote. For example, the parameters to describe a Gaussian (normal) distribution are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. Knowing these two values describes the shape of the distribution. I can use <span class="math inline">\(\theta\)</span> to describe these parameters, but since I’ve specified we are talking about a normal distribution, <span class="math inline">\(\theta\)</span> is a catch all symbol for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. Likelihood is the product of probability distributions, evaluated at each of our observed data points. When we select a model, we have to choose a convenient distribution for our data. Convenient distribution in this case refers to a a parametric probability distribution.</p>
<p><strong>To reiterate, in Maximum likelihood, we specify our distribution based on the type of data and then we are estimating that distribution’s respective parameters. Each distribution, whether poisson, Bernoulli, normal, etc. will have some parameter that describes the shape of distribution.</strong></p>
<p>Note that the purpose of this section is to explain the logic behind the concept of likelihood. This is important as in the next section we will explain how we attempt to maximize the likelihood. That is, what parameters generated the data we observed. This is a maximization problem, where as in OLS, it is a minimization problem (minimizing the sum of the squared residuals). We now turn to the MLE method. <em>Note, I will restate a lot about likelihood in the following section. This stuff is confusing and needs to be constantly re-explained and re-formulated for you to get a grasp on what is going on.</em></p>
</section>
<section id="section-x-maximum-likelihood-estimation-mle" class="level1">
<h1>Section X: Maximum Likelihood Estimation (MLE)</h1>
<p>Like everything in statistics, we are estimating. Estimating is just another jargony way to say that we are making <em>educated guesses.</em> Maximum Likelihood is one way we make these educated guesses. Maximum likelihood is a way we estimate the <span class="math inline">\(\beta\)</span> coefficients, hence its full name: maximum likelihood estimation (MLE). The way we estimate the <span class="math inline">\(\beta\)</span>’s is different from how we estimate them in OLS. In general, MLE is just a different way to estimate our <span class="math inline">\(\beta\)</span>’s. We cannot estimate the <span class="math inline">\(\beta\)</span>’s how we would in OLS and expect results to be accurate given the different data generating processes that can occur. Thinking of OLS and MLE as separate is somewhat of a misnomer, since OLS is actually an instance of a GLM which uses MLE. <strong>Maximum Likelihood Estimation (MLE) is a technique to find the <em>most likely</em> function that explains observed data.</strong></p>
<p>Say you have some data. Say you’re willing to assume that the data comes from some distribution -- perhaps Gaussian. There are an infinite number of different Gaussians that the data could have come from (which correspond to the combination of the infinite number of means and variances that a Gaussian distribution can have). MLE will pick the Gaussian (i.e., the mean and variance) that is “most consistent” with your data (the precise meaning of <em>consistent</em> is explained below).</p>
<p>So, say you’ve got a data set of <span class="math inline">\(𝑦={−1,3,7}\)</span> . The most consistent Gaussian from which that data could have come has a mean of 3 and a variance of 16. It could have been sampled from some other Gaussian. But one with a mean of 3 and variance of 16 is most consistent with the data in the following sense: <em>the probability of getting the particular</em> <span class="math inline">\(y\)</span> values you observed is greater with this choice of mean and variance, than it is with any other choice.</p>
<p>Moving to regression: instead of the mean being a constant, the mean is a linear function of the data, as specified by the regression equation. So, say you’ve got data like <span class="math inline">\(x={2,4,10}\)</span> along with <span class="math inline">\(y\)</span> from before. The mean of that Gaussian is now the fitted regression model <span class="math inline">\(X'\hat{\beta_i}\)</span>, where <span class="math inline">\(\hat{\beta}= [-1.9,.9]\)</span></p>
<p>Moving to GLMs: replace Gaussian with some other distribution (from the exponential family). The mean is now a linear function of the data, as specified by the regression equation, transformed by the link function. So, it’s <span class="math inline">\(g(X'\beta)\)</span> where <span class="math inline">\(g(x)=e{\frac{x}{1+e^{x}}}\)</span> for logit (with binomial data).</p>
<p><strong>What is a likelihood?</strong> The likelihood is the <strong>joint probability of observing our entire dataset</strong>, conditional on the observed covariates and a set of model parameters. Under the assumption that observations are independent given the covariates, the likelihood is the <strong>product of the individual probability distributions</strong> for each observation, evaluated at the observed data points.</p>
<p>When we select a model, we must choose a probability distribution that we believe generated the outcome variable (e.g., normal, Poisson, binomial). The model parameters describe how the covariates relate to the outcome by determining the parameter(s) of this distribution as a function of the systematic component. <strong>What is a probability distribution?</strong> A probability distribution lists possible outcomes and their probabilities.</p>
<p>A <strong>joint probability distribution</strong> answers the question: <em>how likely is it that all of our observed outcomes occur together</em>, given the covariates and the model? The likelihood measures exactly this quantity, and maximum likelihood estimation chooses the parameter values that make the observed data as plausible as possible under the assumed model.</p>
<p>Each observation contributes a probability term to the likelihood, but a single shared set of betas is chosen to maximize the joint probability of observing the entire dataset.</p>
<p><span class="math display">\[
\mathcal{L}(\theta)= \prod p(y_i;\theta)
\]</span></p>
<p>To the right of the equal sign is the <em>joint probability distribution.</em> For a joint probability distribution, the parameter value is assumed to be known or fixed. However, in the likelihood, the parameter value is unknown. The data is <em>generated</em> from the parameters of the model. BUT, we do not see the parameters. We have to infer what the parameters are from the data we observed. The parameters tell us how the covariates relate to the outcome. It is here where maximum likelihood plays a role.</p>
<p>The likelihood is a function of the parameter. The product (on the right) allows us to summarize the data into a single value that could be useful for guessing the parameter. The MLE is ultimately a guess. Is it a good guess? The MLE is a great guess, <strong>sometimes, it is the best guess we can make</strong>! MLE is a consistent estimator, as the sample size grows, the value of MLE approaches to true value. An estimator is “consistent” if the it gets closer to the true parameter value as the sample grows to infinity. Even though the true <span class="math inline">\(\theta\)</span> is ultimately unknowable, a consistent estimator will at least get us close. Under the right conditions, the maximum likelihood estimator has the best (read: smallest) variance among unbiased estimators. This is because of the Cramer-Rao Lower Bound. Meaning in large enough samples, the variance of our ML estimate can do no better than (go below this lower bound) and is thus the best we can do. Remember, that we care about variance because of its relation to the confidence interval.</p>
<p>When we make models and estimate parameters from the data, we assume that the parameter that generated that data is fixed and unknown. The whole point of estimation is to try to guess what this unknown value is. Remember: we are only observing the data, and we are trying to figure out what that parameter is that generated the data! <span class="math inline">\(Y\)</span> is random and the distribution of <span class="math inline">\(Y\)</span> is a function of the parameter. In regression models, the parameters of the outcome distribution (such as the mean, rate, or probability) are modeled as functions of observed covariates through a systematic component, while the regression coefficients themselves remain fixed and unknown.</p>
<p>How do we find the maximum? We rely on computers. Their job is to find the maximum. Their job is to get the precise value. Even though we don’t have a nice analytic form to look, it’s the method of finding the maximum itself that produces those desirable qualities.</p>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<p>Long story short, in statistics we rely on solutions to problems that have nice equations. However, in most cases, we don’t have this. So we have some algorithm that searches for the solution. We give the algorithm some “criteria” to find that solution. The loss function is the criteria we use – which is calculus.</p>
<p>How do we find the maximum? We rely on computers. Their job is to find the maximum. Their job is to get the precise value. Even though we don’t have a nice analytic form to look at, it’s the method of finding the maximum itself that produces those desirable qualities. MLE is not an estimation problem, but rather, an optimization <em>problem.</em> We are finding the parameter values that best fit data. We have to find the <span class="math inline">\(\beta\)</span>’s, that maximize our <span class="math inline">\(\theta\)</span>.</p>
<p>IF our data has a joint probability distribution, given some <span class="math inline">\(\theta\)</span>, you can plug in the observed data that you collected to get a likelihood function of how likely it was to observe that data given those parameters. and then you optimize to find the maximum likelihood, the parameter <span class="math inline">\(\theta\)</span> that give you mos</p>
<p>Optimization is look at all the possible parameters to find the maximum likelihood. We are taking derivative with respect to all parameters in <span class="math inline">\(\theta\)</span>. To find the MLE, we are still using calculus. The MLE is found at the parameter values where the derivative of the log-likelihood with respect to the parameters is zero (and the solution corresponds to a maximum). Formally, this is represented as:</p>
<p><span class="math display">\[
\frac{\partial\mathcal{L}(\beta)}{\partial\beta}=0
\]</span></p>
<p>OLS, we do the same thing, however, we do not have a closed form solution (we don’t have a formula like we do in OLS). In these other models, we are still trying to achieve the same thing, which is when derivative is equal to zero, but since we don’t have closed form solutions, we have to find it. Hence, the use of optimization algorithms to make our computers better at finding the solution. You don’t really need to know these algorithms. Just know that they are searching for the estimates where the derivative is equal to zero (or as close as possible).</p>
</section>
<section id="the-structure-of-mle" class="level2">
<h2 class="anchored" data-anchor-id="the-structure-of-mle">The Structure of MLE</h2>
<p>MLE can and should be thought of in three components:a stochastic component, a systematic component, and a link function. Luckily for us, the math of figuring out all this has already been done. Thus you will end up just picking the generalized linear model and using its appropriate functions in your statistical software.</p>
<section id="stochastic-component" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-component">Stochastic Component</h3>
<p>Stochastic component: <span class="math inline">\(Y_i\sim f(\theta_i)\)</span></p>
<p>Here, <span class="math inline">\(f\)</span> is some probability distribution or mass function. These can be Gaussian, binomial, Poisson, Weibull, etc. The stochastic statement describes our assumptions about the probability distributions that govern our data-generating process. The outcome needs to come from an exponential family.</p>
</section>
<section id="systematic-component" class="level3">
<h3 class="anchored" data-anchor-id="systematic-component">Systematic Component</h3>
<p>Systematic component: <span class="math inline">\(\theta_i = \beta_0+ \beta_1x_i\)</span></p>
<p>The systematic statement describes our model for the parameters of the assumed probability distribution. We characterize the systematic components by creating a vector of explanatory variables that are linear in the predictors…these are just our normal betas setup in regression. These are the factors that affect the conditional mean of <span class="math inline">\(Y_i, \mu\)</span>.</p>
</section>
<section id="the-link-function" class="level3">
<h3 class="anchored" data-anchor-id="the-link-function">The Link Function</h3>
<p>There are various types of link functions and each are associated with a specific model. These link functions will look scary. All you need to know is that these are functions that mathematicians solved a long long time ago <em>to bend the line.</em> No I am not joking, all these are doing is bending the line. We do this because we have a boundedness problem. We are “non-linearizing” the relationship between the systematic component and the outcome. Now, we need to be careful, as these sets of models are called <em>generalized linear models</em>. We still consider them linear. We are transforming the predictions.</p>
</section>
</section>
<section id="estimating-beta-in-mle" class="level2">
<h2 class="anchored" data-anchor-id="estimating-beta-in-mle">Estimating <span class="math inline">\(\beta\)</span> in MLE</h2>
<p>In simple terms, we are plugging and chugging. Each probability distribution has some link function that we have derived (well someone else smarter than us did). The parameter value within that link function is a function of our systematic component <span class="math inline">\(X\beta\)</span>’s. We don’t know the <span class="math inline">\(\beta\)</span>. So we are going to plug in different values. Step by step this is what is going on:</p>
<ol type="1">
<li>Pick some candidate value of <span class="math inline">\(\beta\)</span></li>
<li>compute <span class="math inline">\(x_i\beta\)</span> for all observations</li>
<li>apply the link function to get distribution parameters</li>
<li>compute the likelihood for all observed <span class="math inline">\(y_i\)</span>’s using these parameters</li>
<li>Adjust <span class="math inline">\(\beta\)</span> to increase likelihood</li>
<li>Repeat until the likelihood is maximized.</li>
</ol>
<p>The betas determine Xβ, which is transformed by the link function to give the distribution parameters, and MLE chooses the set of betas that makes the observed data most likely under that distribution.</p>
<p><strong>MLE is all about finding the beta vector</strong> that maximizes the likelihood, given the observed data. Multiply X by beta, transform via the link to get the distribution parameter, compute the likelihood, and iterate beta values until the likelihood is maximized; the resulting betas are your regression coefficients.</p>
<p>THE PARAMETER WE ESTIMATE IS A FUNCTION OF OUR SYSTEMATIC COMPONENT. FOR EVERY DISTRIBUTION.</p>
<section id="mle-uses-logarithms" class="level3">
<h3 class="anchored" data-anchor-id="mle-uses-logarithms">MLE uses Logarithms</h3>
<p>When we are estimating the likelihood, we are actually estimating the log-likelihood. Why do we do this? Without the log we run into some mathematical issues. For one, we are going to be multiplying a lot of numbers. When we multiply large or small numbers, our machines run into some issues. We are estimating a joint probability.</p>
<p>what happens when we multiply a bunch of probabilities together? They get incredibly small.</p>
<p>Logs turn multiplication into addition.</p>
</section>
</section>
</section>
<section id="section-x-selecting-what-model-to-use" class="level1">
<h1>Section X: Selecting What Model To Use</h1>
<p>How do we select which statistical model to use for inference? This choice is dependent on our data structure, specifically, our dependent variable. Don’t over think this! We use GLMs when our residuals are <em>not normally distributed</em> (except for linear regression which the GLM is OLS and is normally distributed)<em>.</em></p>
<table class="table">
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 26%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>Random</strong></td>
<td><strong>Link</strong></td>
<td><strong>Systematic Component</strong></td>
</tr>
<tr class="even">
<td>Linear Regression</td>
<td>Normal</td>
<td>Identity</td>
<td>Continuous</td>
</tr>
<tr class="odd">
<td>ANOVA</td>
<td>Normal</td>
<td>Identity</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>ANCOVA</td>
<td>Normal</td>
<td>Identity</td>
<td>Mixed</td>
</tr>
<tr class="odd">
<td>Logistic Regression</td>
<td>Binomial</td>
<td>Logit</td>
<td>Mixed</td>
</tr>
<tr class="even">
<td>Loglinear</td>
<td>Poisson</td>
<td>Log</td>
<td>Categorical</td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td>Poisson</td>
<td>Log</td>
<td>Mixed</td>
</tr>
<tr class="even">
<td>Multinomial</td>
<td>Multinomial</td>
<td>Generalized Logit</td>
<td>Mixed</td>
</tr>
</tbody>
</table>
<p>The table above is very important, and in my opinion, extremely useful. In a nutshell, all we are doing is figuring out what data we have, and picking which model is associated with that data structure to use for inference and statistical analysis. <strong>THAT IS IT!</strong> On the surface, this is incredibly simple. There is a <em>lot</em> of math under these, BUT don’t worry too much about it. You will be shown what R (or Stata) is doing under the hood in lecture. Don’t let this intimidate you, your professor is just showing how these models operate and really, how they came into existence. Knowing what is going on is still important, but your job as the researcher will really just boil down to knowing your data structure and picking the model associated with that structure.</p>
<section id="a-not-so-quick-aside" class="level2">
<h2 class="anchored" data-anchor-id="a-not-so-quick-aside">A Not-So Quick Aside</h2>
<p>The random components are probability density functions (PDFs).</p>
</section>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear Regression</h2>
<p>Earlier we learned about Ordinary Least Squares (OLS). However, we can do the same estimation within the world of generalized linear models. These are technically not the same estimations; however, they are equivalent. Linear regression is a type of generalized linear model. But are there any advantages of the maximum likelihood estimation of linear regression to the OLS estimation? <strong>No.</strong> They are equivalent. However, the OLS approach assumes a linear model and unbounded, continuous outcome <span class="citation" data-cites="agresti2015foundations">@agresti2015foundations</span>.</p>
</section>
<section id="dichotomousbinary-dependent-variable" class="level2">
<h2 class="anchored" data-anchor-id="dichotomousbinary-dependent-variable">Dichotomous/Binary Dependent Variable</h2>
<p>What statistical model do we use when our dependent variable is binary? This section is to provide insight into the question. First, let’s think of a few possible political science questions that would contain an outcome variable that is dichotomous: will the senator vote for the bill? Did the individual turnout to vote? The possible choices are endless, but these question examples use a dichotomous variable as our dependent variable. <em>We are interested in what set of X variables will predict the outcome variable to be 0 or 1.</em> Notation for a dichotomous dependent variable typically takes form below:</p>
<p><span class="math display">\[
y_i =\begin{cases}0, &amp; \text{if trait is not present} \\1, &amp; \text{if trait is present}\end{cases}
\]</span></p>
<p>When we have a dichotomous dependent variable, we are using a <em>Bernoulli distribution.</em> But what is a Bernoulli distribution actually? It is a probability distribution for a random variable that has two outcomes. The math lingo we will see typically will say something like:</p>
<p><span class="math display">\[
Y\sim Benoullli(p)
\]</span></p>
<p>In plain English, the equation is saying that our random variable (Y) is a Bernoulli distribution with the probability of <span class="math inline">\(p\)</span>. What is <span class="math inline">\(p\)</span> though? <span class="math inline">\(p\)</span> is the probability of our outcome variable <span class="math inline">\(y\)</span> taking on 1! We are going to use our models to figure out what this probability is. In a Bernoulli distribution, we just need to know one parameter, <span class="math inline">\(p\)</span>, to know <em>everything</em> about the distribution. For comparison, in the normal distribution, we need know two parameters to know everything about the shape of the distribution, these two parameters are <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Now at this point, you may be wondering, how do I figure out what that <span class="math inline">\(p\)</span> is? In our modeling strategy, we have a few different avenues to estimate this parameter. In summary we can express <span class="math inline">\(p\)</span> (also sometimes expressed <span class="math inline">\(\pi\)</span>, is a function of our covariates, <span class="math inline">\(\pi = \textbf{X}_i\beta = \beta_0 + \beta_1X_1 + …+\beta_kX_{ik}\)</span></p>
<p>There are three models we can use to estimate a binary outcome: <strong>Linear Probability Model (LPM), logistic-regression (Logit),</strong> and <strong>Probit</strong>. These three are going to estimate the one parameter <span class="math inline">\(p\)</span> we care about to help us yield predictions about how our X’s relate to our Y. However, each of these use different strategies to do so and thus yield different results. I discuss how these models work and their potential pros and cons.</p>
<section id="linear-probability-model-lpm" class="level3">
<h3 class="anchored" data-anchor-id="linear-probability-model-lpm">Linear Probability Model (LPM)</h3>
<p>What is a Linear Probability Model (LPM). In simple terms, an LPM is going to estimate <span class="math inline">\(p\)</span> using Ordinary Least Squares (OLS). In our R code, we would simply just run the regression as we would normally! Don’t over think it. Literally just put the dichotomous variable as our dependent variable and the appropriate covariates into our regression code and run the model.</p>
<p>Let’s work through an example to illustrate what an LPM looks like.</p>
<section id="lpm-example" class="level4">
<h4 class="anchored" data-anchor-id="lpm-example">LPM Example</h4>
<p>For this example, we are going to use the Titanic data set. This data set is built into R. Our dependent variable will be the dichotomous variable: whether the passenger survived or not. Our independent variable will be gender, another dichotomous variable. Thus, we are estimating the effect of gender on survival on the Titanic.</p>
<p><code>survived</code> is a binary variable indicating whether the passenger survived (1) or not (0)</p>
<p><code>sex</code> is the passenger’s gender (male or female)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># quick data cleaning</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>titanic_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(Titanic)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>titanic_full <span class="ot">&lt;-</span> <span class="fu">uncount</span>(titanic_df, Freq)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>titanic_full<span class="sc">$</span>Survived <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(titanic_full<span class="sc">$</span>Survived <span class="sc">==</span> <span class="st">"Yes"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>titanic_full<span class="sc">$</span>Sex <span class="ot">&lt;-</span> <span class="fu">factor</span>(titanic_full<span class="sc">$</span>Sex)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># lpm</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>lpm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Survived <span class="sc">~</span> Sex, <span class="at">data =</span> titanic_full)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lpm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Survived ~ Sex, data = titanic_full)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.7319 -0.2120 -0.2120  0.2681  0.7880 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.21202    0.01001   21.18   &lt;2e-16 ***
SexFemale    0.51990    0.02166   24.00   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4165 on 2199 degrees of freedom
Multiple R-squared:  0.2076,    Adjusted R-squared:  0.2072 
F-statistic:   576 on 1 and 2199 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We now have our LPM output with our relevant coefficients! What do these numbers actually mean though? The intercept is where our line of prediction crosses the Y-axis. Our intercept is actually being the predicted probability of surviving when being a male. The predicted probability of surviving as a male is .21. The effect of being female and surviving is .52. Remember that this coefficient on female is the difference in probability compared to the reference group (men)! So, being female is predicted to have a .52 higher probability of surviving compared to males. In plain English, this means males had a baseline survival rate of 21%; females had a baseline survival rate of 73%. This effect is statistically significant.</p>
</section>
<section id="the-problems-with-lpm" class="level4">
<h4 class="anchored" data-anchor-id="the-problems-with-lpm">The Problems with LPM</h4>
<p>Everything ran and we got a coefficient, so what is the problem with this? Note that some of the problems with LPM are debated, and many people (economist) will tell you to just run it. However, it is still important to note its problems.</p>
<section id="problem-1-impossible-predictions" class="level5">
<h5 class="anchored" data-anchor-id="problem-1-impossible-predictions">Problem 1: Impossible Predictions</h5>
<p>When we run an LPM, we can get impossible predictions. This means we can predictions that are below zero (negative) or above 1. Given probability can only be between 0 and 1, these possible predictions don’t make any sense. Let’s look at another example with the cars data set:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mtcars<span class="sc">$</span>foreign_car <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(mtcars<span class="sc">$</span>vs <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>lpm <span class="ot">&lt;-</span> <span class="fu">lm</span>(foreign_car <span class="sc">~</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>wt_grid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">wt =</span> <span class="fu">seq</span>(<span class="fu">min</span>(mtcars<span class="sc">$</span>wt), <span class="fu">max</span>(mtcars<span class="sc">$</span>wt), <span class="at">length.out =</span> <span class="dv">100</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>wt_grid<span class="sc">$</span>fitted_lpm <span class="ot">&lt;-</span> <span class="fu">predict</span>(lpm, <span class="at">newdata =</span> wt_grid)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot without jitter</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mtcars, <span class="fu">aes</span>(<span class="at">x =</span> wt, <span class="at">y =</span> foreign_car)) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span>  <span class="co"># actual 0/1 points, no jitter</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> wt_grid, <span class="fu">aes</span>(<span class="at">x =</span> wt, <span class="at">y =</span> fitted_lpm), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span>  <span class="co"># full LPM line</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>), <span class="at">labels =</span> scales<span class="sc">::</span><span class="fu">percent_format</span>()) <span class="sc">+</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Linear Probability Model: Fitted Values Across Full Range"</span>,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Weight (1000 lbs)"</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Probability of Being Foreign"</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 9 rows containing missing values or values outside the scale range
(`geom_line()`).</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Notice how when the weight is around ~4,800 lbs, the line is below zero. According to this, when the weight of a car is ~4,800, the predicted probability of being a foreign car is less than zero, which is nonsensical.</p>
</section>
<section id="problem-2-errors-are-not-normally-distributed" class="level5">
<h5 class="anchored" data-anchor-id="problem-2-errors-are-not-normally-distributed">Problem 2: Errors are not normally distributed</h5>
<p>Recall we are using OLS to estimate the probability of either zero or one. OLS makes some important assumptions, one of those: errors are normally distributed; is violated when we use the LPM. But why? Think for a moment what a residual is.</p>
<p><span class="math display">\[
\hat{\epsilon}_i = Y_i-\hat{Y}_i
\]</span></p>
<p>In our LPM, the residuals can only take two possible values, meaning it is impossible for the residuals to be normally distributed. If we were to plot the residuals, they would give us parallel lines. The graph below certainly does not look normally distributed! Remember why the normally distributed errors are important, they allows us make inference as our distribution of <span class="math inline">\(\hat{\beta}\)</span> depends on the distribution of <span class="math inline">\(\epsilon\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>residuals_lpm <span class="ot">&lt;-</span> <span class="fu">residuals</span>(lpm)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mtcars, <span class="fu">aes</span>(<span class="at">x =</span> wt, <span class="at">y =</span> residuals_lpm)) <span class="sc">+</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span>                      <span class="co"># points exactly on lines</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Residuals from Linear Probability Model"</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Weight (1000 lbs)"</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Residuals"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="problem-3-errors-epsilon-are-heteroskedastic-tk" class="level5">
<h5 class="anchored" data-anchor-id="problem-3-errors-epsilon-are-heteroskedastic-tk">Problem 3: Errors <span class="math inline">\(\epsilon\)</span> are heteroskedastic TK</h5>
<p>All LPMS have heteroskasticity. This is a similar problem to problem 2. Recall that our Gauss-Markov assumptions require homoskedasticity (constant error variance).</p>
</section>
<section id="problem-4-a-one-unit-increase-in-x-affect-pry1-may-not-be-true-across-the-range-of-xs." class="level5">
<h5 class="anchored" data-anchor-id="problem-4-a-one-unit-increase-in-x-affect-pry1-may-not-be-true-across-the-range-of-xs.">Problem 4: A one unit increase in X affect Pr(y=1) may not be true across the range of Xs.</h5>
<p>This problem will be elucidated in the following sections in logit and probit. Essentially, different values of X may have different levels of probability for Y. This relates to the linearity form.</p>
</section>
</section>
<section id="benefits-of-the-lpm" class="level4">
<h4 class="anchored" data-anchor-id="benefits-of-the-lpm">Benefits of the LPM</h4>
<p>Despite the inherent drawbacks, LPM is not something that should be avoided. Many researchers like it for a variety of reasons but do recognize these issues when using it.</p>
<section id="benefit-1-linear-interpretation-of-betas" class="level5">
<h5 class="anchored" data-anchor-id="benefit-1-linear-interpretation-of-betas">Benefit 1: Linear Interpretation of <span class="math inline">\(\beta\)</span>’s</h5>
<p>Similar to OLS, we like linearity because it is easy to interpret! This becomes more apparent when you move towards interpreting a logit.</p>
</section>
<section id="benefit-2-simple-and-cost-efficient" class="level5">
<h5 class="anchored" data-anchor-id="benefit-2-simple-and-cost-efficient">Benefit 2: Simple and Cost Efficient</h5>
<p>It is super efficient to run the LPM. Your computer is doing less math and thus less computational power to execute. This isn’t really a big deal and won’t matter much. However, this benefit has proved useful once when I was using big data 100 million + observations; logit takes longer to run and I was also limited by computational credits that cost money (more money meant I could use a better computer, thus faster execution times; however, I couldn’t afford it) however, I could use the LPM since it was most cost efficient to run and could be executed with significantly less computational power.</p>
</section>
<section id="benefit-3-works-well-if-xs-are-distributed-bernoulli" class="level5">
<h5 class="anchored" data-anchor-id="benefit-3-works-well-if-xs-are-distributed-bernoulli">Benefit 3: Works well if <span class="math inline">\(X\)</span>’s are distributed Bernoulli</h5>
<p>If your independent variables are dichotomous as well (Bernoulli), then your LPM fairs much better. Why?</p>
</section>
</section>
</section>
<section id="logistic-regression-logit" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-logit">Logistic Regression (Logit)</h3>
<p>The logit is another way to estimate the parameter <span class="math inline">\(p\)</span> such that we can estimate the probability of our <span class="math inline">\(Y\)</span> values conditional on our set of variables. The logit is going to help us avoid the issues inherent in the LPM in a few ways.</p>
<section id="what-is-the-logit" class="level4">
<h4 class="anchored" data-anchor-id="what-is-the-logit">What is the Logit?</h4>
<p>The logit, like the LPM, models the probability of an event. We use it when our dependent variable is zero or one. The logit is going to bound our predicted values between 0 and 1. Note that the LPM does not do this and allows for predicted results from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>. Again, what we want to do is to estimate the probability <span class="math inline">\(p\)</span> aka <span class="math inline">\(\pi\)</span> as it will tell us everything about the distribution. Thus:</p>
<p><span class="math display">\[
\pi_i=g(X_i\beta)
\]</span></p>
<p><span class="math inline">\(\pi_i\)</span> = the probability that <span class="math inline">\(Y_i=1\)</span> for observation <span class="math inline">\(i\)</span></p>
<p><span class="math inline">\(X_i\beta\)</span> = the linear combination of predictors (our covariates!)</p>
<p><span class="math inline">\(g(\cdot)\)</span> = the <strong>link function</strong> that transforms the linear predictor into probability. More on this below.</p>
</section>
<section id="expressing-pi_i-as-an-odds-ratio" class="level4">
<h4 class="anchored" data-anchor-id="expressing-pi_i-as-an-odds-ratio">Expressing <span class="math inline">\(\pi_i\)</span> as an Odds Ratio</h4>
<p>What are ‘odds’ actually? Odd take the probability of an event occurring and divide it by one minus the probability. It is the ratio of success to failure. Thus:</p>
<p><span class="math display">\[
Odds = \frac{Probability \ Event \ Occurs\ (p)}{ Probability \ Event \ Does \ Not\ Occur \ (1-p)}
\]</span></p>
<p>But why odds ratios? Odds ratios are the proportion of odds under on assumption to odds under a different assumption. Odds ratios specifically have the seemingly nice feature of being invariant to scale, since a 1 unit change in x produces an odds ratio of <span class="math inline">\(Exp[\beta_1]\)</span> regardless of the baseline.</p>
</section>
<section id="logging-the-odds" class="level4">
<h4 class="anchored" data-anchor-id="logging-the-odds">Logging the Odds</h4>
<p>We then log the odds ratio we calculated. Why do we take the log? For each observation, we have an associated odds ratio. We are multiplying the odds across predictors together. Multiplying odds ratios can become either very large or very small which makes it mathematically messy to use. <strong>We take the log of the odds ratio because it converts the multiplicative effects to additive effects, making our result linear and easier to interpret.</strong></p>
<p><span class="math display">\[
logit(\pi_i) = ln \left( \frac{\pi_i}{1-(\pi_i)} \right)
\]</span></p>
<p>All this does equation does is take the log of the odds ratio. This is for mathematical reasons. We then model it as a linear function of the predictors:</p>
<p><span class="math display">\[
logit(\pi_i) = X_i\beta
\]</span></p>
<p><span class="math inline">\(X_i\beta = \beta_0 +\beta_1X_{i1}+...\beta_kX_{ik}\)</span></p>
<p>Each <span class="math inline">\(\beta\)</span> is the effect of the corresponding predictor on the log-odds of <span class="math inline">\(Y=1\)</span>. We are going to estimate those <span class="math inline">\(\beta\)</span>’s using maximum likelihood. <strong>THIS IS THE LINK FUNCTION!</strong></p>
</section>
<section id="example-and-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="example-and-interpretation">Example and Interpretation</h4>
<p>The logit can be confusing to interpret. For one, the values we get out of our model are expressed as the log odds…how the hell do we interpret that? Second, the effect size can be different at different levels of X. Let’s go through an example. We are trying to predict the probability of an individual getting diabetes based on some covariates:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load example data </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>data<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data)<span class="ot">&lt;-</span><span class="fu">tolower</span>(<span class="fu">colnames</span>(data))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>outcome<span class="ot">&lt;-</span><span class="fu">as.factor</span>(data<span class="sc">$</span>outcome) </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Multivariate model</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>logistic <span class="ot">&lt;-</span> <span class="fu">glm</span>(outcome <span class="sc">~</span> bmi <span class="sc">+</span> glucose <span class="sc">+</span> bloodpressure,<span class="at">data=</span>data,<span class="at">family=</span><span class="st">"binomial"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logistic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = outcome ~ bmi + glucose + bloodpressure, family = "binomial", 
    data = data)

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -7.182306   0.635462 -11.303  &lt; 2e-16 ***
bmi            0.079793   0.013565   5.882 4.05e-09 ***
glucose        0.035746   0.003328  10.740  &lt; 2e-16 ***
bloodpressure -0.007420   0.004862  -1.526    0.127    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 993.48  on 767  degrees of freedom
Residual deviance: 769.07  on 764  degrees of freedom
AIC: 777.07

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>We can see we have a similar table. However, these coefficients represent the <strong>log-odds</strong> effects of having diabetes. These are <strong>not</strong> probabilities. There are quite a few ways to interpret these results and present them. These include:</p>
<ul>
<li><p>Odds Ratios</p></li>
<li><p>First difference</p></li>
<li><p>Partial derivatives</p></li>
<li><p>Graphical</p></li>
</ul>
<p>Why do we need all these interpretation methods? We can have different levels of effects at different values of X! To illustrate the point, let’s look at this graphically in a simulated logit curve:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">to =</span> <span class="dv">10</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>pi.<span class="fl">1.1</span> <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">1</span><span class="sc">*</span>x)))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, pi.<span class="fl">1.1</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"1/(1+exp(-(1+1*x)))"</span>, <span class="at">col =</span> <span class="st">"coral"</span>, <span class="at">lwd =</span> <span class="dv">3</span> )</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dev.off</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>null device 
          1 </code></pre>
</div>
</div>
<p>Think about this for a moment and follow the “S”. This is the probability which is no longer linear. The effect of X varies depending on it’s value!</p>
<p>However, before we jump into discussion of the different ways to interpret the coefficient, let’s look at won’t be different: <strong>AIC/BIC</strong>.</p>
<section id="aic---akaike-information-criterion" class="level5">
<h5 class="anchored" data-anchor-id="aic---akaike-information-criterion">AIC - Akaike Information Criterion</h5>
<p>Returning back to our model of diabetes, we see it reports an AIC of 777.07. But what does this mean? AIC is a measure of goodness-of-fit. Think of this as the logit version of the <span class="math inline">\(R^2\)</span>. It is a measure of how well the model fits the data while penalizing complexity. How do we derive the AIC? The formula is:</p>
<p><span class="math display">\[
AIC = 2k-2ln(\hat{L})
\]</span></p>
<p>Where <span class="math inline">\(k\)</span> is the number of estimated parameters (intercept + coefficient)</p>
<p>Where <span class="math inline">\(\hat{L}\)</span> is the maximized value of the likelihood.</p>
<p>Lower is AIC is better. But <em>lower to what?</em> The purpose of AIC is not to measure goodness-of-fit in isolation, but rather a point of comparison. If we compare the AIC of one model to a model with different variables, we will get different AICs. Then the AIC that is lowest tells us which model is a better goodness-of-fit. In summary, <strong>the AIC provides a method for assessing the quality of your model through comparison of related models.</strong></p>
</section>
<section id="coefficients---odds-ratio-interpretation" class="level5">
<h5 class="anchored" data-anchor-id="coefficients---odds-ratio-interpretation">Coefficients - Odds Ratio Interpretation:</h5>
<p>The connection between log-odds and odds ratio: you can have a negative log-odds but you cannot have a negative odds. If you have a negative coefficient from your logit that translates to the coefficient having an odds ratio of less than 1.</p>
<p>Odds ratios can be a bit confusing. They <em>are not</em> probabilities. A 10% increase is not a 10% increase in probability. Odds ratios are constant across values of X.</p>
<p><em>Note that we can have R exponentiate the values for us:</em></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(logistic<span class="sc">$</span>coefficients)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept)           bmi       glucose bloodpressure 
 0.0007599132  1.0830623854  1.0363928719  0.9926073171 </code></pre>
</div>
</div>
<p><em>We can also get confidence intervals:</em></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(logistic))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Waiting for profiling to be done...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                     2.5 %     97.5 %
(Intercept)   0.0002093526 0.00253447
bmi           1.0552703819 1.11295509
glucose       1.0298330254 1.04337509
bloodpressure 0.9831254924 1.00209766</code></pre>
</div>
</div>
<p><code>Intercept</code> - like in any other model, the intercept represents the log-odds of having diabetes when all variables are equal to zero. The log-odds of our constant is -7.182306. But what does that actually tell us? Let’s convert the log-odds to odds. We can do this by exponentiating them. The odds of having diabetes when all predictors are at zero is .00076. <strong>The odds of having diabetes with all other predicted values at zero is 0.00076 to 1.</strong></p>
<p><code>bmi</code> - the log-odds of <code>bmi</code> on <code>outcome</code> is .079793. When we convert this value to an odds ratio we get: 1.083. For every 1 unit increase in BMI, the odds of having diabetes increases by a factor of 1.083, meaning the odds of diabetes increases by 8.3%.</p>
<p><code>glucose</code> - the log-odds of <code>glucose</code> on <code>outcome</code> is .035746. For every 1 unit increase in glucose level, the odds of having diabetes increases by a factor of 1.036, meaning the odds of diabetes increase by approximately 3.6% for each additional unit of glucose.</p>
<p><code>bloodpressure</code> - the log-odds of <code>bloodpressure</code> on <code>outcome</code> is -.007420. For every 1 unit increase in blood pressure, the odds of having diabetes increases by a factor of 0.9926, meaning the odds of diabetes decrease by approximately .74% for each one unit increase in blood pressure. However, this is not a statistically significant effect.</p>
</section>
<section id="coefficients---first-difference-interpretation" class="level5">
<h5 class="anchored" data-anchor-id="coefficients---first-difference-interpretation">Coefficients - First Difference Interpretation:</h5>
<p>The first difference is, if you make a fixed, concrete change in x, such as a 1 unit, how much does that affect <span class="math inline">\(Pr(y)\)</span>. Think about this visually for a moment. In OLS, this is not an issue because we have a straight line. Now imagine if that line is curved, the change in effect could be different (not linear). Why do we like probability? Because probability is easy to understand (0-100%) and first differences reflect the nonlinear effect of X on probability in a logistic model. Remember: <strong>the effect of X depends on where we are on the curve (sigmoid).</strong></p>
<p>For example:</p>
<ul>
<li><p>At low probability (P=.05), a 1 unit increase in X might only increase the probability to .06. A one percentage point change.</p></li>
<li><p>At medium probability (P=.5), a 1 unit increase in X might increase the probability to .52. A two percentage point change.</p></li>
<li><p>At high probability (P=.9), a 1 unit increase in X might increase the probability to .91. A one percentage point change.</p></li>
</ul>
<p>In summary, a change in X can have different levels of effect, our odds interpretation will not account for this!</p>
</section>
<section id="coefficients---partial-derivatives" class="level5">
<h5 class="anchored" data-anchor-id="coefficients---partial-derivatives">Coefficients - Partial derivatives</h5>
<p>Partial derivatives show the <strong><em>instantaneous</em></strong> effect of a change in X, give a set of values in <span class="math inline">\(\pi\)</span>. For our diabetes example let’s walk through the partial derivative interpretation:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># partials show the instantaneous effect of a change in X, given a set value of pi (since we said the inflection point makes these effects non-linear)</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>pi <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.10</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>partials <span class="ot">&lt;-</span> logistic<span class="sc">$</span>coefficients[<span class="st">"glucose"</span>]<span class="sc">*</span>pi<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>pi)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>partials</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.000000000 0.003217166 0.005719407 0.007506721 0.008579110 0.008936573
 [7] 0.008579110 0.007506721 0.005719407 0.003217166 0.000000000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pi,partials, <span class="at">main =</span> <span class="st">"Instantaneous effect of Glucose | pi"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>logistic<span class="sc">$</span>coefficients[(<span class="st">"glucose"</span>)]<span class="sc">/</span><span class="dv">4</span> <span class="co"># maximum instantaneous effect</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    glucose 
0.008936573 </code></pre>
</div>
</div>
<p>How do we interpret these results?</p>
</section>
<section id="coefficients---graphical" class="level5">
<h5 class="anchored" data-anchor-id="coefficients---graphical">Coefficients - Graphical</h5>
<p>This method of interpretation is probably the one you would actually present. People like graphs!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline coefficients (all except the variable to plot)</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>coef.baseline <span class="ot">&lt;-</span> logistic<span class="sc">$</span>coefficients[<span class="fu">c</span>(<span class="st">"(Intercept)"</span>, <span class="st">"bmi"</span>, <span class="st">"bloodpressure"</span>)]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline values for other variables</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>x.means <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">25</span>, <span class="dv">80</span>)  <span class="co"># intercept=1, BMI=25, BP=80</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute linear predictor for baseline (scalar)</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>xb <span class="ot">&lt;-</span> <span class="fu">sum</span>(coef.baseline <span class="sc">*</span> x.means)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Values of glucose to plot</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>glucose_levels <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">70</span>, <span class="dv">180</span>, <span class="at">by=</span><span class="dv">10</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute predicted probabilities</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>pi.hat <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>(xb <span class="sc">+</span> logistic<span class="sc">$</span>coefficients[<span class="st">"glucose"</span>] <span class="sc">*</span> glucose_levels)))</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(glucose_levels, pi.hat, <span class="at">type=</span><span class="st">"b"</span>,</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Predicted Probability of Diabetes Across Glucose Levels"</span>,</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"Glucose Level"</span>, <span class="at">ylab=</span><span class="st">"Predicted Probability"</span>,</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">pch=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We are holding our other variables constant. <code>bmi</code> = 25 and <code>bloodpressure</code> = 80. We are the seeing how the probability of getting diabetes varies as we change <code>glucose</code> level.</p>
<p>Let’s do another example but with a political science table. This example will use the INES data set and was done in class. You can download the data here: [<a href="https://drive.google.com/file/d/1Pv9PrI-cB14_XxNd1Z94udwpzWbXviYr/view?usp=sharing">data</a>].</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>ies <span class="ot">&lt;-</span> <span class="fu">read_dta</span>(<span class="st">"survey_INES (2).dta"</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>logit.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(farm_laborer <span class="sc">~</span> dalit <span class="sc">+</span> brahmin <span class="sc">+</span> male <span class="sc">+</span> muslim <span class="sc">+</span> education, <span class="at">data =</span> ies, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = farm_laborer ~ dalit + brahmin + male + muslim + 
    education, family = binomial(link = logit), data = ies)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.37039    0.09373 -14.620  &lt; 2e-16 ***
dalit        0.58067    0.08801   6.598 4.18e-11 ***
brahmin     -1.18473    0.28786  -4.116 3.86e-05 ***
male         0.36065    0.07796   4.626 3.73e-06 ***
muslim       0.23464    0.10800   2.173   0.0298 *  
education   -0.52300    0.05468  -9.565  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 5492.7  on 6750  degrees of freedom
Residual deviance: 5235.9  on 6745  degrees of freedom
AIC: 5247.9

Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
<p>This is the output from our logistic regression. Like before, these are given to us as the log-odds, which can be somewhat difficult to interpret. We are going to use a graphical method to show how the effect of variable of interest, <code>education</code>, changes the probability.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot \hat{\pi} across a variable (this case education), holding all else constant</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>coef.means <span class="ot">&lt;-</span> logit.glm<span class="sc">$</span>coefficients[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>] <span class="co"># grab all but education</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>coef.means</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)       dalit     brahmin        male      muslim 
 -1.3703942   0.5806719  -1.1847316   0.3606510   0.2346365 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's look at a Dalit who's male and not Muslim</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>x.means <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>) </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>x.means</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 1 0 1 0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>xb <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(x.means<span class="sc">*</span>coef.means) </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>xb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  [,1]
(Intercept) -1.3703942
dalit        0.5806719
brahmin      0.0000000
male         0.3606510
muslim       0.0000000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colSums</span>(xb) <span class="co"># sum of XB's</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.4290714</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>levelsofedu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>pi.hat <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="fu">colSums</span>(xb)<span class="sc">-</span>logit.glm<span class="sc">$</span>coefficients[<span class="dv">6</span>]<span class="sc">*</span>levelsofedu))</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>pi.hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.27846883 0.18617359 0.11940617 0.07439466 0.04547454</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(levelsofedu,pi.hat, <span class="at">main =</span> <span class="st">"Pr(Farm Laborer) Across Education"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.0</span>,<span class="fl">0.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The probability of being a farm laborer decreases as level of education increases. Of course, we could intuitively tell this from the table; however, what this graph provides is the probability for each level of education when the baseline is Dalit, male, and <strong>not</strong> Muslim.</p>
<p>As you may realize, these probabilities are in relation to our baseline, Thus, when we change our baseline, the effect of education will be different. To show this, let’s now change our baseline to a Muslim woman.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># just to show you how what you set to clearly changes this story:</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>pi.hat2 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>(<span class="fl">1.0198183</span>)<span class="sc">-</span>logit.glm<span class="sc">$</span>coefficients[<span class="dv">6</span>]<span class="sc">*</span>levelsofedu)) <span class="co"># Now change to muslim woman (0.2113876 [constant] + 1.0198183 [muslim])</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(levelsofedu,pi.hat, <span class="at">main =</span> <span class="st">"Pr(Farm Laborer) Across Education"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(pi.hat2, <span class="at">type =</span> <span class="st">"p"</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">pty =</span> <span class="st">"3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>As we can see in the graph comparing the probabilities relative to their baseline, we see the different levels of effect of education on being a farm laborer. This interpretation is super easy to understand and communicates a lot of information quickly. However, it is <strong>dependent on what our baseline is.</strong> We need to justify our baseline such that we aren’t just</p>
</section>
</section>
</section>
<section id="probit" class="level3">
<h3 class="anchored" data-anchor-id="probit">Probit</h3>
<p>The probit is an alternative cumulative density function (the cumulative standard normal density) that can be used as an alternative to logit models. It is short for “probability unit”. These do not differ very much from logit, but you tend to see less of these used in political science. Probit does not use the binomial distribution but rather, the normal distribution. The estimates will be provided as a z-score, making it somewhat less intuitive.</p>
<section id="probit-link-function" class="level4">
<h4 class="anchored" data-anchor-id="probit-link-function">Probit Link Function</h4>
<p><span class="math display">\[
\Phi^{-1}(\gamma)
\]</span></p>
<p>Where <span class="math inline">\(\Phi\)</span> is the cumulative density function (CDF) of a <strong>standard normal distribution.</strong> I’m not going to break this open.</p>
</section>
<section id="probit-example-and-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="probit-example-and-interpretation">Probit Example and Interpretation</h4>
<p>Interpreting probit is quite difficult and less intuitive. This is because it uses a z-score!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>probit.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(farm_laborer <span class="sc">~</span> dalit <span class="sc">+</span> brahmin <span class="sc">+</span> male <span class="sc">+</span> muslim <span class="sc">+</span> education, <span class="at">data =</span> ies, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> probit))</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(probit.glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = farm_laborer ~ dalit + brahmin + male + muslim + 
    education, family = binomial(link = probit), data = ies)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.86594    0.04856 -17.833  &lt; 2e-16 ***
dalit        0.32714    0.05029   6.505 7.76e-11 ***
brahmin     -0.53717    0.12529  -4.287 1.81e-05 ***
male         0.20224    0.04240   4.770 1.85e-06 ***
muslim       0.12424    0.05963   2.083   0.0372 *  
education   -0.26350    0.02644  -9.966  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 5492.7  on 6750  degrees of freedom
Residual deviance: 5238.3  on 6745  degrees of freedom
AIC: 5250.3

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>So we run the same specification, only this time we are using the probit. On average, the effect of being Dalit is associated with a .327 z-score increase in being a farm laborer, all else equal. Recall that our z-score relates to our test statistic. We can conduct a similar interpretation across the other variables estimated.</p>
<p>So why do we have different coefficients? Let’s compare the two really quick using a stargazer table:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stargazer</span>(logit.glm, probit.glm, <span class="at">type =</span> <span class="st">"text"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
==============================================
                      Dependent variable:     
                  ----------------------------
                          farm_laborer        
                     logistic       probit    
                       (1)            (2)     
----------------------------------------------
dalit                0.581***      0.327***   
                     (0.088)        (0.050)   
                                              
brahmin             -1.185***      -0.537***  
                     (0.288)        (0.125)   
                                              
male                 0.361***      0.202***   
                     (0.078)        (0.042)   
                                              
muslim               0.235**        0.124**   
                     (0.108)        (0.060)   
                                              
education           -0.523***      -0.264***  
                     (0.055)        (0.026)   
                                              
Constant            -1.370***      -0.866***  
                     (0.094)        (0.049)   
                                              
----------------------------------------------
Observations          6,751          6,751    
Log Likelihood      -2,617.934    -2,619.166  
Akaike Inf. Crit.   5,247.868      5,250.333  
==============================================
Note:              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
</div>
<p>Now we can see clearly we have different coefficients between the probit and logit. Again probit coefficients are showing a unit increase in <span class="math inline">\(X\)</span> has a z-score change in <span class="math inline">\(Y\)</span>. We can also observe that the effect sizes (only due to different scale of units) is bigger within the logit estimation.</p>
</section>
</section>
<section id="logit-or-probit" class="level3">
<h3 class="anchored" data-anchor-id="logit-or-probit">Logit or Probit?</h3>
<p>Logit? Probit? Fuck it. Which one should we use? To be honest, I haven’t seemed to find a good answer to this. They are both nearly identical in results. However, the logit is considered the <em>canonical link function</em>, and thus has considerably more resources on it. Additionally, the logit uses the log of odds-ratios for interpretation.</p>
<p>Aside from greater resources available for the use of the logit, it seems the logits popularity might also be a result of it’s coefficients being larger than probits. It is not that logit estimates a different true effect, it’s just that the coefficient is higher because of the units our estimation is using. While it is not actually bigger effect size, it gives the appearance of being bigger. Perhaps the best reason I can give for its’ use is that bigger = better. Again, there is probably a specific reason to choose one over the other, but I have not been taught that and don’t really know.</p>
<p>An important point though is that probit is using the normal distribution. Intuitively this refers to a “latent index”. Meaning our variable may be coded as a dichotomous but underlying it might be a continuous data generating process; some legislators vote yay or nay; some legislators are very likely to vote yay, others may be on the fence.</p>
</section>
</section>
<section id="multiple-categoricalordinal-variable-choice-models" class="level2">
<h2 class="anchored" data-anchor-id="multiple-categoricalordinal-variable-choice-models">Multiple Categorical/Ordinal Variable (Choice Models)</h2>
<p>In this section, we look at the models we use when we have a categorical variable with more than two categories for our dependent variable. There are two classes of choice variables:</p>
<ol type="1">
<li>Multinomial Choice
<ol type="1">
<li>unordered (nominal)</li>
<li>no natural rank or order</li>
<li>ex: take the bus, train, car, or bike to work
<ol type="1">
<li>Which variables predict support for the 3 candidates?</li>
</ol></li>
</ol></li>
<li>Ordered Choice
<ol type="1">
<li>When choices have a natural rank</li>
<li>a form of ordinal data</li>
<li>Distance my vary between choices</li>
<li>ex: education level.</li>
</ol></li>
</ol>
<p>These outcomes do not share the same distribution that our binary variable had (the Bernoulli distribution). These outcomes are distributed from a <strong>multinomial distribution</strong>. We will start with the ordinal models first because they are relatively easier.</p>
<section id="ordered-logit" class="level3">
<h3 class="anchored" data-anchor-id="ordered-logit">Ordered Logit</h3>
<p>These models are appropriate for when we have a small number of ordered categories. Example: our DV is a survey question with ‘Strongly Agree’, ‘Agree’, ‘Disagree’, ‘Strongly Disagree’. These models have the same sort of trade-off that we observe in OLS - that is, we could just do OLS, but will have similar issues like out of bounds predictions and weird residuals. Running OLS on an ordered variable assumes that the data is interval: going from a 1 to 2 is the same <em>distance</em> as 3 to 4, or similar. Thus, we need to be aware of the distance between our categories. If we are concerned, we should certainly plan on using this model rather than a simple OLS estimation.</p>
</section>
<section id="multinomial-logit" class="level3">
<h3 class="anchored" data-anchor-id="multinomial-logit">Multinomial Logit</h3>
<p>This isn’t much different, only now we have multiple categories instead of just 0 or 1. So our goal here is to find out what variables predict which category is most likely to be selected. Like with categorical variables on our independent variables, we need to have some sort of baseline. We will do the same here just on our dependent variables. Thus,</p>
<section id="interpretation-1" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-1">Interpretation</h4>
<p>Regression output from R will be in the log-odds.</p>
</section>
<section id="multinomial-logit-code" class="level4">
<h4 class="anchored" data-anchor-id="multinomial-logit-code">Multinomial Logit Code</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read data </span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>ml <span class="ot">&lt;-</span> <span class="fu">read.dta</span>(<span class="st">"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(ml, <span class="fu">table</span>(ses, prog))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        prog
ses      general academic vocation
  low         16       19       12
  middle      20       44       31
  high         9       42        7</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>ml<span class="sc">$</span>prog2 <span class="ot">&lt;-</span> <span class="fu">relevel</span>(ml<span class="sc">$</span>prog, <span class="at">ref =</span> <span class="st">"academic"</span>) <span class="co"># define baseline category for DV</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">multinom</span>(prog2 <span class="sc">~</span> ses <span class="sc">+</span> write, <span class="at">data =</span> ml)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># weights:  15 (8 variable)
initial  value 219.722458 
iter  10 value 179.982880
final  value 179.981726 
converged</code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
multinom(formula = prog2 ~ ses + write, data = ml)

Coefficients:
         (Intercept)  sesmiddle    seshigh      write
general     2.852198 -0.5332810 -1.1628226 -0.0579287
vocation    5.218260  0.2913859 -0.9826649 -0.1136037

Std. Errors:
         (Intercept) sesmiddle   seshigh      write
general     1.166441 0.4437323 0.5142196 0.02141097
vocation    1.163552 0.4763739 0.5955665 0.02221996

Residual Deviance: 359.9635 
AIC: 375.9635 </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">summary</span>(test)<span class="sc">$</span>coefficients<span class="sc">/</span><span class="fu">summary</span>(test)<span class="sc">$</span>standard.errors</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         (Intercept)  sesmiddle   seshigh     write
general     2.445214 -1.2018081 -2.261334 -2.705562
vocation    4.484769  0.6116747 -1.649967 -5.112689</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2-tailed z test</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="fu">abs</span>(z), <span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          (Intercept) sesmiddle    seshigh        write
general  0.0144766100 0.2294379 0.02373856 6.818902e-03
vocation 0.0000072993 0.5407530 0.09894976 3.176045e-07</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="selection-models" class="level2">
<h2 class="anchored" data-anchor-id="selection-models">Selection Models</h2>
<p>Selection models are used when outcomes are observed only for a non-random subset of the population, leading to selection bias. A classic example is wages. Suppose we regress wages on education, experience, and other covariates. Wages, however, are only observed for individuals who choose to work. Because the decision to work depends partly on unobserved factors such as ability, motivation, or health—factors that also affect wages—the observed sample of workers is not random. As a result, running OLS on observed wages leads to biased and inconsistent estimates of the wage equation, and we cannot interpret the estimated coefficients as the causal effects of education on wages for the full population. <strong>We technically have a truncated sample.</strong> The problem is who we observe depends on unobservables related to wages.</p>
<p>With this in mind, there are actually two data generating processes going on.</p>
<ol type="1">
<li>Whether you work or not (dichotomous)</li>
<li>Given that you work, what are your wages? (continuous)</li>
</ol>
<p>To be quite honest, I’m not sure how much this model is even used today. I don’t think I’ve ever seen it in a political science paper. And you’ll notice this is attempting to solve a problem related to causal inference. Causal inference has come a long way since Heckman developed this 40 years ago. When we design research studies, we generally have a much better understanding of what our counterfactual is, and we have developed better causal designs to make it easier to identify the causal estimate. As such, selection bias is usually accounted for through the design process of our research. To reiterate, selection models are trying to deal with the problem of selection bias, specifically selection bias as a result of <em>unobservables,</em> if the selection bias was due to observables, we would simply condition on those variables to then argue our treatment is as-if random. There are more causal inference methods to account for these selection bias problems – DiD, IV, Natural Experiments, etc – which still make assumptions, but their commonality and intuition has led to the decline in usage of selection models.</p>
<section id="heckman-selection-model" class="level3">
<h3 class="anchored" data-anchor-id="heckman-selection-model">Heckman Selection Model</h3>
<p>The Heckman Selection Model is going to help us with this selection problem. It was developed by James Heckman, for which he won a Nobel Prize for. <strong>The idea behind the model is that we will estimate the extent of selection bias and control for it.</strong></p>
<p>Steps:</p>
<ol type="1">
<li>Estimate the selection equation using probit</li>
</ol>
</section>
</section>
<section id="count-models" class="level2">
<h2 class="anchored" data-anchor-id="count-models">Count Models</h2>
<p>What if we observe multiple Bernoulli trials over some length of time? This gives rise to <em>count</em> data. Importantly, count data exists within a set [0, <span class="math inline">\(\infty\)</span>] (it is not possible to have negative counts). Like with other data generating processes, we have a boundedness problem we must account for.</p>
<p><strong>But why can’t we use OLS with count data?</strong> We can (and this was popular to use in political science back in the day) but it leads to serious problems. First, when <span class="math inline">\(\lambda\)</span> is large enough, we can run OLS. However, if <span class="math inline">\(\lambda\)</span> is small (and this is so with truncation at zero) we should estimate a Poisson regression. Estimating OLS when we should have instead used a Poisson, will lead to inefficient, inconsistent, and biased estimates.</p>
<section id="poisson" class="level3">
<h3 class="anchored" data-anchor-id="poisson">Poisson</h3>
<p>The Poisson distribution is a probability distribution (no different than the other ones we have looked at).</p>
<p>The probability mass function (the discrete equivalent to a pdf) for the poisson distribution is as follows:</p>
<p><span class="math display">\[
P(X=x)=\frac{\lambda^xe^{-\lambda}}{x!}
\]</span></p>
<p>For <span class="math inline">\(x=0,1,2,3,…\infty\)</span></p>
<p>The <strong>mean</strong> <span class="math inline">\(\mu = \lambda\)</span>. We could have used <span class="math inline">\(\mu\)</span> and some people do that but because people are annoying, we are using <span class="math inline">\(\lambda\)</span> to represent the mean in the poisson distribution.</p>
<p>The <strong>variance</strong> <span class="math inline">\(\sigma^2=\lambda\)</span>. The mean and the variance in a poisson are equal! We are only estimating one parameter. If we know <span class="math inline">\(\lambda\)</span>, it tells us everything we need to know about distribution. While it is nice that the variance is equal to the mean, this is not always true. When this occurs, our model will likely have to change to accommodate this. More on this later below. Poisson can generally suffer from right skewness, especially when x (<span class="math inline">\(\lambda\)</span>) is small. As x gets bigger, the distribution becomes more symmetric. With this in mind (and discussed a bit above) we could get away with estimating at Poisson when Y is distributed normal, since the Poisson converges on the normal as <span class="math inline">\(\lambda\)</span> gets large.</p>
<p>In Poisson regression, we take the log of the conditional mean of the outcome, set it equal to a linear predictor, estimate the coefficients via MLE, and then exponentiate to obtain the expected count <span class="math inline">\(\lambda_i\)</span>​. The log of the mean is <em>linear!</em></p>
<section id="poisson-regression" class="level4">
<h4 class="anchored" data-anchor-id="poisson-regression">Poisson Regression</h4>
<p><span class="math display">\[
\begin{cases}ln(\lambda) = \beta_0+\Sigma\beta_iX_i &amp; Systematic \ component \\ y \sim pois(\lambda) &amp; Random \ component\end{cases}
\]</span></p>
<p>Where <span class="math inline">\(\lambda\)</span> is the expected number of occurrences for specified values of the explanatory variables. The coefficients <span class="math inline">\(\beta_i\)</span> are unknown and must be estimated from sample data. Notice the left hand side has a log (ln). This will take the positive values of our count variables and reconcile the fact the left side can take any value. This is the link function and all this is doing is bounding our results to be [0, <span class="math inline">\(\infty\)</span>].</p>
<p>That log we took is invertable. So our poisson regression looks like:</p>
<p><span class="math display">\[
ln(\lambda) = \beta_0 +\beta_1x_1+...\beta_ix_i
\]</span></p>
<p>We can write the inverted log which results in an equivalent equation form as:</p>
<p><span class="math display">\[
\lambda= e^{\beta_0 +\beta_1x_1+...\beta_ix_i}
\]</span></p>
<p><strong>To run a Poisson regression in R</strong>, we run the following code (note the code below does not do anything and just provides the syntax to run it with your associated data):</p>
</section>
<section id="poisson-assumptions" class="level4">
<h4 class="anchored" data-anchor-id="poisson-assumptions">Poisson Assumptions</h4>
<p>Events are occurring independently. That is, the fact that one event does not influence future or past events occurring. The probability that an event occurs in a given length of time does not change through time. <em>The events are occurring randomly and independently.</em> It is easy to lose track of the fact that Poisson distribution describes the event count per unit of time (per month, per year, etc.). Standard count models assume all units of have the same exposure time. This is an important assumption and you should be aware of it, and assess whether it is true in your data.</p>
</section>
<section id="poisson-regression---under-the-hood" class="level4">
<h4 class="anchored" data-anchor-id="poisson-regression---under-the-hood">Poisson Regression - Under the Hood</h4>
<p>Recall that we just need to estimate <span class="math inline">\(\lambda_i\)</span> to describe the distribution. Of course, we want to know the relationship between our systematic component (Xs) and our random component (Y). Since we are modeling count data, we will predict the conditional mean using a log-linear link:</p>
<p><span class="math display">\[
\lambda_i=E[y_i|x_i]=exp(\beta_0+\beta_1x_{i1}+\beta_2x_{2i}....+\beta_kx_{ik})=exp(x_i\pmb{\beta})
\]</span></p>
<p>The <span class="math inline">\(\lambda\)</span> represents the expected average value of y <em>conditional</em> on the covariates, <span class="math inline">\(x_i\)</span>. <strong>Why do we take the exponent?</strong> <span class="math inline">\(x_i\pmb{\beta} \in (-\infty, \infty)\)</span> The exponent will map the systematic component to between the values for our Poisson, which is 0 and positive infinity. This will guarantee the mean count is positive.</p>
<p>We are not taking the log of the linear predictors. We are taking the log of the <em>conditional mean.</em></p>
</section>
<section id="poisson-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="poisson-interpretation">Poisson Interpretation</h4>
<p>The coefficient we get is a change in the log-likelihood. That is, a one unit change in <span class="math inline">\(x\)</span> has a <span class="math inline">\(\beta\)</span> change in the log of the expected count of <span class="math inline">\(y_i\)</span>. Like any of the previous models, these can be bit wonky to read from a regression table.</p>
<section id="example-from-the-internet-authors-used-stata" class="level5">
<h5 class="anchored" data-anchor-id="example-from-the-internet-authors-used-stata">Example from the internet (authors used Stata)</h5>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="poisson_table.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1002"></p>
</figure>
</div>
<p><code>mathnce</code> represents the coefficient on <code>daysabs</code>. <code>mathnce</code> refers to a math standardize score. Thus a one unit increase in <code>mathnce</code> , the difference in the logs of expected counts would be expected to decrease by -.0035232, holding all else (other variables) constant. This result yields a p-value of .053. If our alpha level is .05, we would reject the null hypothesis and conclude the effect is not statistically difference from zero. What our <code>mathnce</code> represents is really a one unit increase in <code>mathnce</code>, decreases <code>daysabs</code> by a factor of <span class="math inline">\(e^{-.0035232}\approx 0.99648\)</span></p>
<p>This interpretation would follow similarly for the other covariates. However, like logit, the effect of this magnitude depends on a baseline.</p>
</section>
<section id="incidence-rate-ratio-irr" class="level5">
<h5 class="anchored" data-anchor-id="incidence-rate-ratio-irr">Incidence-Rate Ratio (IRR)</h5>
<p>IRR’s seem to be most common method of interpretation outside of the raw reports in regression tables. IRRs are obtained by exponentiating the Poisson regression coefficient. Note that we actually did this above and got a value of .99648. A one unit increase in <code>mathnce</code> is associated with .00352% in the expected count, all else equal.</p>
</section>
</section>
<section id="overdispersion" class="level4">
<h4 class="anchored" data-anchor-id="overdispersion">Overdispersion</h4>
<p>Poisson regression assumes that the variance of <span class="math inline">\(y\)</span> equals the mean of <span class="math inline">\(y\)</span>, but in some/many datasets the variance is greater than the mean, which we call “overdispersion”. There is also underdispersion (but is less common) and occurs when the variance is less than the mean. In notation, overdispersion is shown as: <span class="math inline">\(\lambda &lt; Var(Y)\)</span>. If a random variable is overdispersed, it was <em>not</em> generated via a Poisson process. Overdispersion is <em>extremely common and hugely problematic.</em> <strong>Why do we care about overdispersion?</strong> It influences our standard error and thus our standard error is underestimated; the model is assuming lower variance than is actually true. This will in turn, influence our p-value and result in a type 1 error. <strong>What causes overdispersion?</strong> There are predictable variables that we have not included (omitted variable bias), need to look at theory. Or we might have clustering or heterogeneity in the sampled population. Finally, we may more zero values than expected (zero-inflated).</p>
<p><strong>What do we do with overdispersion?</strong> We can do a <em>quasi-poisson</em>. The quasi-poisson basically takes the raw Poisson model, estimates the dispersion, and adjusts the standard errors for it. OR, we can also do a <em>negative binomial</em>, which is what most people seem to do. The negative binomial models a slightly different distribution with an extra parameter that characterizes the variance, and derives a different likelihood based on that. More on negative binomial below.</p>
<p><strong>How do we know if we have overdispersion?</strong> <em>We only care about conditional overdispersion, i.e., how much overdispersion is left over once we’ve modeled the conditional mean.</em> A casual (first-cut) test is simply to take the mean and variance of our DV and see how far apart they are. This is not terrible but is also not precise. It might be better to think over overdispersion as a form of specification error.</p>
</section>
<section id="poisson-issues" class="level4">
<h4 class="anchored" data-anchor-id="poisson-issues">Poisson Issues</h4>
<p>As discussed earlier, Poisson makes a critical assumption that the variance and mean are equal. This is commonly not true.</p>
</section>
</section>
<section id="negative-binomial" class="level3">
<h3 class="anchored" data-anchor-id="negative-binomial">Negative Binomial</h3>
<p>We use these also for count data! When we use a Poisson we make an assumption that may not be true, which is the mean is equal to variance. The negative binomial will have a different mean and variance. That is the difference! By relaxing this assumption, we can allow for overdispersion. Recall that if we have overdispersion, our random variable was not generated via a Poisson process.</p>
<p>The <strong>negative binomial distribution</strong> describes the number of Bernoulli trials needed to obtain set number of successes. For instance, suppose we flip a fair coin until the fourth head. While there is no upper limit on the number of flips that could be needed, larger values are increasingly unlikely. The negative binomial distribution can also be viewed as modeling the number of success before a desired number of failures. For our purposes, and to see the connection with the Poisson, it is easiest to think about it as a distribution that allows unobserved heterogeneity in a count process. In my humble opinion, just run a negative binomial model before doing a Poisson.</p>
<section id="when-to-use-a-negative-binomial" class="level4">
<h4 class="anchored" data-anchor-id="when-to-use-a-negative-binomial">When to use a negative binomial</h4>
<p>Negative binomial regression is for modeling overdispersed count outcome variables.</p>
<ul>
<li><p>Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean</p></li>
<li><p>It can be considered as a generalization of Poisson regression since it has the same mean structure as Poisson regression and it has an extra parameter (r) to model the over-dispersion.</p></li>
</ul>
</section>
<section id="negative-binomial-under-the-hood" class="level4">
<h4 class="anchored" data-anchor-id="negative-binomial-under-the-hood">Negative Binomial under the hood</h4>
<p>Within the conditional mean framework, the Poisson models:</p>
<p><span class="math display">\[
\lambda_i = Exp[\pmb{X}_i\beta]
\]</span></p>
<p>When we do this, we are saying that all variation in <span class="math inline">\(\lambda_i\)</span> is due to observed heterogeneity. Instead suppose the following:</p>
<p><span class="math display">\[
\hat{\lambda}_i = Exp[\pmb{X}_i\beta +\epsilon_i]
\]</span></p>
<p>Now, <span class="math inline">\(\lambda\)</span> is a random variable and depends upon both observed <em>and</em> unobserved heterogeneity. We can further express <span class="math inline">\(\hat{\lambda}\)</span> as:</p>
<p><span class="math display">\[
\hat{\lambda}_i = Exp[\pmb{X}_i\beta]Exp[\epsilon_i]
\]</span></p>
<p>Let <span class="math inline">\(\gamma_i\)</span> denote <span class="math inline">\(Exp[\epsilon_i]\)</span>, then we have <span class="math inline">\(\hat{\lambda}_i=\lambda_i\gamma_i\)</span>. Hence, our mean is the Poisson conditional mean, <strong>scaled</strong> by a multiplicative term based on the error. It is necessary to assume something about the distribution of <span class="math inline">\(\epsilon\)</span>, and in particular we want <span class="math inline">\(E(\gamma_i)=1\)</span> We still assume the baseline Poisson process. However, we do not know <span class="math inline">\(\gamma_i\)</span>, and only know <span class="math inline">\(x_i\)</span>; we can construct an integral to sum across all the possible values of <span class="math inline">\(\gamma\)</span>, effectively averaging across. But doing so forces us to specify a distribution, we will use the <em>Gamma distribution</em>.</p>
</section>
<section id="negative-binomial-in-r" class="level4">
<h4 class="anchored" data-anchor-id="negative-binomial-in-r">Negative Binomial in R</h4>
<p>When running a negative binomial in R, we will use the <code>MASS</code> package. So make sure you have that installed and running! Note the code below does not run.</p>
</section>
</section>
<section id="poisson-or-negative-binomial-model" class="level3">
<h3 class="anchored" data-anchor-id="poisson-or-negative-binomial-model">Poisson or Negative Binomial Model?</h3>
<p>Which model do we chose? The negative binomial estimates only one more parameter than the Poisson/quasi-poisson, there is little efficiency downside to using it. <em>Thus, defaulting to negative binomial is defensible.</em> However, it is usually recommended to prefer the Poisson setups in small samples, as NB can be a little oversensitive when your estimates of the variance aren’t that precise. Also be careful when doing more ‘fancy’ modeling; sometimes the simplicity of the Poisson/quasi-Poisson is preferred, e.g.&nbsp;for hierarchical modeling.</p>
</section>
</section>
<section id="extensions-of-count-models" class="level2">
<h2 class="anchored" data-anchor-id="extensions-of-count-models">Extensions of Count Models</h2>
<p>All of these extensions can be done with either Poisson or Negative Binomial distribution assumptions. Like before, you should probably just lean towards using the negative binomial model. The idea is similar to the Heckman selection models, the observed data are generated by more than one process.</p>
<section id="zero-inflated-models" class="level3">
<h3 class="anchored" data-anchor-id="zero-inflated-models">Zero-inflated models</h3>
<p>Zero-inflated models are designed to handle cases where we have ‘excess’ zeros. Zero-inflated doesn’t just mean “a lot of zeros”. The ZI setup is designed for situations where you have a mixture of data-generating processes. Think of carving up your observations into two pools:</p>
<ul>
<li><p>‘Eligible’: observations which are experiencing the same event being counted</p></li>
<li><p>‘Ineligible’: observations which cannot experience the event, and therefore will always equal 0.</p></li>
</ul>
<p><em>This is largely a theoretical matter</em> and something you should be thinking about in those terms. Let’s look at an example from class to better illustrate what is going on. Zero inflation of the data set might happen if our sample includes a subgroup, <em>which can only have a count of zero</em>. For example, if we have data of the number of hours people watch TV per day, it may invovle zeros for people who do not watch TV that often, but also zeros for people who do not have a TV. The people who do not have a TV, will cause an inflation of zeros that cannot be explained by the Poisson distribution. These types of zeros are called <strong>structural zeros.</strong> The zero-inflated aspect can be applied to both Poisson or Negative Binomial models.</p>
<section id="example" class="level4">
<h4 class="anchored" data-anchor-id="example">Example</h4>
<p>Suppose we are predicting the number of Netflix shows watched last month. As of October 2021, Netflix has over 213 million streaming subscribers worldwide (74 million in the US/Canada). Suppose we take a random sample of <em>the entire US</em>; we would suspect that we are drawing from <strong>two distinct populations:</strong></p>
<ol type="1">
<li>For those that don’t have Netflix, <span class="math inline">\(y_i=0\)</span> by definition</li>
<li>For those that do, a subscriber may watch <span class="math inline">\(y_i=0,1,2,…\)</span></li>
</ol>
<p>Let’s assume that “Have Netflix” <span class="math inline">\(\sim Bernoulli \ (p=.65)\)</span>. Where if “Have Netflix” <span class="math inline">\(=1, \sim Poisson \ (\lambda =2)\)</span>. Let’s read through what this means in plain English. Recall that <span class="math inline">\(p\)</span> is the parameter we are estimating in a Bernoulli trial. If we know <span class="math inline">\(p\)</span>, we can describe the shape of the Bernoulli distribution. Because <span class="math inline">\(p=.65\)</span> we are saying that the probability of having Netflix is <span class="math inline">\(.65\)</span>.</p>
<p>Let’s visualize this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create a ZIP:</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>netflix.has <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(sample, <span class="at">size=</span><span class="dv">1</span>, <span class="at">prob =</span> <span class="fl">0.65</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(netflix.has) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>netflix.has
   0    1 
3453 6547 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>netflix.poi <span class="ot">&lt;-</span> <span class="fu">rpois</span>(sample<span class="sc">*</span><span class="fu">mean</span>(netflix.has), <span class="at">lambda =</span> <span class="dv">2</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(netflix.poi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>netflix.poi
   0    1    2    3    4    5    6    7    8   10 
 908 1803 1735 1180  602  222   69   22    4    1 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>netflix.has[netflix.has <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span> <span class="co"># b/c 1 means you're in Poisson </span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>netflix.has <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(netflix.has)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>combined <span class="ot">&lt;-</span> <span class="fu">c</span>(netflix.has,netflix.poi)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(combined)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>combined
   0    1    2    3    4    5    6    7    8   10 
4361 1803 1735 1180  602  222   69   22    4    1 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(netflix.poi, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">main =</span> <span class="st">"Netflix Shows Binged"</span>, <span class="at">sub=</span> <span class="st">"w/o non-subscribers"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">4500</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(combined, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">main =</span> <span class="st">"Netflix Shows Binged"</span>, <span class="at">sub=</span><span class="st">"with non-subscribers"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">4500</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="intuition" class="level4">
<h4 class="anchored" data-anchor-id="intuition">Intuition</h4>
<p>When we are doing a zero-inflated model, we are saying there is a two step process. We are combining a logistic regression with a poisson regression. The logistic regression will be used to predict who gets a zero and any other number. Then, if you didn’t have a zero, what is the number that you did have.</p>
</section>
<section id="when-do-i-know-you-have-zero-inflation" class="level4">
<h4 class="anchored" data-anchor-id="when-do-i-know-you-have-zero-inflation">When do I know you have zero-inflation?</h4>
<p>There are tests you can do. In all honesty, just run both models (ZI-Poisson and Poisson) and see if findings change substantively. Compare the AIC/BIC.</p>
</section>
<section id="code-for-zero-inflated-models" class="level4">
<h4 class="anchored" data-anchor-id="code-for-zero-inflated-models">Code for Zero-Inflated Models</h4>
<p>…and an example to boot! This example will use code from Finocchiaro and Mackenzie 2017. Download [<a href="https://drive.google.com/file/d/13_choKihDD1XQlbbxBGkkWJyDxMDqQTU/view?usp=sharing">here</a>]. Example is provided through Andy Philips.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>us.bills <span class="ot">&lt;-</span> <span class="fu">read.dta</span>(<span class="st">"Finocchiaro and MacKenzie 2017.dta"</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>us.bills <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(us.bills) <span class="co"># remove NAs if needed</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(us.bills<span class="sc">$</span>policy, <span class="at">main =</span> <span class="st">"No. Bills Introduced (policy)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The DV policy has a large number of zeros.…if we had some suspicion that there was a different DGP producing some of these 0’s, we may want to run a zero-inflated model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>zip <span class="ot">&lt;-</span> <span class="fu">zeroinfl</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom, <span class="at">data =</span> us.bills, <span class="at">dist =</span> <span class="fu">c</span>(<span class="st">"poisson"</span>), <span class="at">link =</span> <span class="fu">c</span>(<span class="st">"logit"</span>))</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(zip)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
zeroinfl(formula = policy ~ margin + gdem_p_12_ave + democrat + ind + 
    com_pcom, data = us.bills, dist = c("poisson"), link = c("logit"))

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.4511 -0.9823 -0.5437  0.3243 44.3598 

Count model coefficients (poisson with log link):
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    0.3105258  0.0978615   3.173  0.00151 ** 
margin         0.0079158  0.0006923  11.434  &lt; 2e-16 ***
gdem_p_12_ave  0.0189232  0.0020159   9.387  &lt; 2e-16 ***
democrat      -0.1466010  0.0247088  -5.933 2.97e-09 ***
ind           -0.0074117  0.0221020  -0.335  0.73737    
com_pcom      -0.6328539  0.0604152 -10.475  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.679500   0.314422  -2.161   0.0307 *  
margin        -0.006614   0.002634  -2.510   0.0121 *  
gdem_p_12_ave -0.002078   0.006454  -0.322   0.7475    
democrat       0.046105   0.077761   0.593   0.5532    
ind            0.324291   0.070616   4.592 4.38e-06 ***
com_pcom       0.262836   0.145529   1.806   0.0709 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 20 
Log-likelihood: -1.142e+04 on 12 Df</code></pre>
</div>
</div>
<p>The top part predicts counts for units that could be greater than zero (including zeros from chance), while the bottom part predicts which units are structurally zero. Together, they determine the expected count for every observation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Looks like ind and margin are sig. in the binomial part of the model. We can estimate just these coefficients (as well as a few others we may think determines 0's) as follows: </span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>zip<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">zeroinfl</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom <span class="sc">|</span> margin <span class="sc">+</span> ind <span class="sc">+</span> <span class="fu">log</span>(age), <span class="at">data =</span> us.bills, <span class="at">dist =</span> <span class="fu">c</span>(<span class="st">"poisson"</span>), <span class="at">link =</span> <span class="fu">c</span>(<span class="st">"logit"</span>))</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(zip<span class="fl">.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
zeroinfl(formula = policy ~ margin + gdem_p_12_ave + democrat + ind + 
    com_pcom | margin + ind + log(age), data = us.bills, dist = c("poisson"), 
    link = c("logit"))

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.4553 -0.9747 -0.5348  0.3244 44.4218 

Count model coefficients (poisson with log link):
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    0.3030747  0.0969146   3.127  0.00176 ** 
margin         0.0079202  0.0006921  11.444  &lt; 2e-16 ***
gdem_p_12_ave  0.0190968  0.0019953   9.571  &lt; 2e-16 ***
democrat      -0.1484943  0.0244894  -6.064 1.33e-09 ***
ind           -0.0073148  0.0221074  -0.331  0.74074    
com_pcom      -0.6796520  0.0571132 -11.900  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.230498   0.732201   0.315   0.7529    
margin      -0.006505   0.002567  -2.534   0.0113 *  
ind          0.330997   0.070368   4.704 2.55e-06 ***
log(age)    -0.253860   0.189753  -1.338   0.1809    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 16 
Log-likelihood: -1.142e+04 on 10 Df</code></pre>
</div>
</div>
<p>This model is basically the same, however, we are more closely specifying what determines the structural zeros. We were doing that before but using the same variables for both the poisson and the logistic regression. Here, we are using different specifications for the two different data generating process. This gives us more flexibility.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create predicted count for each obs</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">fitted</span>(zip<span class="fl">.2</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(preds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.6809  2.0806  2.3416  2.3046  2.5676  6.0937 </code></pre>
</div>
</div>
<section id="comparison-to-poisson-results" class="level5">
<h5 class="anchored" data-anchor-id="comparison-to-poisson-results">Comparison to Poisson Results:</h5>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>poi <span class="ot">&lt;-</span> <span class="fu">glm</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom, <span class="at">family =</span> <span class="st">"poisson"</span>, <span class="at">data =</span> us.bills)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">stargazer</span>(zip<span class="fl">.2</span>, poi, <span class="at">type =</span> <span class="st">"text"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
==============================================
                      Dependent variable:     
                  ----------------------------
                             policy           
                   zero-inflated    Poisson   
                    count data                
                        (1)           (2)     
----------------------------------------------
margin               0.008***       0.010***  
                      (0.001)       (0.001)   
                                              
gdem_p_12_ave        0.019***       0.018***  
                      (0.002)       (0.002)   
                                              
democrat             -0.148***     -0.155***  
                      (0.024)       (0.023)   
                                              
ind                   -0.007       -0.119***  
                      (0.022)       (0.021)   
                                              
com_pcom             -0.680***     -0.721***  
                      (0.057)       (0.049)   
                                              
Constant             0.303***        -0.044   
                      (0.097)       (0.089)   
                                              
----------------------------------------------
Observations           4,488         4,488    
Log Likelihood      -11,423.990   -13,099.620 
Akaike Inf. Crit.                  26,211.230 
==============================================
Note:              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
</div>
<p>With comparison, we can see the different results between the ZIP and the regular Poisson. We observe the biggest difference in the models is with the <code>ind</code> variable no longer being significant. This is because the <code>ind</code> variable is <em>not</em> determining the count but rather, determining whether the observation is a structural zero or not.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(poi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 26211.23</code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(zip)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 22870.44</code></pre>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(zip<span class="fl">.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 22867.97</code></pre>
</div>
</div>
<p>If we compare the AIC’s of the models, we can see the difference. Remember, lower AIC = better model. There is a huge jump between <code>poi</code> to <code>zip</code>. Our model improves, but only slightly from <code>zip</code> to <code>zip.2</code>. <code>zip.2</code> is still the preferable model.</p>
</section>
</section>
<section id="code-for-zero-inflated-negative-binomial" class="level4">
<h4 class="anchored" data-anchor-id="code-for-zero-inflated-negative-binomial">Code for Zero-Inflated Negative Binomial</h4>
<p>Everything is the same just with a negative binomial.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We can use the pscl package to estimate a ZINB. We'll also change the link function for the Bernoulli model:</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>zinb <span class="ot">&lt;-</span> <span class="fu">zeroinfl</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom <span class="sc">|</span> margin <span class="sc">+</span> ind <span class="sc">+</span> age, <span class="at">data =</span> us.bills, <span class="at">dist =</span> <span class="fu">c</span>(<span class="st">"negbin"</span>), <span class="at">link =</span> <span class="fu">c</span>(<span class="st">"probit"</span>)) <span class="co"># note that keeping log(age) results in an indet. Hessian</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(zinb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
zeroinfl(formula = policy ~ margin + gdem_p_12_ave + democrat + ind + 
    com_pcom | margin + ind + age, data = us.bills, dist = c("negbin"), 
    link = c("probit"))

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.7845 -0.7139 -0.3794  0.2381 28.4273 

Count model coefficients (negbin with log link):
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.185281   0.188253  -0.984  0.32501    
margin         0.012151   0.001729   7.027 2.12e-12 ***
gdem_p_12_ave  0.020537   0.003824   5.371 7.85e-08 ***
democrat      -0.136405   0.047756  -2.856  0.00429 ** 
ind           -0.100354   0.043710  -2.296  0.02168 *  
com_pcom      -0.718785   0.082689  -8.693  &lt; 2e-16 ***
Log(theta)    -0.389794   0.031308 -12.450  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with probit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   7.48919  499.97314   0.015 0.988049    
margin        0.02064    0.01225   1.686 0.091886 .  
ind           6.33284  499.96193   0.013 0.989894    
age          -0.46227    0.13010  -3.553 0.000381 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Theta = 0.6772 
Number of iterations in BFGS optimization: 46 
Log-likelihood: -8900 on 11 Df</code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare to a regular neg-bin:</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>nb <span class="ot">&lt;-</span> <span class="fu">glm.nb</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom, <span class="at">data =</span> us.bills)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="fu">stargazer</span>(zinb, nb, <span class="at">type =</span> <span class="st">"text"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
================================================
                       Dependent variable:      
                  ------------------------------
                              policy            
                  zero-inflated     negative    
                   count data       binomial    
                       (1)            (2)       
------------------------------------------------
margin              0.012***        0.012***    
                     (0.002)        (0.002)     
                                                
gdem_p_12_ave       0.021***        0.021***    
                     (0.004)        (0.004)     
                                                
democrat            -0.136***      -0.154***    
                     (0.048)        (0.048)     
                                                
ind                 -0.100**       -0.116***    
                     (0.044)        (0.044)     
                                                
com_pcom            -0.719***      -0.714***    
                     (0.083)        (0.083)     
                                                
Constant             -0.185          -0.188     
                     (0.188)        (0.189)     
                                                
------------------------------------------------
Observations          4,488          4,488      
Log Likelihood     -8,900.035      -8,916.633   
theta                           0.668*** (0.021)
Akaike Inf. Crit.                  17,845.270   
================================================
Note:                *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(zinb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 17822.07</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(nb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 17845.27</code></pre>
</div>
</div>
</section>
<section id="in-summary" class="level4">
<h4 class="anchored" data-anchor-id="in-summary">In Summary:</h4>
<p>Count model (Poisson/Neg-BIN) ---&gt; expected counts if at risk</p>
<p>Zero-inflation model (Logit) ---&gt; probability of structural zero</p>
</section>
</section>
<section id="truncated-count-models" class="level3">
<h3 class="anchored" data-anchor-id="truncated-count-models">Truncated count models</h3>
<p>Recall that truncation means we have values excluded from our sample. This is very similar to the zero-inflated count model but the problem is inversed. That is, we do not observe zeros. This is related to the Heckman selectionn model, in that we have a selection problem. Our data is truncated such that we are not observing observations from a specific group. Usually, truncation at zero.</p>
<section id="code-for-truncated-counts" class="level4">
<h4 class="anchored" data-anchor-id="code-for-truncated-counts">Code for Truncated Counts</h4>
<p>Also provided by our lord and savior, Andy Philips:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we use truncated models when there is truncation at zero (less commonly, some type of upper truncation)</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's create some data w/ zero-truncation....obviously we wouldn't do this with actual data!</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>trunc.data <span class="ot">&lt;-</span> us.bills</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>trunc.data<span class="sc">$</span>policy[trunc.data<span class="sc">$</span>policy <span class="sc">==</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span> <span class="co"># replace 0's with NA</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(trunc.data<span class="sc">$</span>policy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 
927 657 349 301 181 114  80  57  48  29  21  17  10   9   7  21   5   4   2   1 
 22  23  24  26  28  30  31  33  35  36  42  47  52  62  75 104 
  1   2   1   2   5   5   6   1   1   2   1   1   2   1   1   1 </code></pre>
</div>
</div>
<p><strong>Truncated Poisson:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Zero-Truncated Poisson:</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(VGAM)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>ztp <span class="ot">&lt;-</span> <span class="fu">vglm</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom, <span class="at">family =</span> <span class="fu">pospoisson</span>(), <span class="at">data =</span> trunc.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, :
iterations terminated because half-step sizes are very small</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some
quantities such as z, residuals, SEs may be inaccurate due to convergence at a
half-step</code></pre>
</div>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ztp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
vglm(formula = policy ~ margin + gdem_p_12_ave + democrat + ind + 
    com_pcom, family = pospoisson(), data = trunc.data)

Coefficients: 
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    0.324959   0.097846   3.321 0.000897 ***
margin         0.007890   0.000693  11.385  &lt; 2e-16 ***
gdem_p_12_ave  0.018617   0.002015   9.239  &lt; 2e-16 ***
democrat      -0.143195   0.024658  -5.807 6.35e-09 ***
ind           -0.008700   0.022130  -0.393 0.694233    
com_pcom      -0.628335   0.060321 -10.417  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Name of linear predictor: loglink(lambda) 

Log-likelihood: -8523.525 on 2867 degrees of freedom

Number of Fisher scoring iterations: 5 

No Hauck-Donner effect found in any of the estimates</code></pre>
</div>
</div>
<p><strong>Truncated Neg-BIN:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Zero-truncated Neg-BIN:</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co"># note this was not working! </span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="co">#ztnb &lt;- vglm(policy ~ margin + gdem_p_12_ave + democrat + ind + com_pcom,family = posnegbinomial(),data = trunc.data)</span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(ztnb)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="hurdle-models" class="level3">
<h3 class="anchored" data-anchor-id="hurdle-models">Hurdle Models</h3>
<p>Based in part on the truncated models. The assumption is that there is a unique DGP, with Bernoulli distribution, that determines whether we observe either a zero, or a truncated Poisson. It only models positive counts. Basically, you have to pass the hurdle of being a zero or not. If you are not a zero, then you pass the hurdle and get modeled into the count.</p>
<section id="code-for-hurdle-models" class="level4">
<h4 class="anchored" data-anchor-id="code-for-hurdle-models">Code for Hurdle Models:</h4>
<p><strong>Poisson Hurdle:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pscl estimates hurdle models. Once again we can put whatever we want as Z in the selection model</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="co"># first the Poisson hurdle</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>p.hurd <span class="ot">&lt;-</span> <span class="fu">hurdle</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom <span class="sc">|</span> margin <span class="sc">+</span> ind <span class="sc">+</span> <span class="fu">log</span>(age), <span class="at">data =</span> us.bills, <span class="at">dist =</span> <span class="fu">c</span>(<span class="st">"poisson"</span>), <span class="at">link =</span> <span class="fu">c</span>(<span class="st">"logit"</span>), <span class="at">zero.dist =</span> <span class="fu">c</span>(<span class="st">"binomial"</span>))</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(p.hurd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
hurdle(formula = policy ~ margin + gdem_p_12_ave + democrat + ind + com_pcom | 
    margin + ind + log(age), data = us.bills, dist = c("poisson"), zero.dist = c("binomial"), 
    link = c("logit"))

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.4650 -0.9833 -0.5224  0.3211 44.2858 

Count model coefficients (truncated poisson with log link):
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    0.3249813  0.0978210   3.322 0.000893 ***
margin         0.0078897  0.0006927  11.390  &lt; 2e-16 ***
gdem_p_12_ave  0.0186167  0.0020141   9.243  &lt; 2e-16 ***
democrat      -0.1431932  0.0246588  -5.807 6.36e-09 ***
ind           -0.0087019  0.0221297  -0.393 0.694155    
com_pcom      -0.6283296  0.0603206 -10.417  &lt; 2e-16 ***
Zero hurdle model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.173206   0.669603  -0.259 0.795890    
margin       0.008613   0.002419   3.560 0.000371 ***
ind         -0.311135   0.064487  -4.825  1.4e-06 ***
log(age)     0.194591   0.173385   1.122 0.261733    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 13 
Log-likelihood: -1.144e+04 on 10 Df</code></pre>
</div>
</div>
<p><strong>Neg-Bin Hurdle:</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and the neg-bin hurdle:</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>nb.hurd <span class="ot">&lt;-</span> <span class="fu">hurdle</span>(policy <span class="sc">~</span> margin <span class="sc">+</span> gdem_p_12_ave <span class="sc">+</span> democrat <span class="sc">+</span> ind <span class="sc">+</span> com_pcom <span class="sc">|</span> margin <span class="sc">+</span> ind <span class="sc">+</span> <span class="fu">log</span>(age), <span class="at">data =</span> us.bills, <span class="at">dist =</span> <span class="fu">c</span>(<span class="st">"negbin"</span>), <span class="at">link =</span> <span class="fu">c</span>(<span class="st">"logit"</span>), <span class="at">zero.dist =</span> <span class="fu">c</span>(<span class="st">"binomial"</span>))</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(nb.hurd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
hurdle(formula = policy ~ margin + gdem_p_12_ave + democrat + ind + com_pcom | 
    margin + ind + log(age), data = us.bills, dist = c("negbin"), zero.dist = c("binomial"), 
    link = c("logit"))

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.8665 -0.6507 -0.3495  0.2198 27.4545 

Count model coefficients (truncated negbin with log link):
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.955456   0.283882  -3.366 0.000764 ***
margin         0.014417   0.002577   5.594 2.22e-08 ***
gdem_p_12_ave  0.027093   0.005380   5.036 4.75e-07 ***
democrat      -0.187636   0.067939  -2.762 0.005748 ** 
ind           -0.014570   0.062309  -0.234 0.815115    
com_pcom      -0.815771   0.125175  -6.517 7.17e-11 ***
Log(theta)    -1.130049   0.135079  -8.366  &lt; 2e-16 ***
Zero hurdle model coefficients (binomial with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.173206   0.669603  -0.259 0.795890    
margin       0.008613   0.002419   3.560 0.000371 ***
ind         -0.311135   0.064487  -4.825  1.4e-06 ***
log(age)     0.194591   0.173385   1.122 0.261733    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Theta: count = 0.323
Number of iterations in BFGS optimization: 19 
Log-likelihood: -8894 on 11 Df</code></pre>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="section-x-panel-data" class="level1">
<h1>Section X: Panel Data</h1>
<p>Panel data aka longitudinal data aka data over time aka time-series data; in short, panel data has many names but is simply what we call data that observes the same observation at multiple time periods.</p>
<section id="fixed-effects" class="level2">
<h2 class="anchored" data-anchor-id="fixed-effects">Fixed Effects</h2>
<p>Fixed effects is a regression specification that typically is used with panel data (observations over time). This is not the only way fixed effects gets used but for now we will only focus on its’ use in relation to time. To understand the intuition of fixed effects, it is important to understand what is going on in a <em>pooled</em> model. By pooled, I mean that all observations over time are estimated together.</p>
<p>Imagine our data set looks something like the following:</p>
<table class="table">
<thead>
<tr class="header">
<th>ID</th>
<th>year</th>
<th>treated</th>
<th>outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2018</td>
<td>0</td>
<td>5</td>
</tr>
<tr class="even">
<td>1</td>
<td>2019</td>
<td>0</td>
<td>5</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2018</td>
<td>0</td>
<td>7</td>
</tr>
<tr class="even">
<td>2</td>
<td>2019</td>
<td>1</td>
<td>9</td>
</tr>
<tr class="odd">
<td>3</td>
<td>2018</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="even">
<td>3</td>
<td>2019</td>
<td>1</td>
<td>7</td>
</tr>
<tr class="odd">
<td>4</td>
<td>2018</td>
<td>0</td>
<td>2</td>
</tr>
<tr class="even">
<td>4</td>
<td>2019</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Here we have four individuals, observed at two different time periods. When we run a pooled model, our regression is going to treat each of these rows as an individuals. We will the effect of our treatment across <em>all</em> observations. The pooled model literally takes all the rows and throws them in the same ‘washing machine’. But in doing this, our OLS estimate will be biased. What we need to do is tell R to estimate the effect <em>within</em> each individual. That is, rather than throw them all in the same washing machine, take the ‘1’ observations and put them into the washing machine separate from the ‘2’ observations, etc. What is the difference in our outcome when we compare the treatment effect within the individual? Fixed effects is literally just controlling for this individual being <em>this individual.</em> It will treat the individual as a categorical variable. Since we control for <em>who the person is</em>, we partial out a lot of variation. Importantly, running fixed effects at the ID level, we control for all <strong><em>time-invariant factors</em></strong>. These are variables that do not vary over time. One example would be race. If we were to run a regression with fixed effects at the id level, we would not need to include race as a covariate in the equation because the fixed effects already contains that information. There is no variation in race over time, it is static! However, it will not partial out variation that occurs over time. An example of a time-varying variable might be income, people get new jobs/raises with different pay. These time-varying characteristics need to still be controlled for in our equation.</p>
<p>Again we have an unobserved variable problem.</p>
<p>Like we have previously, we want to invoke the conditional independence assumption - conditional on observable and unobservable variables, our treatment is as good as random. How do we control for unobservable variables? Fixed effects.</p>
<p>it is going to partial out the effects for who these individuals are.</p>
<p>So if we do fixed effects on year. Its going to treat the year as a categorical variable and subtract out the unobserved variation specific to that year on our outcome variable.</p>
<p>THESE REMOVE TIME INVARIANT CHARACTERISTICS!</p>
<p>Fixed effects is going to</p>
<p>WE USE CLUSTERED STANDARD ERRORS WHEN OUR ERRORS ARE CORRELATED WITH EACH OTHER. MEANING WE VIOLATE OUR ASSUMPTION OF INDEPENDENT ERRORS. THIS OCCURS IN PANEL DATA BECAUSE WE HAVE INDIVIDUALS OVER TIME. IF WE DONT CLUSTER, OUR STANDARD ERRORS ARE POOLED. WE NEED TO TELL STATA THAT THE THESE ARE INDIVIDUALS OVER TIME. MEANING INDIVIDUAL AT T1 T2 T3 HAVE CORRELATED RESIDUALS AKA AUTOCORRELATION. WHEN WE POOL IT, WE CALCULATE THE SE WITHOUT ACCOUNTING FOR THIS. WE NEED SAY “HEY TREAT THESE SETS OF RESIDUALS FOR EACH INDVIDIAUL SO THAT WE CAN KEEP THEM INDEPENDENT FROM OTHER INDIVIDUALS OWN SET OF RESIDUALS”.</p>
<p>Fixed effects is going to produce a different intercept for each individual.</p>
</section>
<section id="fixed-effects-does-not-fix-everything" class="level2">
<h2 class="anchored" data-anchor-id="fixed-effects-does-not-fix-everything">Fixed effects does not fix everything</h2>
<p>Fixed effects is controlling for the uniqueness of that observation (person, organization, firm, etc.). But can we still make causal claims? It depends but usually FE alone will not yield the causal effect. Fixed effects may help us considerably, but once again, it depends on our ability to argue that we have satisfied the conditional independence assumption (CIA). On one hand we have account for unobserved variables, but we still need to argue that we have no selection bias. Further, what about our ability to condition on variables that do change over time? The treatment can still be endogenous. Fixed effects can also amplify bias. Since we switch to within estimation, we lose a lot of variation. In doing so, just a little bit of measurement error can amplify our bias. Just remember that we are trying to get to that RCT standard. Is conditioning on all these variables get us closer to arguing that our treatment is as if random? All this does is just make our estimate more robust to unobserved differences. <strong>That is it.</strong> You still must confront all other possible issues to get closer to estimating a causal effect.</p>
<section id="example-of-fixed-effects-not-being-a-causal-estimate" class="level3">
<h3 class="anchored" data-anchor-id="example-of-fixed-effects-not-being-a-causal-estimate">Example of fixed effects not being a causal estimate</h3>
<p><em>Example taken directly from Brian:</em></p>
<p>As another example, there is a persistent finding in the labor literature documenting a marriage premium for men: men who are married earn substantially more than observationally similar single men. Clearly one question that arises is whether this relationship is causal or simply the result of OVB, where men who are successful in convincing someone to marry them have diﬀerent unobservable characteristics than do single men. If these same unobservables aﬀect earnings, then we have a classic OVB problem. Well, someone realized that we could use panel data (like the NLSY or the PSID) to run a fixed eﬀects model. In general, the marriage premium “survives” fixed eﬀects. But, this is actually not all that unexpected. The fixed eﬀects model simply asks if men get paid more when they finally have their act together and have grown up enough to get married. The FE model, therefore is not really any better. If you think carefully about this question, you will find that frequently people have eliminated one kind of OVB only to replace it with another.</p>
<p>In the marriage premium example, the FE estimate captures “earnings change coinciding with marriage,” but not necessarily “earnings change caused by marriage.” Time-varying confounders (age, career stage, skill growth) remain, so the causal effect is <strong>not identified</strong>.</p>
</section>
<section id="clustered-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="clustered-standard-errors">Clustered Standard Errors</h3>
<p>I will focus again on clustered standard errors elsewhere; however, I want to bring it up here as it is relevant to panel data. Before I explain what they are, let’s first focus on why we might need them.</p>
<section id="why-do-we-need-clustered-standard-errors" class="level4">
<h4 class="anchored" data-anchor-id="why-do-we-need-clustered-standard-errors">Why do we need clustered standard errors?</h4>
<p>Recall standard errors are derived from our errors. In OLS, we make assumptions about the distribution of those errors. Specifically, that the errors are homoskedastic (constant error variance). When we use panel data, we are observing individuals at different time periods. By construction, the errors for each individual in time <span class="math inline">\(t\)</span> will be correlated with observations of that same individual at different periods of <span class="math inline">\(t\)</span>. This violates our assumption that errors are homoskedastic and independent of each other. We have autocorrelation in our errors. If we do not cluster our standard errors, our standard errors will be too small, leading us to improper inferences.</p>
</section>
<section id="what-do-clustered-standard-errors-do" class="level4">
<h4 class="anchored" data-anchor-id="what-do-clustered-standard-errors-do">What do clustered standard errors do?</h4>
<p>Intuitively clustered standard errors are going to fix our non-independent residual issue.</p>
</section>
</section>
</section>
<section id="random-effects" class="level2">
<h2 class="anchored" data-anchor-id="random-effects">Random Effects</h2>
<p>For some seemingly unknown reason, you will hear a lot of discussion about random effects in conjunction with fixed effects. They are separate models. Political scientists will claim that they like random effects for reasons described in this section, but then do not teach really anything about it or how to do it or what is going on. You usually just get taught fixed effects. I don’t understand why this occurs, but if you are like me you will find this extremely frustrating, as I now have to essentially teach myself what this is and how it differs. Even more annoyingly, random effects can mean different things, isn’t that swell!</p>
<p>Like with fixed effects, we are dealing with a problem of correlated data.</p>
<p>Random and fixed effects are part of models known as “mixed” or “hierarchical” models.</p>
<p>When we don’t allow the slopes to vary within cluster, that is fixed effect. Fixed meaning everyone’s parameter is the same. SLOPES STAY THE SAME ACROSS ALL CLUSTERS!</p>
<p>The ones that are free to vary are random effects. Random = unique to each individual cluster.</p>
<p>Are the slopes going to be consistent across clusters? if so, then fixed effects. If not, if we think there is some reason they wont be consistent, then random effects</p>
<p>we almost never do fixed intercepts.</p>
</section>
</section>
<section id="section-x-standard-errors-and-their-fixes" class="level1">
<h1>Section X: Standard Errors and their Fixes</h1>
<p>Why should we care about our standard errors? Standard errors are critical for how we make inference. In simple terms, if our standard errors are too small, it may lead us to incorrectly reject the null hypothesis. When we run our models, we make assumptions about the errors. The two critical assumptions we make are that our error does not suffer from any spherical errors: no autocorrelation and no heteroskedasticity. As we will see, those assumptions are often violated, and we need to know how to deal with those violations so that we can more accurately derive our standard errors and make correct inferences from them.</p>
<section id="back-to-basics-what-is-standard-error" class="level2">
<h2 class="anchored" data-anchor-id="back-to-basics-what-is-standard-error">Back to Basics: What is standard error?</h2>
<p>Standard error is the standard deviation of the means. It quantifies the variation in the means from multiple sets of measurements. In it’s most basic form, it is a <strong>measure of uncertainty.</strong> What gets often confused is that the standard error can be estimated from a SINGLE set of measurements, even though it describes the means from multiple sets. Thus, even if you only have a single set of measurements, you are often given the option to plot the standard error. To sharpen our intuition, let’s start with the our formula for standard deviation:</p>
<p><span class="math display">\[
\sigma = \sqrt{\frac{\sum_{i=1}^{N}(x_i - \mu)^2}{N}}
\]</span></p>
<p><span class="math inline">\(x_i\)</span> is the value for each observation</p>
<p><span class="math inline">\(\mu\)</span> is sample mean</p>
<p><span class="math inline">\(N\)</span> is the number of observations</p>
<p><span class="math inline">\((x_i-\mu)^2\)</span> is just how much different each observation is from the mean, squared. Now our <strong>standard error</strong> formula is simply:</p>
<p><span class="math display">\[
SE = \frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>Standard error is the standard deviation divided by the square root of the number of observations we have in our sample for a sample mean. This relates to one mean, we want standard deviations for our <span class="math inline">\(\hat{\beta}\)</span>. This will be achieved by</p>
</section>
<section id="why-should-we-care-about-our-standard-errors" class="level2">
<h2 class="anchored" data-anchor-id="why-should-we-care-about-our-standard-errors">Why should we care about our standard errors?</h2>
<p>Standard errors are a measure of uncertainty. If we get them wrong, we will make incorrect inferences. This can lead to different conclusions/answer that may have real world policy implications. It may not seem like much but if your job is to predict something like cancer, you are going to want to make sure you are precise. For our purposes, the importance of standard errors manifest when we calculate our t-score (any test statistic). This is because our t-score is simply the <span class="math inline">\(\beta\)</span> divided by our standard error:</p>
<p><span class="math display">\[
t=\frac{\beta}{SE}
\]</span></p>
<p>And of course, the t-score tells us where our <span class="math inline">\(\hat{\beta}\)</span> is on our t-distribution. That value translates into p-values and how we determine statistical significance. We want our estimates to be efficient. Mathematically, this means we want our standard errors to be <em>small</em>. The smaller our standard errors are, the more precise we can be about coefficient.</p>
</section>
<section id="variance-co-variance-matrix-vcv" class="level2">
<h2 class="anchored" data-anchor-id="variance-co-variance-matrix-vcv">Variance Co-Variance Matrix (VCV)</h2>
<p>Since we are usually using a lot of data, we employ matrix algebra to estimate our parameters and standard errors. When we use OLS, we are estimating the <span class="math inline">\(\hat{\beta}\)</span>. The <span class="math inline">\(\hat{\beta}\)</span> is comes from our sample, which of course is a random draw from the population. But we don’t know how “weird” our draw of <span class="math inline">\(\hat{\beta}\)</span> we observed is. There is uncertainty around <span class="math inline">\(\hat{\beta}\)</span>, we want to know the variance of our estimate, that is, how much would our estimate vary across repeated samples.</p>
<p>When we estimate our standard errors using OLS, we estimate them using the following VCV formula:</p>
<p><span class="math display">\[
\sigma^2(\textbf{X'X})^{-1}
\]</span></p>
<p>Let’s break this formula down in plain English: The variance of <span class="math inline">\(\hat{\beta}\)</span> given <span class="math inline">\(X\)</span> is equal to the variance of <span class="math inline">\(\hat{\beta}\)</span> minus <span class="math inline">\(\beta\)</span> given <span class="math inline">\(X\)</span>.</p>
<p><span class="math inline">\(\sigma^2\)</span> is the variance of our errors TK</p>
<p><span class="math inline">\((\textbf{X'X})^{-1}\)</span> TK</p>
<p>The <strong>standard error (SE) of a coefficient</strong> measures <strong>how much the estimated coefficient</strong> <span class="math inline">\(\hat{\beta}\)</span> would vary if we repeated the experiment/sample many times. We want SE small because = more precise.</p>
<section id="vcv-example" class="level3">
<h3 class="anchored" data-anchor-id="vcv-example">VCV Example</h3>
<p>NOTE: this is just for your intuition, your statistical software is doing all of this under the hood, with a lot more numbers.</p>
<p>Let’s imagine we have some X variable and some outcome variable, y. (column 1 in X is our intercept)</p>
<p><span class="math display">\[
X = \begin{bmatrix}1 &amp; 1 \\1 &amp;2\\1 &amp; 3\end{bmatrix}, \ y = \begin{bmatrix}2 \\3\\5 \end{bmatrix}
\]</span></p>
<p>This is our data. Let’s use the OLS formula to estimate our <span class="math inline">\(\beta\)</span>’s. <span class="math inline">\(\hat{\beta} = (\textbf{X'X})^{-1}\textbf{X'}y\)</span></p>
<p>Step 1:</p>
<p><span class="math display">\[
X'X = \begin{bmatrix}1 &amp; 1 &amp; 1 \\1 &amp; 2 &amp; 3\end{bmatrix}\begin{bmatrix}1 &amp; 1 \\1 &amp; 2 \\1 &amp; 3\end{bmatrix}=\begin{bmatrix}3 &amp; 6 \\6 &amp; 14\end{bmatrix}
\]</span></p>
<p>Step 2: take the inverse of matrix we calculated.</p>
<p><span class="math display">\[
(X'X)^{-1} = \frac{1}{\det(X'X)} \begin{bmatrix}14 &amp; -6 \\-6 &amp; 3\end{bmatrix} = \frac{1}{3 \cdot 14 - 6 \cdot 6} \begin{bmatrix}14 &amp; -6 \\-6 &amp; 3\end{bmatrix} =\begin{bmatrix}2.3333 &amp; -1 \\-1 &amp; 0.5\end{bmatrix}
\]</span></p>
<p>Step 3:</p>
<p><span class="math display">\[
X'y =\begin{bmatrix}1 &amp; 1 &amp; 1 \\1 &amp; 2 &amp; 3\end{bmatrix}\begin{bmatrix}2 \\ 3 \\ 5\end{bmatrix}=\begin{bmatrix}1*2 + 1*3 + 1*5 \\1*2 + 2*3 + 3*5\end{bmatrix}=\begin{bmatrix}10 \\ 23\end{bmatrix}
\]</span></p>
<p>Step 4:</p>
<p><span class="math display">\[
\begin{bmatrix}2.3333 &amp; -1 \\-1 &amp; 0.5\end{bmatrix}\begin{bmatrix}10 \\ 23\end{bmatrix}=\begin{bmatrix}0.333 \\ 1.5\end{bmatrix} \\[1mm]
\]</span></p>
<p>These are our coefficients. Great, we just did OLS. But now we need our standard errors. <strong>This is how we get the VCV:</strong></p>
<p>Step 1: Calculate the residual - how much our y observations are different from our predicted value!</p>
<p><span class="math display">\[
\begin{bmatrix}2 \\ 3 \\ 5\end{bmatrix}-\begin{bmatrix}1 &amp; 1 \\1 &amp; 2 \\1 &amp; 3\end{bmatrix}\begin{bmatrix}0.333 \\ 1.5\end{bmatrix}=\begin{bmatrix}0.167 \\ -0.333 \\ 0.167\end{bmatrix}
\]</span></p>
<p>Step 2: VCV</p>
<p><span class="math display">\[
\hat{\epsilon} = \begin{bmatrix} 0.167 \\ -0.333 \\ 0.167 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\hat{\sigma}^2= \frac{\sum \hat{\varepsilon}_i^2}{n-k} = 0.1668
\]</span></p>
<p>This step above, we take each residual and square it. Then add them together. Then subtract the number of parameters we estimate (2 in our case) from the number of residuals (observations). Finally, divide the two and we get our value. <strong>.1668 is the variation, which we use to calculate the standard errors!</strong></p>
<p><span class="math display">\[
\text{Var}(\hat{\beta})= \hat{\sigma}^2 (X'X)^{-1} = 0.1668 \begin{bmatrix} 2.3333 &amp; -1 \\ -1 &amp; 0.5 \end{bmatrix} = \begin{bmatrix} 0.389 &amp; -0.167 \\ -0.167 &amp; 0.083 \end{bmatrix}
\]</span></p>
<p>To get the variance on our <span class="math inline">\(\beta\)</span> we simply take the square root of the main diagonal:</p>
<p><span class="math display">\[
\text{SE}(\hat{\beta}_0) = \sqrt{0.389} \approx 0.624, \quad\text{SE}(\hat{\beta}_1) = \sqrt{0.083} \approx 0.288
\]</span></p>
</section>
<section id="how-do-we-get-standard-errors-using-maximum-likelihood-estimation-mle." class="level3">
<h3 class="anchored" data-anchor-id="how-do-we-get-standard-errors-using-maximum-likelihood-estimation-mle.">How do we get standard errors using Maximum Likelihood Estimation (MLE).</h3>
<p>In MLE, we are maximizing the the likelihood, which will tell us the optimal parameter values that explain the data generating process. When we are estimating the MLE, we are using different algorithms to help us find where the derivative is zero. Think of hills, we are trying to find the top of the hill! When you are at the top of the hill, your gradient (derivative) is zero. Great, we have identified the coefficients with this, but now we need our standard errors (our measure of uncertainty). To do this, we will measure how steep the hill is around the top. The steepness/curvature of the function tells us how certain/uncertain we are.</p>
<p>Thus, we need to be able to find some measure of the steepness to give us our variance (and thus, standard errors). The standard errors of ML estimates are derived from the negative of the inverse of the second derivative. We take the negative, since variance is always positive. We take the inverse since larger (more negative) values indicate sharper curvature, which indicates more certainty in our estimate.</p>
<p>The calculus intuition behind the first derivative – which is what we are setting to zero to find the maximum (where the slope =0). This tells us which direction to move. The second derivative, tells us how fast the slope changes. It is a measure of how sharp or flat the peak is. When we take the second derivative, this will give us the Hessian matrix. <strong>The Hessian Matrix</strong> is just a table of second derivatives, one row and column per parameter. The standard errors are the square root of the main diagonal in the Hessian. Your computer is doing all of this for you.</p>
</section>
</section>
<section id="what-can-influence-our-standard-errors" class="level2">
<h2 class="anchored" data-anchor-id="what-can-influence-our-standard-errors">What can influence our standard errors?</h2>
<p>Whatever influences our variance, will influence our standard errors! Let’s work through some possible example that <strong>may or may not</strong> influence our standard errors. Note the sections below are an exercise.</p>
<section id="does-including-lots-of-control-variables-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-including-lots-of-control-variables-influence-our-standard-errors">Does including lots of control variables influence our standard errors?</h3>
<p>Yes. The effect of this depends on a few factors.</p>
<p>Assuming we have added defensible control variables.</p>
</section>
<section id="does-transforming-a-nonlinear-relationship-by-logging-x-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-transforming-a-nonlinear-relationship-by-logging-x-influence-our-standard-errors">Does transforming a nonlinear relationship by logging x influence our standard errors?</h3>
</section>
<section id="does-positive-spatial-autocorrelation-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-positive-spatial-autocorrelation-influence-our-standard-errors">Does positive spatial autocorrelation influence our standard errors?</h3>
<p>Yes. Regardless of what type it is, the presence of autocorrelation will influence our standard errors.</p>
</section>
<section id="does-the-error-term-not-being-normally-distributed-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-the-error-term-not-being-normally-distributed-influence-our-standard-errors">Does the error term not being normally distributed influence our standard errors?</h3>
<p>Yes.</p>
</section>
<section id="does-measurement-error-associated-with-the-dependent-variable-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-measurement-error-associated-with-the-dependent-variable-influence-our-standard-errors">Does measurement error associated with the dependent variable influence our standard errors?</h3>
<p>Yes. TK. Make a note about difference between systematic and non-systematic measurement error in DV</p>
</section>
<section id="does-a-predictor-being-correlated-with-the-error-term-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-a-predictor-being-correlated-with-the-error-term-influence-our-standard-errors">Does a predictor being correlated with the error term influence our standard errors?</h3>
<p>Yes.</p>
</section>
<section id="does-modeling-country-differences-through-the-inclusion-of-fixed-effects-influence-our-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="does-modeling-country-differences-through-the-inclusion-of-fixed-effects-influence-our-standard-errors">Does modeling country differences through the inclusion of fixed effects influence our standard errors?</h3>
<p>Yes because we are going to lose a lot more variation since we are comparing within countries. This is because we are removing between country variation. Hence, why fixed effects is sometimes referred to the within estimator. As such, when we lose variation, our standard errors will increase.</p>
<p>Note that while not directly asked about it, we usually incorporate clustered standard errors in fixed effects models. This is to correct the standard errors as we have not properly estimated that our errors are correlated in our model. Thus without CSE, our errors are pooled together, resulting in smaller standard errors.</p>
</section>
</section>
<section id="the-omega-matrix" class="level2">
<h2 class="anchored" data-anchor-id="the-omega-matrix">The Omega Matrix</h2>
<p>Confusingly, there is the VCV of our independent variables and a VCV of our errors. For simplicity, the VCV of our errors is called the Omega Matrix.</p>
<p><span class="math display">\[
\Omega \equiv E[\epsilon \epsilon']
\]</span></p>
<p>This is just the residuals of each observation into one vector and multiplying it by its transpose.</p>
</section>
<section id="the-sandwich-estimator" class="level2">
<h2 class="anchored" data-anchor-id="the-sandwich-estimator">The Sandwich Estimator</h2>
<p>To understand standard errors and their fixes, it is imperative that you understand the sandwich estimator. In fact, we have been using the sandwich estimator this whole time to derive our standard errors already. In classical OLS, we make assumptions about our residuals, which influence how we model the sandwich estimator. Let’s walk through the math a bit and break this down.</p>
<p>The sandwich estimator:</p>
<p><span class="math display">\[
Var(\hat{\beta}) = (X'X)^{-1}X'\Omega X(X'X)^{-1}
\]</span></p>
<p>With:</p>
<p><span class="math display">\[
\Omega \equiv E[\epsilon \epsilon']
\]</span></p>
<p>In classical OLS, we make assumptions about the <span class="math inline">\(\Omega\)</span>. Specifically, we say that there is no autocorrelation or heteroskedasticity. This assumption, mathematically, ends up looking like the following:</p>
<p><span class="math display">\[
\Omega = \sigma^2I_n
\]</span></p>
<p>The <span class="math inline">\(I_n\)</span> is an identity matrix. The <span class="math inline">\(\sigma^2\)</span> is the variance of the errors. <em>Which is common across all observations.</em> <span class="math inline">\(\sigma^2\)</span> is a scalar. This is because we assume no heteroskedasticity. All errors have the same variance. <strong>The identity matrix embodies our spherical assumptions.</strong> Let’s say our <span class="math inline">\(\sigma^2\)</span> is 2. What this will do is the following:</p>
<p><span class="math display">\[
\sigma^2I_3=2\times\begin{bmatrix}1 &amp;0 &amp; 0 \\0 &amp;1 &amp;0\\0 &amp; 1 &amp; 0\end{bmatrix}= \begin{bmatrix}2 &amp;0 &amp; 0 \\0 &amp;2 &amp;0\\0 &amp; 2 &amp; 0\end{bmatrix}
\]</span></p>
<p>Long story short, this really doesn’t change anything! This ends up reducing down to the classical OLS variance estimator:</p>
<p><span class="math display">\[
\sigma^2_{\epsilon}(XX')^{-1}
\]</span></p>
<p>In our running example, this would just be:</p>
<p><span class="math display">\[
2(XX')^{-1}
\]</span></p>
<p>2 is our variance and is constant because of assumed homoskedasticity! When we multiply this out, this will give us the standard errors along the main diagonal (when we take the square root). Note that this is with us assuming no spherical errors, but of course, this isn’t always true, and if we don’t account for when these spherical errors may be violated, our errors are like wrong and will lead to type 1 &amp; 2 errors. No bueno! In the sections that follow, we will look at what happens when we relax these assumptions. As you can imagine, these assumptions will relate back to what is going on with the Omega matrix and thus, we will return to where we started, which is the sandwich estimator!</p>
</section>
<section id="robust-standard-errors" class="level2">
<h2 class="anchored" data-anchor-id="robust-standard-errors">Robust Standard Errors</h2>
<p>What are robust standard errors and why do we need them? We use robust standard errors when we believe our errors are heteroskedastic. Recall, that our traditional OLS model is assuming homoskedasticity and no autocorrelation. Specifically, we will use robust standard errors when we have heterosekdasticity. Thus:</p>
<p><span class="math display">\[
\Omega \neq \sigma^2I_n
\]</span></p>
<p>This is because the variance is not constant and thus no longer a scalar. When we have heteroskedasticity, we have no constant error variance; the error differs across observations. Thus, we need to account for this! We will allow for each residual to have its own variance.</p>
<p><span class="math display">\[
\Omega = diag(\hat{\epsilon}^2_1,\hat{\epsilon}^2_2,...,\hat{\epsilon}^2_n)
\]</span></p>
<p>Here, each residual’s squared value is treated as an estimate of its variance. Thus robust standard errors will use the sandwich estimator with this in mind and provide us the ‘correct’ standard errors. You’ve probably already been told that assumed homoskedasticity is an assumption that is often violated to a degree, so why not always use robust standard errors? Well, this depends who you ask. Economist run it all the time on basically all of their models. They don’t care. Political scientists however, argue that robust standard errors might just be masking a specification problem. This is the argument of Gary King. In short, King’s argument is that if your robust standard error and your regular standard error don’t differ much, than you are golden. Where does robust standard errors pop up in political science? Usually you will see them in experiments. When you run an experiment, always add robust standard errors.</p>
<p><em>Note: Robust SE’s have other names, including:</em></p>
<section id="the-math-of-robust-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="the-math-of-robust-standard-errors">The Math of Robust Standard Errors</h3>
<p>We hinted above what is going on when we have heteroskedasticity in our residuals, thus violating our spherical assumption and leading to incorrect estimates of our standard errors. There are other ways to parametrically fix this (GLS or FGLS). However, it seems no one really does this and everyone instead estimates the OLS coefficients and then corrects the standard errors. Thus, we focus on robust standard errors when we have heteroskedasticity in our model. Returning back to our sandwich estimator:</p>
<p><span class="math display">\[
Var(\hat{\beta}) = (X'X)^{-1}X'\Omega X(X'X)^{-1}
\]</span></p>
<p>Where:</p>
<p><span class="math display">\[
\Omega = diag(\hat{\epsilon}^2_1,\hat{\epsilon}^2_2,...,\hat{\epsilon}^2_n)
\]</span></p>
<p>The variance is not a constant, we allow each observation to have its own variance, represented by the <span class="math inline">\(\epsilon^2\)</span>. <strong>Everything remains the same except instead of multiplying by a scalar and the identity matrix, we are using the sandwich estimator with the omega matrix being a matrix of the errors squared across the main diagonal.</strong> Thus:</p>
<p><span class="math display">\[
X'\Omega X = \Sigma \hat{\epsilon}^2_iX_iX'_i
\]</span></p>
<ul>
<li><p>Each term <span class="math inline">\(X_iX'_i\)</span> represents how observation <span class="math inline">\(i\)</span> contributes to the coefficient estimates.</p></li>
<li><p>Multiplying by <span class="math inline">\(\hat{\epsilon}^2_i\)</span> means observations with bigger residuals contribute more to the variance of <span class="math inline">\(\hat{\beta}\)</span>.</p></li>
<li><p>Conversely, observations with tiny residuals contribute less.</p></li>
</ul>
<p>So it's a <strong>weighted sum</strong>, where the "weights" are the squared residuals — intuitively, observations that are more "noisy" increase uncertainty in the coefficient estimates. Classical OLS assumes all residuals have the same variance → every observation contributes equally. Heteroskedasticity breaks that assumption, thus some observations are noisier than others. If an observation has a huge residual, it can affect the slope a lot; we should inflate the variance estimate to reflect that, and vice versa if the observation has a tiny residual.</p>
<p><em>Note: Robust standard errors maintain 0 values on the off-diagonal, thus no autocorrelation is still an assumption we are making.</em></p>
</section>
<section id="the-code-for-robust-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="the-code-for-robust-standard-errors">The code for robust standard errors</h3>
<p>TK</p>
</section>
</section>
<section id="clustered-standard-errors-1" class="level2">
<h2 class="anchored" data-anchor-id="clustered-standard-errors-1">Clustered Standard Errors</h2>
<p>Clustered standard errors are related to the Moulton problem. Long story short, this is a problem when our errors are often severely underestimated (biased downward) in grouped data. This is because in grouped data – think students in schools – are correlated with each other and <em>we have not accounted for that.</em> In essence, we are saying that our errors are <strong>not</strong> independent. This is a different issue than robust standard errors.</p>
<section id="the-math-of-clustered-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="the-math-of-clustered-standard-errors">The Math of Clustered Standard Errors</h3>
<p>To understand the intuition of clustered standard errors, let’s recall the variance of the OLS coefficient:</p>
<p><span class="math display">\[
Var(\hat{\beta}) = (X'X)^{-1}X'\Omega X(X'X)^{-1}
\]</span></p>
<p>With:</p>
<p><span class="math display">\[
\Omega \equiv E[\epsilon \epsilon']
\]</span></p>
<p>Aha! We see the return of our Omega matrix. Now, we assume that Omega matrix does not have any autocorrelation or heteroskedasticity! When we impose this assumption, the true variance is <span class="math inline">\(\sigma^2_{\epsilon}(XX')^{-1}\)</span>. We estimate it with sample equivalents. When we cluster, we allow for off-diagonal elements of the <span class="math inline">\(\Omega\)</span> to be non-zero within the specified group (G), but requires them to be zero for observations in different groups. We re-estimate the standard errors with the sandwich estimator. Imagine your big matrix of errors, what clustered SE’s are doing, is they are drawing boxes around certain parts of the matrix. These ‘boxes’ are the specified groups. They are all the students in one school and not the other school. Basically, we are telling the matrix <em>which errors are correlated with each other.</em></p>
<p>Need to find the math for this, but I shit you not all you do i just tell R or Stata what your clustering on.</p>
</section>
<section id="the-code-for-clustered-standard-errors" class="level3">
<h3 class="anchored" data-anchor-id="the-code-for-clustered-standard-errors">The code for clustered standard errors</h3>
<p>TK</p>
</section>
</section>
</section>
<section id="section-x-various-diagnostic-tests-and-other-important-considerations" class="level1">
<h1>Section X: Various Diagnostic Tests and Other Important Considerations</h1>
<section id="checking-for-multicollinearity" class="level2">
<h2 class="anchored" data-anchor-id="checking-for-multicollinearity">Checking for Multicollinearity</h2>
<p>Why might we care about multicollinearity? In many cases, we don’t. It really is only a problem in some cases but is mostly overblown. However, it can influence our standard errors. <strong>It leads to inefficiency.</strong> If we are, for whatever reason, concerned about variance, then we can use the variance inflation factor (VIF). Recall multillinarity is not a statistical problem, it is a data problem. It goes away as <span class="math inline">\(N -&gt; \infty\)</span>. OLS remains BLUE in presence of multicollinearity. <strong>Solution to multicollinearity:</strong> Increase your N; Know your data; present the multicollinearity; live with it.</p>
</section>
<section id="checking-for-heteroskedasticity" class="level2">
<h2 class="anchored" data-anchor-id="checking-for-heteroskedasticity">Checking for Heteroskedasticity</h2>
<section id="breush-pagan-godfrey-test" class="level3">
<h3 class="anchored" data-anchor-id="breush-pagan-godfrey-test">Breush-Pagan-Godfrey Test</h3>
</section>
<section id="whites-general-heteroscedasticity" class="level3">
<h3 class="anchored" data-anchor-id="whites-general-heteroscedasticity">White’s General Heteroscedasticity</h3>
</section>
</section>
<section id="checking-for-autocorrelation" class="level2">
<h2 class="anchored" data-anchor-id="checking-for-autocorrelation">Checking for Autocorrelation</h2>
<section id="durbin-watson-test" class="level3">
<h3 class="anchored" data-anchor-id="durbin-watson-test">Durbin-Watson Test</h3>
<p>Durbin-Watson test, the <span class="math inline">\(d\)</span> statistic. Basically, if the <span class="math inline">\(d\)</span> is equal to 2, we have no autocorrelation. If it is not equal then we do. However, what is the threshold? ? This is one of the downsides. Thus, if it strays only a little but from 2, we are probably fine, but if it is 1 or 3, then we are in trouble.</p>
</section>
<section id="breusch-godfrey-test" class="level3">
<h3 class="anchored" data-anchor-id="breusch-godfrey-test">Breusch-Godfrey Test</h3>
</section>
</section>
<section id="common-support" class="level2">
<h2 class="anchored" data-anchor-id="common-support">Common Support</h2>
<p>This primarily comes up in matching (which I did not cover). This is a data problem.</p>
</section>
</section>
<section id="section-x-miscellaneous-other-stuff-that-might-be-useful-to-know" class="level1">
<h1>Section X: Miscellaneous Other Stuff That Might Be Useful To Know</h1>
<section id="bootstrapping" class="level2">
<h2 class="anchored" data-anchor-id="bootstrapping">Bootstrapping</h2>
<p>We want use our sample to make inferences about the population. Bootstrapping is a way to generate an empirical distribution. This distribution has technically always been there, but it has been theoretical. That is we have our sample and it comes from some theoretical distribution of the samples and we use this to make inferences about the population. Bootstrapping is a method to actually generate that empirical distribution. In short, we take the information from our sample to generate multiple samples.</p>
<p>Sampling without replacement v. Sampling with replacement</p>
<p>Imagine you draw an M&amp;M, use it for your sample and then put back that M&amp;M back in the box, then we take another draw. Thus, we have the possibility to select the same M&amp;M again.</p>
<section id="why-use-a-bootstrap-sample" class="level3">
<h3 class="anchored" data-anchor-id="why-use-a-bootstrap-sample">Why use a bootstrap sample?</h3>
<p>It is a powerful way to figure out confidence intervals, when there is no theoretical distribution you can find.</p>
<p>You can always do bootstrapping</p>
<p>Biggest problem with bootstrapping, it is sample dependent. If your sample is weird – then bootstrapping your sample is also weird.</p>


</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{neilon2024,
  author = {Neilon, Stone},
  title = {Data {Comps} {Prep}},
  date = {2024-05-15},
  url = {https://stoneneilon.github.io/notes/Data_comps/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-neilon2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Neilon, Stone. 2024. <span>“Data Comps Prep.”</span> May 15, 2024. <a href="https://stoneneilon.github.io/notes/Data_comps/">https://stoneneilon.github.io/notes/Data_comps/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>