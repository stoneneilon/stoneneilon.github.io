<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stone Neilon">
<meta name="dcterms.date" content="2024-08-26">
<meta name="description" content="Text as Data with Alex Siegel">

<title>Stone Neilon - Text as Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stone Neilon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/file/d/1eZT7KaLsqdW3k4WxoeloMPm_9cdf-9A6/view?usp=sharing"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#week-1" id="toc-week-1" class="nav-link active" data-scroll-target="#week-1">Week 1</a>
  <ul class="collapse">
  <li><a href="#why-should-we-take-text-as-data" id="toc-why-should-we-take-text-as-data" class="nav-link" data-scroll-target="#why-should-we-take-text-as-data">Why should we take text as data?</a></li>
  <li><a href="#what-can-we-learn" id="toc-what-can-we-learn" class="nav-link" data-scroll-target="#what-can-we-learn">What can we learn?</a></li>
  </ul></li>
  <li><a href="#week-2-labor-day---no-class" id="toc-week-2-labor-day---no-class" class="nav-link" data-scroll-target="#week-2-labor-day---no-class">Week 2 (Labor Day - No Class)</a></li>
  <li><a href="#week-3---introduction-to-text-as-data" id="toc-week-3---introduction-to-text-as-data" class="nav-link" data-scroll-target="#week-3---introduction-to-text-as-data">Week 3 - Introduction to Text as Data:</a>
  <ul class="collapse">
  <li><a href="#lecture-notes" id="toc-lecture-notes" class="nav-link" data-scroll-target="#lecture-notes">Lecture Notes:</a>
  <ul class="collapse">
  <li><a href="#a-fundamentally-qualitative-exercise" id="toc-a-fundamentally-qualitative-exercise" class="nav-link" data-scroll-target="#a-fundamentally-qualitative-exercise">A fundamentally qualitative exercise:</a></li>
  <li><a href="#data-generating-process" id="toc-data-generating-process" class="nav-link" data-scroll-target="#data-generating-process">Data Generating Process:</a></li>
  <li><a href="#validation" id="toc-validation" class="nav-link" data-scroll-target="#validation">Validation:</a></li>
  <li><a href="#what-automated-approaches-can-and-cant-do" id="toc-what-automated-approaches-can-and-cant-do" class="nav-link" data-scroll-target="#what-automated-approaches-can-and-cant-do">What automated approaches can and can’t do:</a></li>
  <li><a href="#big-data-challenges" id="toc-big-data-challenges" class="nav-link" data-scroll-target="#big-data-challenges">Big Data Challenges:</a></li>
  <li><a href="#ethics" id="toc-ethics" class="nav-link" data-scroll-target="#ethics">Ethics:</a></li>
  <li><a href="#theory-build-or-theory-testing" id="toc-theory-build-or-theory-testing" class="nav-link" data-scroll-target="#theory-build-or-theory-testing">Theory Build or Theory Testing?</a></li>
  <li><a href="#how-do-i-collect-data" id="toc-how-do-i-collect-data" class="nav-link" data-scroll-target="#how-do-i-collect-data">How do I collect data?</a></li>
  </ul></li>
  <li><a href="#grimmer-justin-margaret-roberts-and-brandon-stewart.-text-as-data-chapters-1-2." id="toc-grimmer-justin-margaret-roberts-and-brandon-stewart.-text-as-data-chapters-1-2." class="nav-link" data-scroll-target="#grimmer-justin-margaret-roberts-and-brandon-stewart.-text-as-data-chapters-1-2.">Grimmer, Justin, Margaret Roberts, and Brandon Stewart. Text as Data, Chapters 1-2.</a>
  <ul class="collapse">
  <li><a href="#chapter-1" id="toc-chapter-1" class="nav-link" data-scroll-target="#chapter-1">Chapter 1:</a></li>
  <li><a href="#chapter-2" id="toc-chapter-2" class="nav-link" data-scroll-target="#chapter-2">Chapter 2:</a></li>
  </ul></li>
  <li><a href="#henry-e.-brady.-2019.-the-challenge-of-big-data-and-data-science.-annual-review-of-political-science." id="toc-henry-e.-brady.-2019.-the-challenge-of-big-data-and-data-science.-annual-review-of-political-science." class="nav-link" data-scroll-target="#henry-e.-brady.-2019.-the-challenge-of-big-data-and-data-science.-annual-review-of-political-science.">Henry E. Brady. 2019. “The Challenge of Big Data and Data Science.” Annual Review of Political Science.</a>
  <ul class="collapse">
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract:</a></li>
  <li><a href="#bumper-sticker" id="toc-bumper-sticker" class="nav-link" data-scroll-target="#bumper-sticker">Bumper Sticker:</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background:</a></li>
  </ul></li>
  <li><a href="#grimmer-justin-and-brandon-stewart.-2013.-text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-documents-political-analysis.-21-3-267-297." id="toc-grimmer-justin-and-brandon-stewart.-2013.-text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-documents-political-analysis.-21-3-267-297." class="nav-link" data-scroll-target="#grimmer-justin-and-brandon-stewart.-2013.-text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-documents-political-analysis.-21-3-267-297.">Grimmer, Justin and Brandon Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Documents” Political Analysis. 21, 3 267-297.</a>
  <ul class="collapse">
  <li><a href="#abstract-1" id="toc-abstract-1" class="nav-link" data-scroll-target="#abstract-1">Abstract:</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview:</a></li>
  <li><a href="#background-1" id="toc-background-1" class="nav-link" data-scroll-target="#background-1">Background:</a></li>
  <li><a href="#four-principles-of-automated-text-analysis" id="toc-four-principles-of-automated-text-analysis" class="nav-link" data-scroll-target="#four-principles-of-automated-text-analysis">Four Principles of Automated Text Analysis:</a></li>
  <li><a href="#supervised-method" id="toc-supervised-method" class="nav-link" data-scroll-target="#supervised-method">Supervised Method:</a></li>
  <li><a href="#fully-automated-clustering-fac" id="toc-fully-automated-clustering-fac" class="nav-link" data-scroll-target="#fully-automated-clustering-fac">Fully Automated Clustering (FAC):</a></li>
  <li><a href="#computer-assisted-clustering" id="toc-computer-assisted-clustering" class="nav-link" data-scroll-target="#computer-assisted-clustering">Computer Assisted Clustering</a></li>
  </ul></li>
  <li><a href="#michel-et-al.-2011-quantitative-analysis-of-culture-using-millions-of-digitized-books-science-3316014." id="toc-michel-et-al.-2011-quantitative-analysis-of-culture-using-millions-of-digitized-books-science-3316014." class="nav-link" data-scroll-target="#michel-et-al.-2011-quantitative-analysis-of-culture-using-millions-of-digitized-books-science-3316014.">Michel et al.&nbsp;2011, “Quantitative analysis of culture using millions of digitized books” Science, 331:6014.</a>
  <ul class="collapse">
  <li><a href="#abstract-2" id="toc-abstract-2" class="nav-link" data-scroll-target="#abstract-2">Abstract:</a></li>
  <li><a href="#bumper-sticker-1" id="toc-bumper-sticker-1" class="nav-link" data-scroll-target="#bumper-sticker-1">Bumper Sticker:</a></li>
  <li><a href="#research-question" id="toc-research-question" class="nav-link" data-scroll-target="#research-question">Research Question:</a></li>
  <li><a href="#datamethods" id="toc-datamethods" class="nav-link" data-scroll-target="#datamethods">Data/methods:</a></li>
  <li><a href="#findings" id="toc-findings" class="nav-link" data-scroll-target="#findings">Findings:</a></li>
  </ul></li>
  <li><a href="#dimaggio-paul.-adapting-computational-text-analysis-to-social-science-and-vice-versa.-big-data-society-2.2-2015." id="toc-dimaggio-paul.-adapting-computational-text-analysis-to-social-science-and-vice-versa.-big-data-society-2.2-2015." class="nav-link" data-scroll-target="#dimaggio-paul.-adapting-computational-text-analysis-to-social-science-and-vice-versa.-big-data-society-2.2-2015.">DiMaggio, Paul. “Adapting computational text analysis to social science (and vice versa).” Big Data &amp; Society 2.2 (2015).</a></li>
  </ul></li>
  <li><a href="#week-4-canceled---alex-out-sick" id="toc-week-4-canceled---alex-out-sick" class="nav-link" data-scroll-target="#week-4-canceled---alex-out-sick">Week 4 (Canceled - Alex out sick)</a></li>
  <li><a href="#week-5" id="toc-week-5" class="nav-link" data-scroll-target="#week-5">Week 5</a>
  <ul class="collapse">
  <li><a href="#lecture-notes-1" id="toc-lecture-notes-1" class="nav-link" data-scroll-target="#lecture-notes-1">Lecture Notes:</a></li>
  <li><a href="#grimmer-chp.-3-5." id="toc-grimmer-chp.-3-5." class="nav-link" data-scroll-target="#grimmer-chp.-3-5.">Grimmer, chp. 3-5.</a>
  <ul class="collapse">
  <li><a href="#chapter-3" id="toc-chapter-3" class="nav-link" data-scroll-target="#chapter-3">Chapter 3:</a></li>
  <li><a href="#chapter-4-selecting-documents" id="toc-chapter-4-selecting-documents" class="nav-link" data-scroll-target="#chapter-4-selecting-documents">Chapter 4: Selecting Documents</a></li>
  <li><a href="#chapter-5-bag-of-words" id="toc-chapter-5-bag-of-words" class="nav-link" data-scroll-target="#chapter-5-bag-of-words">Chapter 5: Bag of Words</a></li>
  </ul></li>
  <li><a href="#when-is-a-liability-not-a-liability-textual-analysis-dictionaries-and-10-ks" id="toc-when-is-a-liability-not-a-liability-textual-analysis-dictionaries-and-10-ks" class="nav-link" data-scroll-target="#when-is-a-liability-not-a-liability-textual-analysis-dictionaries-and-10-ks">When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks</a>
  <ul class="collapse">
  <li><a href="#abstract-3" id="toc-abstract-3" class="nav-link" data-scroll-target="#abstract-3">Abstract:</a></li>
  <li><a href="#question" id="toc-question" class="nav-link" data-scroll-target="#question">Question:</a></li>
  <li><a href="#bumper-sticker-2" id="toc-bumper-sticker-2" class="nav-link" data-scroll-target="#bumper-sticker-2">Bumper Sticker:</a></li>
  </ul></li>
  <li><a href="#the-psychological-meaning-of-words-liwc-and-computerized-text-analysis-methods" id="toc-the-psychological-meaning-of-words-liwc-and-computerized-text-analysis-methods" class="nav-link" data-scroll-target="#the-psychological-meaning-of-words-liwc-and-computerized-text-analysis-methods">The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods</a>
  <ul class="collapse">
  <li><a href="#abstract-4" id="toc-abstract-4" class="nav-link" data-scroll-target="#abstract-4">Abstract:</a></li>
  <li><a href="#background-2" id="toc-background-2" class="nav-link" data-scroll-target="#background-2">Background:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#week-6" id="toc-week-6" class="nav-link" data-scroll-target="#week-6">Week 6</a>
  <ul class="collapse">
  <li><a href="#dictionary-methods" id="toc-dictionary-methods" class="nav-link" data-scroll-target="#dictionary-methods">Dictionary Methods</a></li>
  <li><a href="#validation-1" id="toc-validation-1" class="nav-link" data-scroll-target="#validation-1">Validation</a></li>
  <li><a href="#zipfs-law" id="toc-zipfs-law" class="nav-link" data-scroll-target="#zipfs-law">Zipf’s Law</a></li>
  <li><a href="#wordscores" id="toc-wordscores" class="nav-link" data-scroll-target="#wordscores">Wordscores</a></li>
  </ul></li>
  <li><a href="#week-7---similarity-and-complex-measures" id="toc-week-7---similarity-and-complex-measures" class="nav-link" data-scroll-target="#week-7---similarity-and-complex-measures">Week 7 - Similarity and Complex measures</a>
  <ul class="collapse">
  <li><a href="#lecture-notes-2" id="toc-lecture-notes-2" class="nav-link" data-scroll-target="#lecture-notes-2">Lecture Notes:</a>
  <ul class="collapse">
  <li><a href="#cosine-similarity" id="toc-cosine-similarity" class="nav-link" data-scroll-target="#cosine-similarity">Cosine Similarity:</a></li>
  </ul></li>
  <li><a href="#weighting-and-goldilocks-words" id="toc-weighting-and-goldilocks-words" class="nav-link" data-scroll-target="#weighting-and-goldilocks-words">Weighting and “Goldilocks Words”</a></li>
  <li><a href="#textual-complexity" id="toc-textual-complexity" class="nav-link" data-scroll-target="#textual-complexity">Textual Complexity</a></li>
  </ul></li>
  <li><a href="#week-8---supervised-machine-learning" id="toc-week-8---supervised-machine-learning" class="nav-link" data-scroll-target="#week-8---supervised-machine-learning">Week 8 - Supervised Machine Learning</a>
  <ul class="collapse">
  <li><a href="#supervised-machine-learning" id="toc-supervised-machine-learning" class="nav-link" data-scroll-target="#supervised-machine-learning">Supervised Machine Learning:</a>
  <ul class="collapse">
  <li><a href="#classification-steps" id="toc-classification-steps" class="nav-link" data-scroll-target="#classification-steps">Classification Steps</a></li>
  <li><a href="#human-annotation" id="toc-human-annotation" class="nav-link" data-scroll-target="#human-annotation">Human Annotation</a></li>
  <li><a href="#types-of-supervised-models" id="toc-types-of-supervised-models" class="nav-link" data-scroll-target="#types-of-supervised-models">Types of supervised models:</a></li>
  <li><a href="#naive-bayes-classifiers" id="toc-naive-bayes-classifiers" class="nav-link" data-scroll-target="#naive-bayes-classifiers">Naive Bayes Classifiers:</a></li>
  <li><a href="#support-vector-machines" id="toc-support-vector-machines" class="nav-link" data-scroll-target="#support-vector-machines">Support Vector Machines:</a></li>
  <li><a href="#model-performance" id="toc-model-performance" class="nav-link" data-scroll-target="#model-performance">Model Performance</a></li>
  <li><a href="#validation-2" id="toc-validation-2" class="nav-link" data-scroll-target="#validation-2">Validation:</a></li>
  </ul></li>
  <li><a href="#week-9---unsupervised-machine-learning" id="toc-week-9---unsupervised-machine-learning" class="nav-link" data-scroll-target="#week-9---unsupervised-machine-learning">Week 9 - Unsupervised Machine Learning</a></li>
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering">Clustering</a>
  <ul class="collapse">
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering">K-Means Clustering</a></li>
  </ul></li>
  <li><a href="#topic-models" id="toc-topic-models" class="nav-link" data-scroll-target="#topic-models">Topic Models:</a>
  <ul class="collapse">
  <li><a href="#latent-dirichlet-allocation-lda" id="toc-latent-dirichlet-allocation-lda" class="nav-link" data-scroll-target="#latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Text as Data</h1>
  <div class="quarto-categories">
    <div class="quarto-category">2024</div>
    <div class="quarto-category">Fall</div>
    <div class="quarto-category">Methods</div>
  </div>
  </div>

<div>
  <div class="description">
    Text as Data with Alex Siegel
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://stoneneilon.github.io/">Stone Neilon</a> <a href="https://orcid.org/0009-0006-6026-4384" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.colorado.edu/polisci/people/graduate-students/stone-neilon">
            PhD student of political science @ The University of Colorado Boulder
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="week-1" class="level1">
<h1>Week 1</h1>
<p><a href="https://drive.google.com/file/d/13QYHeUt1mlQ4sIT_N1vfRtfh1ny9jAFF/view?usp=sharing">Click here for syllabus</a></p>
<section id="why-should-we-take-text-as-data" class="level2">
<h2 class="anchored" data-anchor-id="why-should-we-take-text-as-data">Why should we take text as data?</h2>
<ul>
<li><p>Availability!</p></li>
<li><p>Text and language are integral components of political science.</p></li>
<li><p>This field is HOT right now. Very accessible.</p></li>
<li><p>Opens up exciting new opportunities for measurement and inference.</p></li>
<li><p>a lot of text analysis is <em>discovery</em> - this can shape our question moving forward. The process is somewhat inductive.</p></li>
</ul>
</section>
<section id="what-can-we-learn" class="level2">
<h2 class="anchored" data-anchor-id="what-can-we-learn">What can we learn?</h2>
<ul>
<li><p>social media data gives us real-time measures of mass and elite behavior</p></li>
<li><p>Text of floor speeches, press releases, supreme court briefs, and manifestos offer new measures of ideological scaling.</p></li>
<li><p>Web-scraping repeatedly can show us how authoritarian regimes censor content and control information.</p></li>
<li><p>The language in books and magazines over time can show us how social attitudes have evolved.</p></li>
</ul>
</section>
</section>
<section id="week-2-labor-day---no-class" class="level1">
<h1>Week 2 (Labor Day - No Class)</h1>
</section>
<section id="week-3---introduction-to-text-as-data" class="level1">
<h1>Week 3 - Introduction to Text as Data:</h1>
<section id="lecture-notes" class="level2">
<h2 class="anchored" data-anchor-id="lecture-notes">Lecture Notes:</h2>
<section id="a-fundamentally-qualitative-exercise" class="level3">
<h3 class="anchored" data-anchor-id="a-fundamentally-qualitative-exercise">A fundamentally qualitative exercise:</h3>
<ul>
<li><p>substantive knowledge and deep contextual understanding is essential to determine relevant quantities of interest.</p></li>
<li><p>is the method appropriate?</p></li>
<li><p>is this thing I measured actually capturing what I want?</p></li>
<li><p>automated approach allows humans to read, organize, and analyze documents at scale.</p></li>
</ul>
</section>
<section id="data-generating-process" class="level3">
<h3 class="anchored" data-anchor-id="data-generating-process">Data Generating Process:</h3>
<ul>
<li><p>Agnostic about model selection</p></li>
<li><p>as long as we can accurately measure what we care about.</p></li>
<li><p>all text models are wrong!</p></li>
<li><p>evaluate models based on their ability to perform a useful social scientific task.</p></li>
</ul>
</section>
<section id="validation" class="level3">
<h3 class="anchored" data-anchor-id="validation">Validation:</h3>
<ul>
<li><p>Face validity?</p></li>
<li><p>Does the text measure correlate with similar measures?</p></li>
<li><p>How does the measure compare with our bets guess at “ground truth”?</p></li>
</ul>
</section>
<section id="what-automated-approaches-can-and-cant-do" class="level3">
<h3 class="anchored" data-anchor-id="what-automated-approaches-can-and-cant-do">What automated approaches can and can’t do:</h3>
<ul>
<li><p>Algorithms are really good at sorting.</p></li>
<li><p>good at compressing texts into lower dimensions.</p></li>
<li><p>With supervised tasks, if a human can’t do it a machine (usually) can’t either.</p></li>
</ul>
</section>
<section id="big-data-challenges" class="level3">
<h3 class="anchored" data-anchor-id="big-data-challenges">Big Data Challenges:</h3>
<ul>
<li><p>Just because data is really big, doesn’t mean it’s representative.</p></li>
<li><p>Just because we have statistical significance but it substantively isn’t meaningful.</p></li>
</ul>
</section>
<section id="ethics" class="level3">
<h3 class="anchored" data-anchor-id="ethics">Ethics:</h3>
<ul>
<li><p>Just because data is publicly available doesn’t mean you don’t need to protect subjects.</p></li>
<li><p>Prioritize subjects over reproducability</p></li>
<li><p>Following platforms’ terms of service is sometimes at odds with open science.</p></li>
<li><p>Do the best you can but don’t violate ToS if possible.</p></li>
</ul>
</section>
<section id="theory-build-or-theory-testing" class="level3">
<h3 class="anchored" data-anchor-id="theory-build-or-theory-testing">Theory Build or Theory Testing?</h3>
<ul>
<li><p>Theory building: develop a new argument or concept to explain something that is not understood well</p></li>
<li><p>theory testing: takes an existing theory and apply it to new cases or-evaluate existing cases etc. This can also result in the modification of an existing theory.</p></li>
</ul>
</section>
<section id="how-do-i-collect-data" class="level3">
<h3 class="anchored" data-anchor-id="how-do-i-collect-data">How do I collect data?</h3>
<ul>
<li><p>APIs</p></li>
<li><p>Webscraping</p></li>
<li><p>OCR</p></li>
</ul>
</section>
</section>
<section id="grimmer-justin-margaret-roberts-and-brandon-stewart.-text-as-data-chapters-1-2." class="level2">
<h2 class="anchored" data-anchor-id="grimmer-justin-margaret-roberts-and-brandon-stewart.-text-as-data-chapters-1-2."><a href="https://drive.google.com/file/d/1VDMUqd0UTSxeEBUCE1jY_m4D4TDe_vcI/view?usp=sharing">Grimmer, Justin, Margaret Roberts, and Brandon Stewart. Text as Data, Chapters 1-2.</a></h2>
<p>(Textbook)</p>
<section id="chapter-1" class="level3">
<h3 class="anchored" data-anchor-id="chapter-1">Chapter 1:</h3>
<ul>
<li><p>The book uses text as data methods from computer science for social science purposes.</p></li>
<li><p>Big use of inductive reasoning.</p></li>
<li><p>Rather than a focus on prediction, text as data researchers are much more interested in how well their models provide insights into concepts of interest, how well measurement tools sort documents according to those rules, and how well the assumptions needed for accurate causal inference or prediction are met.</p></li>
<li><p>Texts can be organized differently. No one correct organization</p>
<ul>
<li><p>we can then find the best measurement of that particular organization</p>
<ul>
<li><p>can be tested with extensive validation.</p>
<ul>
<li>what is the utility to answering the research question becomes of key interest.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Authors reject views that there is an underlying structure that statistical models applied to text are recovering. Rather, we view statistical models as useful (and incorrect) summaries of the text documents.</p></li>
</ul>
</section>
<section id="chapter-2" class="level3">
<h3 class="anchored" data-anchor-id="chapter-2">Chapter 2:</h3>
<ul>
<li><p>Typical social science research is deductive</p>
<ul>
<li><p>Theory &gt; data &gt; test</p></li>
<li><p>Empirical Implications of Theoretical Models</p>
<ul>
<li>use theory to create formal game model then use the game theory model to extract predictions. Then collect data to test those predictions.</li>
</ul></li>
<li><p>strict deductive research can lead to missed opportunities in new questions + strategies.</p></li>
</ul></li>
<li><p>Authors argue there is value in inductive reasoning and using an iterative approach for text analysis.</p></li>
<li><p>Methods to analyze text as data can contribute to inference at three stages of the process: discovery, measurement, and inference.</p></li>
</ul>
<section id="discovery" class="level4">
<h4 class="anchored" data-anchor-id="discovery">Discovery:</h4>
<ul>
<li><p>Discovery - develop a research question</p>
<ul>
<li><p>the task of deciding what you want to measure from data and goal of inference.</p>
<ul>
<li><p>need to conceptualize the world.</p></li>
<li><p>You want to analyze social media, okay, but what aspect?</p>
<ul>
<li><p>topical content, sentiment, readibility, etc.</p>
<ul>
<li>text analysis helps us discover what part we want to look at through the inductive process.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="inductive_deductive.png" class="img-fluid figure-img"></p>
<figcaption>Figure of deductive vs.&nbsp;inductive.</figcaption>
</figure>
</div>
<ul>
<li>By pointing out new ways out of organizing the document, we can read the text differently and prompt new questions.</li>
</ul>
</section>
<section id="measurement" class="level4">
<h4 class="anchored" data-anchor-id="measurement">Measurement:</h4>
<ul>
<li>researchers have to demonstrate that their method of measurement does indeed describe the concept or behavior they want to measure—that is, they have to validate their measures and provide evidence that the described quantities are relevant to the theoretical quantity at hand.</li>
</ul>
</section>
<section id="inference" class="level4">
<h4 class="anchored" data-anchor-id="inference">Inference:</h4>
<ul>
<li><p>predictions about events</p></li>
<li><p>used for causal inference.</p></li>
</ul>
</section>
<section id="agnostic-approach-to-text-analysis" class="level4">
<h4 class="anchored" data-anchor-id="agnostic-approach-to-text-analysis">Agnostic approach to Text Analysis:</h4>
<ul>
<li><p>No true underlying data-generating process</p>
<ul>
<li>just a bunch of different ones with different insights.</li>
</ul></li>
</ul>
</section>
<section id="six-principles-of-text-analysis" class="level4">
<h4 class="anchored" data-anchor-id="six-principles-of-text-analysis">Six Principles of Text Analysis:</h4>
<ol type="1">
<li><p>Theory and substantive knowledge are essential for research design (Section 2.7.1)</p></li>
<li><p>Text analysis does not replace humans—it augments them (Section 2.7.2)</p></li>
<li><p>Building, refining, and testing social science theories requires iteration and cumulation (Section 2.7.3)</p></li>
<li><p>Text analysis methods distill generalizations from language (Section 2.7.4)</p></li>
<li><p>The best method depends on the task (Section 2.7.5)</p></li>
<li><p>Validations are essential and depend on the theory and the task (Section 2.7.6)</p></li>
</ol>
</section>
</section>
</section>
<section id="henry-e.-brady.-2019.-the-challenge-of-big-data-and-data-science.-annual-review-of-political-science." class="level2">
<h2 class="anchored" data-anchor-id="henry-e.-brady.-2019.-the-challenge-of-big-data-and-data-science.-annual-review-of-political-science."><a href="https://drive.google.com/file/d/1pJ4kuJNkW5BHVAzLwWrg2zioOS8BM9kV/view?usp=sharing">Henry E. Brady. 2019. “The Challenge of Big Data and Data Science.” Annual Review of Political Science.</a></h2>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract:</h3>
<p>Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.</p>
</section>
<section id="bumper-sticker" class="level3">
<h3 class="anchored" data-anchor-id="bumper-sticker">Bumper Sticker:</h3>
<p>Big data changed poli sci a lot - and there is more work to be done!</p>
</section>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background">Background:</h3>
<ul>
<li><p>So much information causing overload -&gt; fragmentation of information flows.</p>
<ul>
<li>good material explanation of maybe why people can chose their preferred media source.</li>
</ul></li>
<li><p>Big data <span class="math inline">\(\neq\)</span> better inference.</p>
<ul>
<li>this is true if our data collection process is bad/false/problematic.</li>
</ul></li>
</ul>
</section>
</section>
<section id="grimmer-justin-and-brandon-stewart.-2013.-text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-documents-political-analysis.-21-3-267-297." class="level2">
<h2 class="anchored" data-anchor-id="grimmer-justin-and-brandon-stewart.-2013.-text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-documents-political-analysis.-21-3-267-297."><a href="https://drive.google.com/file/d/1_LzsCzik89-e6rlAR7vj7GFvEH3Dz-p4/view?usp=sharing">Grimmer, Justin and Brandon Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Documents” Political Analysis. 21, 3 267-297.</a></h2>
<section id="abstract-1" class="level3">
<h3 class="anchored" data-anchor-id="abstract-1">Abstract:</h3>
<p>Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.</p>
</section>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview:</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="grimmer_overview_text.png" class="img-fluid figure-img"></p>
<figcaption>Process of Text as Data</figcaption>
</figure>
</div>
</section>
<section id="background-1" class="level3">
<h3 class="anchored" data-anchor-id="background-1">Background:</h3>
<ul>
<li><p>Just because one text method works well on one data set does not mean it will work will on a different dataset.</p></li>
<li><p>Classification - organizes texts into a set of categories</p>
<ul>
<li><p>Dictionary method - use the frequency of key words to determine a document’s class</p></li>
<li><p>supervised methods: hand coding documents then giving it to the machine to train and classify.</p>
<ul>
<li>easier to validate</li>
</ul></li>
<li><p>Fully automated clustering (FAC) - estimate categories then classify those documents into categories.</p>
<ul>
<li><p>mixed membership models - problem-specific structure to assist in the estimation of categories</p></li>
<li><p>Computer assisted clustering (CAC) - looks at 1000s of potential clusterings.</p></li>
</ul></li>
<li><p>Unsupervised is not perfect and you need validation.</p></li>
</ul></li>
<li><p>Text/document - unit of anaylsis - tweet, facebook post, article, sentence, paragraph</p></li>
<li><p>Corpus - population of texts</p></li>
<li><p>Corpora - collection of corpus.</p></li>
<li><p>Order of words does not matter. We treat our documents as a bag of words.</p></li>
<li><p>stemming - reduces ends of words. This reduces dimensionality and keeps everything simpler.</p>
<ul>
<li><p>family, families, families’, familial</p>
<ul>
<li>all become famili</li>
</ul></li>
</ul></li>
<li><p>Typically also discard capitalization, punctuation, very common words, and very uncommon words.</p></li>
<li><p>We can used supervised methods to validate our use of unsupervised methods.</p></li>
</ul>
</section>
<section id="four-principles-of-automated-text-analysis" class="level3">
<h3 class="anchored" data-anchor-id="four-principles-of-automated-text-analysis">Four Principles of Automated Text Analysis:</h3>
<ol type="1">
<li><p>All quantitative models of language are wrong - but some are useful</p></li>
<li><p>Quantitative methods for text amplify resources and augment humans</p></li>
<li><p>There is no globally best method for automated text analysis</p></li>
<li><p>Validate, Validate, Validate.</p></li>
</ol>
</section>
<section id="supervised-method" class="level3">
<h3 class="anchored" data-anchor-id="supervised-method">Supervised Method:</h3>
<ol type="1">
<li><p>Construct a training set</p>
<ol type="1">
<li><p>iteratively develop coding scheme</p></li>
<li><p>randomly sample to obtain a representative sample of documents</p>
<ol type="1">
<li><p>use this to train data set.</p></li>
<li><p>n = 500 is ideal</p>
<ol type="1">
<li>n = 100 is just enough</li>
</ol></li>
</ol></li>
</ol></li>
<li><p>apply the supervised learning method-learning the relationship between features and categories in the training set, then using this to infer the labels in the test set</p></li>
<li><p>validate the model output and classify the remaining documents</p></li>
</ol>
</section>
<section id="fully-automated-clustering-fac" class="level3">
<h3 class="anchored" data-anchor-id="fully-automated-clustering-fac">Fully Automated Clustering (FAC):</h3>
<p>a supervised method we can use to validate unsupervised method</p>
<section id="single-membership-model" class="level4">
<h4 class="anchored" data-anchor-id="single-membership-model">Single Membership Model:</h4>
<ul>
<li><p>Single membership clustering models estimate a clustering: a partition of documents into</p>
<p>mutually exclusive and exhaustive groups.</p>
<ul>
<li>Each group of documents in a clustering is a cluster, which represents an estimate of a category.</li>
</ul></li>
</ul>
</section>
<section id="mixed-membership-model" class="level4">
<h4 class="anchored" data-anchor-id="mixed-membership-model">Mixed Membership Model</h4>
</section>
</section>
<section id="computer-assisted-clustering" class="level3">
<h3 class="anchored" data-anchor-id="computer-assisted-clustering">Computer Assisted Clustering</h3>
</section>
</section>
<section id="michel-et-al.-2011-quantitative-analysis-of-culture-using-millions-of-digitized-books-science-3316014." class="level2">
<h2 class="anchored" data-anchor-id="michel-et-al.-2011-quantitative-analysis-of-culture-using-millions-of-digitized-books-science-3316014."><a href="https://www.science.org/doi/full/10.1126/science.1199644?casa_token=KsF6n_P2T7wAAAAA%3AjpgR4IuoHzVlqji8a9iemdAeMjw0HkD0pbaRpl4rMJRuR-By9za3j_O86Ucf5A-y90JFt6pHL96-hwc">Michel et al.&nbsp;2011, “Quantitative analysis of culture using millions of digitized books” Science, 331:6014.</a></h2>
<section id="abstract-2" class="level3">
<h3 class="anchored" data-anchor-id="abstract-2">Abstract:</h3>
<p>We constructed a corpus of digitized texts containing about 4% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of ‘culturomics,’ focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.</p>
</section>
<section id="bumper-sticker-1" class="level3">
<h3 class="anchored" data-anchor-id="bumper-sticker-1">Bumper Sticker:</h3>
<p>Text as Data can help us figure out language/culture changes over time.</p>
</section>
<section id="research-question" class="level3">
<h3 class="anchored" data-anchor-id="research-question">Research Question:</h3>
<p>How has culture changed?</p>
</section>
<section id="datamethods" class="level3">
<h3 class="anchored" data-anchor-id="datamethods">Data/methods:</h3>
<ul>
<li><p>15 million books used for corpus</p></li>
<li><p>spans hundreds of years and languages.</p></li>
<li><p>restricted to 1-gram up to 5-gram - looked at how often a 1-gram or n-gram appeared</p>
<ul>
<li>1-gram is a sequence of characters uninterrupted by space.</li>
</ul></li>
<li><p>Usage frequency is computed by dividing the number of instances of the <em>n</em>-gram in a given year by the total number of words in the corpus in that year.</p></li>
</ul>
</section>
<section id="findings" class="level3">
<h3 class="anchored" data-anchor-id="findings">Findings:</h3>
<ul>
<li><p>More words found than in dictionary.</p></li>
<li><p>English lexicon has early doubled from 1950 to 2000.</p></li>
<li><p>grammar has changed</p></li>
</ul>
</section>
</section>
<section id="dimaggio-paul.-adapting-computational-text-analysis-to-social-science-and-vice-versa.-big-data-society-2.2-2015." class="level2">
<h2 class="anchored" data-anchor-id="dimaggio-paul.-adapting-computational-text-analysis-to-social-science-and-vice-versa.-big-data-society-2.2-2015."><a href="https://journals.sagepub.com/doi/full/10.1177/2053951715602908">DiMaggio, Paul. “Adapting computational text analysis to social science (and vice versa).” Big Data &amp; Society 2.2 (2015).</a></h2>
</section>
</section>
<section id="week-4-canceled---alex-out-sick" class="level1">
<h1>Week 4 (Canceled - Alex out sick)</h1>
</section>
<section id="week-5" class="level1">
<h1>Week 5</h1>
<section id="lecture-notes-1" class="level2">
<h2 class="anchored" data-anchor-id="lecture-notes-1">Lecture Notes:</h2>
<ul>
<li><p>Type: is a unique sequence of characters that are grouped together in some meaningful way.</p></li>
<li><p>Token: a particular instance of a type.</p>
<ul>
<li><p>tokenization is the process of splitting a document into its constituent words</p></li>
<li><p>tokens are the individual units we split our document into before counting them up.</p></li>
</ul></li>
<li><p>Term: is a type that is part of the system’s ‘dictionary’.</p></li>
<li><p>To do List:</p>
<ul>
<li><p>choose unit of analysis &amp; make corpus</p></li>
<li><p>tokenize</p></li>
<li><p>reduce complexity</p></li>
<li><p>filter by frequency.</p>
<ul>
<li>remove overly common and overly rare words.</li>
</ul></li>
</ul></li>
<li><p>A <em>lemma</em> is the canonical form (such as one might find in a dictionary) of a set of words that are related by inflection.</p></li>
<li><p>A document is a collection of <strong>W</strong> features tokens</p></li>
</ul>
</section>
<section id="grimmer-chp.-3-5." class="level2">
<h2 class="anchored" data-anchor-id="grimmer-chp.-3-5.">Grimmer, chp. 3-5.</h2>
<section id="chapter-3" class="level3">
<h3 class="anchored" data-anchor-id="chapter-3">Chapter 3:</h3>
<ul>
<li><p>We need to decide what information we want and what information we need discarded.</p>
<ul>
<li>figuring out what to discard and how is the hard part.</li>
</ul></li>
</ul>
<section id="principle-1" class="level4">
<h4 class="anchored" data-anchor-id="principle-1">Principle 1:</h4>
<ul>
<li><p>The usefulness of a corpus depends on the question the researcher wants to answer</p>
<p>and the population they want to study.</p>
<ul>
<li><p>figure out the quanitity and population of interest.</p>
<ul>
<li><p>twitter data is a subset of the population for example.</p>
<ul>
<li>might not be the best way to gauge public opinion.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="principle-2" class="level4">
<h4 class="anchored" data-anchor-id="principle-2">Principle 2:</h4>
<ul>
<li>There is no one right way to represent text for all research questions.</li>
</ul>
</section>
<section id="principle-3" class="level4">
<h4 class="anchored" data-anchor-id="principle-3">Principle 3:</h4>
<ul>
<li><p>The best assurance that a text representation is working is extensive validation.</p>
<ul>
<li><p>how do we know that we have selected the “right” representation for their research question?</p>
<ul>
<li><p>We will know that our representation is working in a measurement model if the</p>
<p>measures that we have created using that representation align with validated hand coded</p>
<p>data and facts that we know about the social world.</p></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="chapter-4-selecting-documents" class="level3">
<h3 class="anchored" data-anchor-id="chapter-4-selecting-documents">Chapter 4: Selecting Documents</h3>
<ul>
<li><p>Four types of Bias:</p>
<ul>
<li><p>Resource Bias</p></li>
<li><p>Incentive Bias</p></li>
<li><p>Medium Bias</p></li>
<li><p>Retrieval Bias</p></li>
</ul></li>
</ul>
</section>
<section id="chapter-5-bag-of-words" class="level3">
<h3 class="anchored" data-anchor-id="chapter-5-bag-of-words">Chapter 5: Bag of Words</h3>
<ul>
<li><p>We now need to represent our words numerically.</p></li>
<li><p>Bag of Words model:</p>
<ul>
<li><p>we will represent each document by counting how many time each word appears in it.</p></li>
<li><p>create a matrix</p></li>
<li><p>Full Recipe:</p>
<ul>
<li><p>Choose the Unit of Analysis</p></li>
<li><p>Tokenize</p>
<ul>
<li><p>Each individual word in the document is a token and the process of splitting a document into its constituent words is called tokenization.</p>
<ul>
<li>Tokens are the individual units we split our document into before counting them up.</li>
</ul></li>
</ul></li>
<li><p>Reduce Complexity - helps computationally</p>
<ul>
<li><p>lowercase</p></li>
<li><p>remove punctuation</p></li>
<li><p>Remove stop words</p></li>
<li><p>Create equivalence classes</p></li>
<li><p>filter by frequency</p></li>
</ul></li>
<li><p>Create the document feature of analysis.</p></li>
</ul></li>
</ul></li>
<li><p>N-grams</p>
<ul>
<li><p>related to tokenization</p>
<ul>
<li><p>basically we may have a word like “white house” BUT we pull out white <em>and</em> house.</p>
<ul>
<li><p>we use n-grams - an ordered set of <em>n</em> words.</p>
<ul>
<li><p><strong>unigrams</strong>- single words</p></li>
<li><p><strong>bigrams</strong>- ordered pairs</p></li>
<li><p><strong>trigrams</strong>- ordered triples.</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="when-is-a-liability-not-a-liability-textual-analysis-dictionaries-and-10-ks" class="level2">
<h2 class="anchored" data-anchor-id="when-is-a-liability-not-a-liability-textual-analysis-dictionaries-and-10-ks">When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks</h2>
<section id="abstract-3" class="level3">
<h3 class="anchored" data-anchor-id="abstract-3">Abstract:</h3>
<p>Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in financial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identified as negative by the widely used Harvard Dictionary are words typically not considered negative in financial contexts. We develop an alternative negative word list, along with five other word lists, that better reflect tone in financial text. We link the word lists to 10-K filing returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.</p>
</section>
<section id="question" class="level3">
<h3 class="anchored" data-anchor-id="question">Question:</h3>
<ul>
<li>Can dictionaries from social science be used for business text data.</li>
</ul>
</section>
<section id="bumper-sticker-2" class="level3">
<h3 class="anchored" data-anchor-id="bumper-sticker-2">Bumper Sticker:</h3>
<ul>
<li>Words mean different things in context.</li>
</ul>
</section>
</section>
<section id="the-psychological-meaning-of-words-liwc-and-computerized-text-analysis-methods" class="level2">
<h2 class="anchored" data-anchor-id="the-psychological-meaning-of-words-liwc-and-computerized-text-analysis-methods"><a href="https://www.cs.cmu.edu/~ylataus/files/TausczikPennebaker2010.pdf">The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods</a></h2>
<section id="abstract-4" class="level3">
<h3 class="anchored" data-anchor-id="abstract-4">Abstract:</h3>
<p>We are in the midst of a technological revolution whereby, for the first time, researchers can link daily word use to a broad array of real-world behaviors. This article reviews several computerized text analysis methods and describes how Linguistic Inquiry and Word Count (LIWC) was created and validated. LIWC is a transparent text analysis program that counts words in psychologically meaningful categories. Empirical results using LIWC demonstrate its ability to detect meaning in a wide variety of experimental settings, including to show attentional focus, emotionality, social relationships, thinking styles, and individual differences.</p>
</section>
<section id="background-2" class="level3">
<h3 class="anchored" data-anchor-id="background-2">Background:</h3>
<ul>
<li><p>Content Words:</p>
<ul>
<li><p>generally nouns, regular verbs, and many adjectives and adverbs.</p></li>
<li><p>“It was a dark and stormy night”</p>
<ul>
<li>content words: “dark”, “stormy”, “night”</li>
</ul></li>
<li><p>convey what people are saying</p></li>
</ul></li>
<li><p>Style Words:</p>
<ul>
<li><p>aka function words.</p>
<ul>
<li>pronouns, prepositions, articles, conjunctions, auxiliary verbs, and a few other esoteric categories.</li>
</ul></li>
<li><p>style words: how people are communicating.</p></li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="week-6" class="level1">
<h1>Week 6</h1>
<section id="dictionary-methods" class="level2">
<h2 class="anchored" data-anchor-id="dictionary-methods">Dictionary Methods</h2>
<ul>
<li><p>Keyword counting</p></li>
<li><p>(weighted) average frequency</p></li>
<li><p>For dictionaries to work well, words must have a particular meaning in a specific context</p></li>
<li><p>Be very skeptical of out of the box approaches!</p>
<ul>
<li>the financial liabilities reading is a good example of this.</li>
</ul></li>
<li><p>Have to do validation.</p></li>
</ul>
</section>
<section id="validation-1" class="level2">
<h2 class="anchored" data-anchor-id="validation-1">Validation</h2>
<ul>
<li><p>Develop a confusion matrix</p></li>
<li><p>minimize false positives and false negatives.</p></li>
</ul>
</section>
<section id="zipfs-law" class="level2">
<h2 class="anchored" data-anchor-id="zipfs-law">Zipf’s Law</h2>
<ul>
<li>In any corpus, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent will word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.</li>
</ul>
</section>
<section id="wordscores" class="level2">
<h2 class="anchored" data-anchor-id="wordscores">Wordscores</h2>
<ul>
<li><p>Extracting dimensional information from political texts using computerized content analysis</p></li>
<li><p>Training a Wordscores model requires reference scores for texts whose positions on well-defined a prior dimensions are “known”. Afterwards, Wordscores estimates the positions for the remaining “unknown” texts.</p></li>
<li><p>semi-supervised machine learning.</p></li>
<li><p>kinda similar to DW Nominate score.</p></li>
<li><p>But the issue is the dimension to which we observe these as</p>
<ul>
<li><p>DW nominate has always had issues with the Y axis.</p>
<ul>
<li>Wordscore has a similar issue with the labeling.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="week-7---similarity-and-complex-measures" class="level1">
<h1>Week 7 - Similarity and Complex measures</h1>
<section id="lecture-notes-2" class="level2">
<h2 class="anchored" data-anchor-id="lecture-notes-2">Lecture Notes:</h2>
<section id="cosine-similarity" class="level3">
<h3 class="anchored" data-anchor-id="cosine-similarity">Cosine Similarity:</h3>
<ul>
<li><p>What makes text similar?</p></li>
<li><p>cosign similarity is how close are documents are when put into a visual space.</p>
<ul>
<li><p>maximum similarity will occur between a document and itself.</p>
<ul>
<li>0 is min similarity and 1 is max for similarity</li>
</ul></li>
</ul></li>
<li><p>Jaccard Similarity</p>
<ul>
<li>calculate jaccard coefficient</li>
</ul></li>
<li><p>Distance metrics</p>
<ul>
<li><p>euclidean distance</p>
<ul>
<li>hypotenuse of Manhattan triangle.</li>
</ul></li>
<li><p>Manhattan distance</p>
<ul>
<li>up and over</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="weighting-and-goldilocks-words" class="level2">
<h2 class="anchored" data-anchor-id="weighting-and-goldilocks-words">Weighting and “Goldilocks Words”</h2>
<ul>
<li><p>we want words that are neither too rare nor too frequent</p>
<ul>
<li><p>weighting can help us with this.</p>
<ul>
<li><p>when we remove stopwords, we are setting their weight to 0.</p></li>
<li><p>common approach tfdif</p>
<ul>
<li>term frequency document infrequency</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>TFDIF</p>
<ul>
<li><p>up-weights rare words within a document</p></li>
<li><p>down-weights words popular across corpus.</p></li>
</ul></li>
</ul>
</section>
<section id="textual-complexity" class="level2">
<h2 class="anchored" data-anchor-id="textual-complexity">Textual Complexity</h2>
<ul>
<li><p>Linguistic complexity can lead to interesting measures/comparisons of documents</p></li>
<li><p>Domain-specific measures of complexity.</p></li>
</ul>
</section>
</section>
<section id="week-8---supervised-machine-learning" class="level1">
<h1>Week 8 - Supervised Machine Learning</h1>
<section id="supervised-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="supervised-machine-learning">Supervised Machine Learning:</h2>
<ul>
<li><p>Create a mapping between the features within a set of documents and more general categories and concepts that have been defined by the researcher.</p></li>
<li><p>We learn the mapping function from the data by training an algorithm to predict the category of interest from the text using a random sample of coded documents.</p></li>
<li><p>we are <em>predicting</em>.</p></li>
<li><p>This approach is quite general and has the advantage that once the parameters of the model are learned classifying additional documents is essentially costless.</p></li>
</ul>
<section id="classification-steps" class="level3">
<h3 class="anchored" data-anchor-id="classification-steps">Classification Steps</h3>
<ul>
<li><p>Accurately Label Documents (create a training set &amp; validation set)</p></li>
<li><p>feature representation</p></li>
<li><p>choose model &amp; classify documents</p></li>
<li><p>IF YOUR TRAINING DATA SUCKS, YOUR CLASSIFIERS WILL SUCK!</p></li>
</ul>
</section>
<section id="human-annotation" class="level3">
<h3 class="anchored" data-anchor-id="human-annotation">Human Annotation</h3>
<ul>
<li><p><em>Stratified</em> random sample of documents to annotate.</p></li>
<li><p>Create a codebook</p>
<ul>
<li><p>codebook = instructions</p>
<ul>
<li><p>how are we defining stuff</p>
<ul>
<li>iterative process</li>
</ul></li>
<li><p>objectivity/inter-subjectivity</p>
<ul>
<li>you can get intercode reliability.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="types-of-supervised-models" class="level3">
<h3 class="anchored" data-anchor-id="types-of-supervised-models">Types of supervised models:</h3>
</section>
<section id="naive-bayes-classifiers" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes-classifiers">Naive Bayes Classifiers:</h3>
<ul>
<li><p>SPAM v. Ham</p>
<ul>
<li><p>how do we classify data example</p></li>
<li><p>Bayes rule</p>
<ul>
<li><p>probability of new email is spam given its text is equal to….see slide.</p>
<ul>
<li><p>lets look at our priors</p>
<ul>
<li>posterior probability is updated probability after taking new info into account.</li>
</ul></li>
</ul></li>
<li><p>for multiple words we treat each word as independent.</p>
<ul>
<li><p>this is obviously not true.</p>
<ul>
<li>but Naive bayes are useful.</li>
</ul></li>
</ul></li>
<li><p>Main thing: when training data is imbalanced - we run into a lot of issues!</p></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="support-vector-machines" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machines">Support Vector Machines:</h3>
<ul>
<li><p>Building a model from a binary labeled training set (yes/no, Rep/Dem) and returns an optimal hyperplane that puts a new examples into one of the categories non-probabilistically.</p>
<ul>
<li>first finds a support vector</li>
</ul></li>
</ul>
</section>
<section id="model-performance" class="level3">
<h3 class="anchored" data-anchor-id="model-performance">Model Performance</h3>
<ul>
<li><p>Is Bayes or SVM better?</p></li>
<li><p>confusion matrix!</p></li>
<li><p>LOOK AT BALANCED ACCURACY FOR UNBALANCED TRAINING DATA!</p></li>
</ul>
</section>
<section id="validation-2" class="level3">
<h3 class="anchored" data-anchor-id="validation-2">Validation:</h3>
<ul>
<li><p>k-fold Cross-validation.</p></li>
<li><p>Face validity?</p></li>
<li><p>Convergent validity</p></li>
<li><p>hypothesis validity</p></li>
</ul>
</section>
</section>
<section id="week-9---unsupervised-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="week-9---unsupervised-machine-learning">Week 9 - Unsupervised Machine Learning</h2>
<ul>
<li><p>Algorithms that learn patterns from unlabeled data.</p>
<ul>
<li>there is no labels to start.</li>
</ul></li>
<li><p>Helpful for exploratory analysis. What topics are in my data?</p></li>
<li><p>Researchers assigns labels to clusters or topics through post-estimation interpretation of their elements-this requires a lot of substantive knowledge!</p></li>
</ul>
</section>
<section id="clustering" class="level2">
<h2 class="anchored" data-anchor-id="clustering">Clustering</h2>
<ul>
<li><p>creating partitions of the data.</p></li>
<li><p>they input a collection of texts and output a set of K categories and document assignments to each of those K categories.</p></li>
<li><p>Documents are organized is estimated as part of the process, rather than being assumed.</p></li>
<li><p>Clustering groups documents such that inside a cluster they are very similar to each other but different from those outside the cluster.</p></li>
<li><p>Clustering only gets one membership - LDA allows this to vary (multiple memberships).</p></li>
</ul>
<section id="k-means-clustering" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering">K-Means Clustering</h3>
<ul>
<li><p>Randomly select K starting centroids</p></li>
<li><p>each data point is assigned to its nearest centroid</p></li>
<li><p>Recompute the centroids as the mean of the data points assigned to the respective cluster.</p>
<ul>
<li>repeat until we are optimized.</li>
</ul></li>
<li><p>Kinda an old method</p></li>
<li><p>Possibility the algorithm finds latent differences that weren’t theorized.</p></li>
<li><p>If we struggle to label a cluster - its probably not a good cluster.</p></li>
</ul>
</section>
</section>
<section id="topic-models" class="level2">
<h2 class="anchored" data-anchor-id="topic-models">Topic Models:</h2>
<ul>
<li>Rather than assign each document to only one cluster, topic models assign each document with proportional membership to all categories.</li>
</ul>
<section id="latent-dirichlet-allocation-lda" class="level3">
<h3 class="anchored" data-anchor-id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h3>
<ul>
<li><p>Bayesian hierarchical model that assumes a particular data generating process for how an author produces a text.</p></li>
<li><p>Suppose that when writing a text the author draws a mixture of topics: a set of weights that will describe how prevalent the particular topics are.</p></li>
<li><p>Given the set of weights, the author generates the actual text.</p></li>
<li><p>This topic-specific distribution is common across the documents and characterizes the rates words appear when discussing a particular topic.</p></li>
<li><p>It is a LATENT VARIABLE MODEL</p>
<ul>
<li>latent variables models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed.</li>
</ul></li>
<li><p>LDA is basic topic model</p></li>
<li><p>Dirichlet is a type of distribution.</p></li>
<li><p>simplex its a generalization of a triangle.</p>
<ul>
<li>triangle is the simplest shape to make in space.</li>
</ul></li>
</ul>


</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{neilon2024,
  author = {Neilon, Stone},
  title = {Text as {Data}},
  date = {2024-08-26},
  url = {https://stoneneilon.github.io/notes/Text_Data/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-neilon2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Neilon, Stone. 2024. <span>“Text as Data.”</span> August 26, 2024. <a href="https://stoneneilon.github.io/notes/Text_Data/">https://stoneneilon.github.io/notes/Text_Data/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>