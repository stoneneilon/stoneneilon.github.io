{"title":"ICPSR - Math For Social Science (workshop)","markdown":{"yaml":{"title":"ICPSR - Math For Social Science (workshop)","author":"Stone Neilon"},"headingText":"Preface","containsRefs":false,"markdown":"\n\n\nThese are notes from the ICPSR workshop, \"Math for Social Science\" with Prof. Sara Tomek. The purpose of this workshop was to review mathematical concepts already taught. Much of the information in here is taken from the lectures and handouts provided.\n\n# Day 1\n\nDay 1 mostly consisted of a basic review of Matrix algebra.\n\n## Matrix Addition and Subtraction\n\nMatrices must have the same dimensions to be added or subtracted.\n\n-   This means they must have the same rows x columns.\n\n## Matrix Multiplication\n\nTK (Just add a cheat sheet for how to do these)\n\n# Day 2\n\n## Determinant\n\nThe determinant is a scalar number that comprises information on the matrix. The information it comprises tells us whether we can take the inverse of the matrix. If the determinant of a matrix is zero, then we **cannot** take the inverse of that matrix. In a sense, the determinant \"summaries\" the information in the matrix. A specific type of information from a matrix. It is easier to talk about a single number than the whole data.\n\n-   Determinants measure the factor of how much the area of a given region increases or decreases in space.\n\n    -   how much does the transformation stretch or squish \"things\".\n\n        -   the determinant tells us the factor of how much a given area stretches or squishes.\n\n            -   in 3 dimensions this tells us the factor of change in *volume*\n\n-   If the determinant is zero, it is squishes all of space onto a line or single point. Since then, the area of any region would be zero.\n\n-   Determinants are non-zero if the matrix has full rank\n\n-   **only** a square matrix can have a determinant.\n\n![](determinant.png){fig-align=\"center\"}\n\n-   If an inverse is badly scaled or ill-conditioned, then the inverse is *unstable.* This means a tiny change in just one of the elements of the original matrix can result in HUGE changes in many (or all) of the elements of its inverse.\n\n-   Knowing how it is calculated isn't super important. You can just google this if needed. Our computers can do this all as it gets much more tedious when we increase the \\# of dimensions.\n\n-   You can compute a negative determinant (pre absolute value). This would tell you orientation and that the space has been inverted. If you still take the absolute value of the determinant, that still gives you the factor in which the space changed.\n\n## Identity Matrix\n\nA square matrix, **I,** with ones on the main diagonal and zeros everywhere else.\n\n-   If the size of **I** is not specified, then it is assumed to be conformable and as big as necessary.\n\n-   The identity matrix is analogous to the number 1.\n\n-   If you multiple any matrix with a conformable identity matrix, the result will be the same matrix (or vector).\n\n## Inverse Matrices\n\n-   There is no way to divide a matrix.\n\n-   we instead take the inverse of a matrix and multiply it by itself.\n\n-   $\\textbf{A}^{-1}$ raising to the power of negative 1 indicates we are taking the inverse of a matrix.\n\n-   We need to make sure the matrix can be inverted.\n\n    -   it is invertible if it is non-singluar.\n\n    -   The matrix times its inverse will return an identity matrix.\n\n-   Determinate has to be non-zero.\n\n    -   Need to have full rank for this.\n\n## Vectors\n\n-   Vectors are matrices with only one row or columns.\n\n-   $x'x = \\sum x^2_i$\n\n    -   This is the sum of squares.\n\n-   Vectors are one variable\n\n-   matrices are multiple variables.\n\n### Dot Product\n\n-   For example, if A = \\[5, -2, 1\\] and B = \\[3, 0, 4\\] then A • B = (5)(3) + (-2)(0) + (1)(4) = 15 + 0 + 4 = 19 .\\\n    So the Dot Product of A and B is 19.\n\n-   If the dot product of two vectors is equal to zero then those two vectors are **Orthogonal**, which implies that the\\\n    angle between them is 90° (i.e., they are perpendicular) and they are independent of each other.\n\n    -   orthogonal means independent.\n\n        -   The dot product tells us how closely two vectors align.\n\n## Idempotent\n\n-   A square matrix, **P** is idempotent if when multiplied by itself, yields itself. PP=P\n\n-   The trace of an idempotent matrix is equal to the rank.\n\n-   1 X 1 = 1 - this is idempotent\n\n## Order of Operations\n\n-   Matrix multiplication is **non-communicative**. $AB \\neq BA$\n\n-   Matrix multiplication is associative. As long as the order stays the same. $(AB)C = A(BC)$\n\n## Rank of Matrix\n\n-   Thank rank of a matrix is the maximal number of linearly independent rows or columns.\n\n-   rank = number dimensions in the output.\n\n-   The columns in a matrix should be independent of each other.\n\n-   How much information can we actually get out of a matrix that is independent of each other.\n\n-   Max rank will be equal to the number of columns in the matrix.\n\n-   Not full rank means some variables are linearly dependent on each other.\n\n    -   classic example is male and female. Perhaps your matrix includes a dummy variable for male (0 = female, 1 = male). If you include another variable for female where (0 = male, 1 = female) - this is a dummy variable trap. The female variable is perfectly dependent on the male variable. They give the exact same information. The female variable is **determined** by the male variable.\n\n-   Kind of similar to degrees of freedom.\n\n-   If a square matrix is of full rank then it is nonsingular (i.e., it does have an inverse).\n\n-   If a square matrix is not of full rank then it is singular (i.e., it does not have an inverse).\n\n## Trace\n\n-   sum of the diagonal elements.\n\n-   Let's say we have a VCV matrix.\n\n    -   the variance of each element along the main diagonal represents variance for that variable.\n\n        -   If you were to take the trace of the main diagonal in a VCV, you would get the variance across all variables together (non-weighted).\n\n            -   VCV's are always square matrices.\n\n## Eigenvalues and Eigenvectors\n\n-   Let's say we have a matrix, **A,** that is square and nxn.\n\n    -   Eigenvectors have a special relationship with this matrix\n\n        -   such that when you multiply $\\textbf{A}\\overrightarrow{x}$ you get $\\lambda \\overrightarrow{x}$\n\n            -   the $\\overrightarrow{x}$ is the eigenvector.\n\n            -   **A** is a matrix. The $\\lambda$ is an EIGENVALUE and is a SCALAR.\n\n        -   When you multiply **A** times the eigenvector x, you get back that same vector, multiplied by a scalar, lambda. These scalars are called eigenvalues.\n\n-   From reddit:\n\n    -   *\"I'm not aware of how it's used, if at all, in statistics. It comes from linear algebra.*\n\n        *Say you have a shape. You can apply a transformation to it - rotate it, stretch bits, squash bits, etc. If you paint an arrow on the shape, after the transformation the arrow will most likely be pointing a different direction.*\n\n        *But sometimes the transformation you apply conspires to make so that the arrow doesn't change direction. Maybe it gets stretched or squished, but it still points in the same direction. From the arrow's perspective, all you did was scale it up or down in size by some amount.*\n\n        *If that happens, the arrow is called an eigenvector of the transformation, and the amount it gets scaled by is its associated eigenvalue. They are properties of the transformation itself, not the shape you apply it to.\"*\n\n-   A matrix can have multiple eigenvalues BUT no more than its **number or rows/columns.**\n\n-   Each eigenvalue is associate with a specific eigenvector.\n\n-   You can get negative eigenvalues but they are not good for us in statistics.\n\n### Definite\n\n-   **eigenvalues are closely related to definiteness.**\n\n-   Why do we care about definiteness?\n\n    -   It is useful for establishing if a (multivariate) function has a maximum, minimum or neither at a critical point.\n\n        -   this is important for regression (OLS). We are trying to find the line that minimizes the squared difference.\n\n-   We want positive definiteness.\n\n    -   To have positive definiteness we need our matrix to satisfy the following:\n\n        -   symmetric\n\n        -   all eigenvalues are positive\n\n        -   all the subdeterminants are positive\n\n        -   you could also just calculate the quadratic form and check its positiveness\n\n    -   if the quadratic form is \\> 0 then it is positive definiteness.\n\n    -   we want a positive definite matrix because it is good for interpretation.\n\n## Variance Inflation Factor (VIF)\n\n-   metric that measures how much overlap we have between our independent variables.\n\n-   If we created a correlation matrix we can find the VIF easily.\n\nEND DAY 2!\n\n# Day 3\n\n## Differentiation\n\n### Derivative\n\nThe instantaneous rate of change. Finding the slope of a single point. The tangent line of the curve.\n\n### How do we write a derivative?\n\nDerivatives are represented by either:\n\n-   f'(x)\n\n-   $\\frac{\\textit{d}}{\\textit{dx}}f(x)$\n\nWe are saying to take the derivative of x\n\n### Differentiation formulas\n\n-   the derivative of a constant is 0\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}c=0$\n\n        -   example: $\\frac{\\textit{d}}{\\textit{du}}7$\n\n            -   if we take the derivative of just 7, we get zero.\n\n-   The derivative of a sum is the sum of the derivatives.\n\n    -   $\\frac{\\textit{d}}{\\textit{dt}}(t+4) = \\frac{\\textit{d}}{\\textit{dt}}(t)+\\frac{\\textit{d}}{\\textit{dt}}(4)=1+0=1$\n\n-   The derivative of u to a constant power (use this one a lot)\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}u^n=n*u^{n-1}du$\n    -   $\\frac{\\textit{d}}{\\textit{dx}}3x^3=3*3x^{2}=9x^2$\n\n-   The derivative of log:\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}log(u)=\\frac{1}{u}du$\n\n        -   $\\frac{\\textit{d}}{\\textit{dy}}3log(x)=3*\\frac{1}{x}*\\frac{\\textit{d}}{\\textit{dx}}x=\\frac{3}{x}$\n\n-   The derivative of e:\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}e^u=e^udu$\n\n        -   $\\frac{\\textit{d}}{\\textit{dy}}e^{4y}=e^{4y}*\\frac{\\textit{d}}{\\textit{dy}}4y=e^{4y}*4=4^{4y}$\n\n-   There is also the Product and Quotient rules\n\n    -   put those here TK.\n\n#### The Chain Rule\n\n-   The chain rule allows you to combine any of the differentiation rules we have already covered\n\n-   first do the derivative of the outside and then do the derivative of the inside.\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}f(g(u))=f'(g(u))*g'(u)*du$\n\n#### Partial derivatives\n\nPartial derivatives are a way to derive functions that have more than one independent variable.\n\nPartial derivatives use a notation that is intentionally similar to that of regular derivatives. Their overall format is the same but the shorthand symbols, such as \"dx,\" are replaced by their stylized version of the letter d: $\\partial$. Partial derivatives should be labeled as $\\frac{\\partial}{\\partial x}$ or $\\frac{\\partial}{\\partial y}$, depending on the variable being derived.\n\nThe two derivatives should be read as \"The partial derivative, with respect to x,\" and \"The partial derivative, with respect to y.\"\n\n-   When taking a partial derivative with respect to a particular variable, treat all other variables as though they are constants.\n\n    -   When deriving with respect to x, treat y as a constant, and do not derive the y.\n\n##### Example:\n\n$f(x,y)=x^2y+2x^3$\n\n$f(x,y)=xy^2+x^3$\n\n-   $\\frac{\\partial}{\\partial x}=\\frac{\\partial}{\\partial x}(x)y^2+\\frac{\\partial}{\\partial x}(x^3)=y^2 +3x^2$\n\n    -   take the partial derivative, with respect to x.\n\n        -   hold y constant.\n\n        -   the $y^2+3x^2$ is the **rate of change** for the function.\n\n-   $\\frac{\\partial}{\\partial y}=\\frac{\\partial}{\\partial y}x(y^2)+\\frac{\\partial}{\\partial y}x^3=2xy+0=2xy$\n\n    -   take the partial derivative, with respect to y.\n\n        -   hold x constant. \\`\n\n        -   the $2xy$ is the **rate of change** for the function.\n\n## Integration\n\n-   The integral provides us with information about the area under the function.\n\n-   The indefinite integral of a function is a function, but the definite integral may be a number.\n\n    -   Example:\n\n        -   the integral of the rate of function is the distance function\n\n        -   integrating over a specified time tells us how far we've gone.\n\n-   Two forms of integration:\n\n    -   indefinite (anti-derivative)\n\n        -   $\\int f(x)dx$\n\n            -   integrate the whole thing.\n\n        -   When doing an indefinite integral, we must add a constant to our function\n\n    -   definite\n\n        -   $\\int_{a}^{b}f(x)dx$\n\n            -   notice the integral sign (the a and b). These serve as limits and tell us what values to integrate over.\n\n            -   the integration is going over a lower end to a positive end.\n\nTK TK TK TK MORE ON INTEGRALS HERE + FORMULAS\n\n## Calculus in Statistics\n\nWe use calculus in many ways, some include:\n\n-   continuous density functions\n\n-   finding the expected value (mean) of a distribution\n\n-   finding the variance (standard deviation) of a distribution\n\n-   finding the median of a distribution\n\n### Probability Density Function (pdf)\n\n-   The probability density function of a variable tells us the probability of a certain event when a continuum of events is possible.\n\n-   The pdf of x is usually noted by the lowercase, f, i.e. f(x).\n\n-   The probability of events at a particular point.\n\n-   The area under a probability density function is 1.\n\n    -   the integral of the pdf over **all** events is 1.\n\n        -   think about what this looks like.\n\n            -   if you were to shade in the pdf, you would color the whole thing!\n\n### Cumulative Density Function (cdf)\n\n-   The cdf measures the area under the pdf.\n\n    -   cdf is the integral of the pdf.\n\n-   denoted with a capital F, i.e. F(x)\n\nEND DAY 3!\n\n# Day 4\n\n## Continuous Probability Distributions\n\n-   Continuous random variables can take on an *infinite* number of possible values, corresponding to every value in an interval.\n\n-   values where the curve is high = higher probability of occurring.\n\n-   we model a continuous random variable with a curve f(x), called a probability density function (pdf).\n\n    -   f(x) represents the height of the curve at point x\n\n    -   for continuous random variables probabilities are *areas under the curve.*\n\n-   The probability that a random value of x is exactly equal to one specific value is ZERO\n\n    -   this is because the variable is continuous.\n\n    -   the value can be an infinitely small value.\n\n        -   so we use ranges. (Pa\\<X\\<b)\n\n            -   need to find area between an interval.\n\n-   Total area under the curve is always 1.\n\n-   What are continuous probability distributions?\n\n    -   they describe the probabilities of a continuous random variable's possible values.\n\n    -   There a couple distributions we should know:\n\n        -   Uniform, Normal (Gaussian), Standardized normal, Gamma,\n\n-   Probabilities and percentiles are found by integrating the probability density function.\n\n    -   Deriving the mean and variance also requires integration.\n\n## Uniform Distribution\n\n-   often called the rectangular distribution.\n\n-   Function of x; A, B\n\n    -   where X is a random variable\n\n    -   where A and B are the known values of the lower bound and upper bound\n\n-   Represented mathematically as:\n\n    -   $\\frac{1}{B-A}$\n\n        -   the pdf will be the function above. Elsewhere it will be 0.\n\n-   How the Uniform Distribution looks:\n\n    -   \n\n        ![Uniform Distribution (visual)](uniform_distribution.png)\n\n## Normal Distribution (Gaussian)\n\n-   Most important continuous probability distribution in the entire field of statistics\n\n-   Approximately describes many phenomena that occur in nature, industry, and research. For example, physical measurements in areas such as meteorological experiments, rainfall studies, and measurements of manufactured parts.\n\n-   This distribution is our assumption in regression\n\n    -   we assume normal distribution.\n\n-   Bell-shaped.\n\n-   The highest probability of events to occur will happen around the mean.\n\n    -   farther from the mean = less probability of event occurring.\n\n-   Mean = median = mode\n\n-   **The beauty of the normal curve:**\n\n    -   **No matter what** $\\mu$ and $\\sigma$ are, the area between $\\mu-\\sigma$ and $\\mu+\\sigma$ is about 68%; the area between $\\mu-2\\sigma$ and $u+2\\sigma$ is about 95%; and the area between $\\mu-3\\sigma$ and $\\mu+3\\sigma$ is about 99.7%. Almost all values fall within 3 standard deviations.\n\n        -   changing the variance simply moves away how far away that 99.7% is. Think: how spread affects the shape of the distribution and subsequently the area under the curve.\n\n### Standard Normal (Z):\n\n-   similar to the normal distribution\n\n    -   we are just standardizing.\n\n        -   normal distribution can have any mean and standard deviation\n\n        -   the standard normal distribution has a mean of 0 and a standard deviation to 1.\n\n            -   makes it easier to interpret and solve\n\n            -   Standard normal distribution simply translates the normal distribution using z-scores.\n\n                -   will give us the same area under the curve.\n\n-   z-scores represent the area under the curve from a value of negative infinity to the z-score.\n\n    -   if z-score is 0, then we get .5 = which means = .5 area under the curve.\n\n-   Below is an image that shows the difference between the normal distribution and the standard normal.\n\n    -   notice the mean and standard deviation values.\n\n![](normal_gaussian_comparison.png){fig-align=\"center\" width=\"401\"}\n\n## Gamma Distribution\n\n-   The gamma function is only for positive values.\n\n    -   we are using it for only values that can be positive.\n\n        -   use for something that cannot be negative (time).\n\n-   Basically can get us a bunch of other distributions.\n\n### Exponential Distribution\n\n-   Poisson distribution is used to compute the probability of specific numbers of events during a particular period of time or span of space.\n\n    -   looking across a time period or space.\n\n    -   count data.\n\n-   just need to know lambda to define shape of distribution.\n\n## Chi-squared Distribution\n\n-   A special case of the gamma distribution.\n\n-   all we need to know is the degrees of freedom.\n\nEND DAY 4.\n\n## Day 5\n\n### Discrete Probability Distributions\n\n-   Not continuous (duh).\n\n-   have to be a whole number!\n\n    -   we treat these different.\n\n    -   count data!\n\n-   We don't have to integrate\n\n-   instead of looking at a PDF, we look at a probability mass function. (PMF)\n\n-   We can get a probability for an exact value!\n\n-   summation instead of integration\n\n-   REMEMBER: X IS NOW COUNTABLE - MUST BE A WHOLE NUMBER TO USE THESE DISTRIBUTIONS!\n\n-   Discrete random variable:\n\n    -   values consitute a finite or countably infinite set\n\n### Bernoulli Random Variable\n\n-   any random variable whose only possible values are 0 and 1 is called a Bernoulli random variable.\n\n-   binary, yes or no, right or wrong, True or False.\n\n-   Each trial is independent.\n\n-   What we count is called a success\n\n    -   everything else is called a failure\n\n-   P(success) = p\n\n    -   P(failure) = 1 - p\n\n-   Let X = 1 if a success occurs, and X = 0 if a failure occurs.\n\n    -   Then X has a Bernoulli distribution\n\n        -   $P(X=x)=p^x(1-p)^{1-x}$\n\n            -   this is referred to the mass function of the Bernoulli distribution.\n\n-   Why is the Bernoulli important?\n\n    -   some other common discrete probability distributions are built on the assumption of independent Bernoulli trials.\n\n        -   Binomial, geometric, negative binomial\n\n### Geometric Random Variable\n\n-   **Distribution of the number of trials to get the first success in n Bernoulli trials.**\n\n-   Similar in the sense that it is also a specific value BUT we are interested in the count of how long it takes us to achieve a success.\n\n### Binomial Distribution\n\n-   **Distribution of the number of success in n independent Bernoulli Trials.**\n\n-   \n\n### Negative Binomial\n\n-   **The distribution of the number of trials to get the r*th* success in independent Bernoulli Trials.**\n\n### Poisson Distribution\n\n-   Poisson can either be continuous or discrete.\n\n-   this section focuses on discrete version of poisson distribution.\n","srcMarkdownNoYaml":"\n\n# Preface\n\nThese are notes from the ICPSR workshop, \"Math for Social Science\" with Prof. Sara Tomek. The purpose of this workshop was to review mathematical concepts already taught. Much of the information in here is taken from the lectures and handouts provided.\n\n# Day 1\n\nDay 1 mostly consisted of a basic review of Matrix algebra.\n\n## Matrix Addition and Subtraction\n\nMatrices must have the same dimensions to be added or subtracted.\n\n-   This means they must have the same rows x columns.\n\n## Matrix Multiplication\n\nTK (Just add a cheat sheet for how to do these)\n\n# Day 2\n\n## Determinant\n\nThe determinant is a scalar number that comprises information on the matrix. The information it comprises tells us whether we can take the inverse of the matrix. If the determinant of a matrix is zero, then we **cannot** take the inverse of that matrix. In a sense, the determinant \"summaries\" the information in the matrix. A specific type of information from a matrix. It is easier to talk about a single number than the whole data.\n\n-   Determinants measure the factor of how much the area of a given region increases or decreases in space.\n\n    -   how much does the transformation stretch or squish \"things\".\n\n        -   the determinant tells us the factor of how much a given area stretches or squishes.\n\n            -   in 3 dimensions this tells us the factor of change in *volume*\n\n-   If the determinant is zero, it is squishes all of space onto a line or single point. Since then, the area of any region would be zero.\n\n-   Determinants are non-zero if the matrix has full rank\n\n-   **only** a square matrix can have a determinant.\n\n![](determinant.png){fig-align=\"center\"}\n\n-   If an inverse is badly scaled or ill-conditioned, then the inverse is *unstable.* This means a tiny change in just one of the elements of the original matrix can result in HUGE changes in many (or all) of the elements of its inverse.\n\n-   Knowing how it is calculated isn't super important. You can just google this if needed. Our computers can do this all as it gets much more tedious when we increase the \\# of dimensions.\n\n-   You can compute a negative determinant (pre absolute value). This would tell you orientation and that the space has been inverted. If you still take the absolute value of the determinant, that still gives you the factor in which the space changed.\n\n## Identity Matrix\n\nA square matrix, **I,** with ones on the main diagonal and zeros everywhere else.\n\n-   If the size of **I** is not specified, then it is assumed to be conformable and as big as necessary.\n\n-   The identity matrix is analogous to the number 1.\n\n-   If you multiple any matrix with a conformable identity matrix, the result will be the same matrix (or vector).\n\n## Inverse Matrices\n\n-   There is no way to divide a matrix.\n\n-   we instead take the inverse of a matrix and multiply it by itself.\n\n-   $\\textbf{A}^{-1}$ raising to the power of negative 1 indicates we are taking the inverse of a matrix.\n\n-   We need to make sure the matrix can be inverted.\n\n    -   it is invertible if it is non-singluar.\n\n    -   The matrix times its inverse will return an identity matrix.\n\n-   Determinate has to be non-zero.\n\n    -   Need to have full rank for this.\n\n## Vectors\n\n-   Vectors are matrices with only one row or columns.\n\n-   $x'x = \\sum x^2_i$\n\n    -   This is the sum of squares.\n\n-   Vectors are one variable\n\n-   matrices are multiple variables.\n\n### Dot Product\n\n-   For example, if A = \\[5, -2, 1\\] and B = \\[3, 0, 4\\] then A • B = (5)(3) + (-2)(0) + (1)(4) = 15 + 0 + 4 = 19 .\\\n    So the Dot Product of A and B is 19.\n\n-   If the dot product of two vectors is equal to zero then those two vectors are **Orthogonal**, which implies that the\\\n    angle between them is 90° (i.e., they are perpendicular) and they are independent of each other.\n\n    -   orthogonal means independent.\n\n        -   The dot product tells us how closely two vectors align.\n\n## Idempotent\n\n-   A square matrix, **P** is idempotent if when multiplied by itself, yields itself. PP=P\n\n-   The trace of an idempotent matrix is equal to the rank.\n\n-   1 X 1 = 1 - this is idempotent\n\n## Order of Operations\n\n-   Matrix multiplication is **non-communicative**. $AB \\neq BA$\n\n-   Matrix multiplication is associative. As long as the order stays the same. $(AB)C = A(BC)$\n\n## Rank of Matrix\n\n-   Thank rank of a matrix is the maximal number of linearly independent rows or columns.\n\n-   rank = number dimensions in the output.\n\n-   The columns in a matrix should be independent of each other.\n\n-   How much information can we actually get out of a matrix that is independent of each other.\n\n-   Max rank will be equal to the number of columns in the matrix.\n\n-   Not full rank means some variables are linearly dependent on each other.\n\n    -   classic example is male and female. Perhaps your matrix includes a dummy variable for male (0 = female, 1 = male). If you include another variable for female where (0 = male, 1 = female) - this is a dummy variable trap. The female variable is perfectly dependent on the male variable. They give the exact same information. The female variable is **determined** by the male variable.\n\n-   Kind of similar to degrees of freedom.\n\n-   If a square matrix is of full rank then it is nonsingular (i.e., it does have an inverse).\n\n-   If a square matrix is not of full rank then it is singular (i.e., it does not have an inverse).\n\n## Trace\n\n-   sum of the diagonal elements.\n\n-   Let's say we have a VCV matrix.\n\n    -   the variance of each element along the main diagonal represents variance for that variable.\n\n        -   If you were to take the trace of the main diagonal in a VCV, you would get the variance across all variables together (non-weighted).\n\n            -   VCV's are always square matrices.\n\n## Eigenvalues and Eigenvectors\n\n-   Let's say we have a matrix, **A,** that is square and nxn.\n\n    -   Eigenvectors have a special relationship with this matrix\n\n        -   such that when you multiply $\\textbf{A}\\overrightarrow{x}$ you get $\\lambda \\overrightarrow{x}$\n\n            -   the $\\overrightarrow{x}$ is the eigenvector.\n\n            -   **A** is a matrix. The $\\lambda$ is an EIGENVALUE and is a SCALAR.\n\n        -   When you multiply **A** times the eigenvector x, you get back that same vector, multiplied by a scalar, lambda. These scalars are called eigenvalues.\n\n-   From reddit:\n\n    -   *\"I'm not aware of how it's used, if at all, in statistics. It comes from linear algebra.*\n\n        *Say you have a shape. You can apply a transformation to it - rotate it, stretch bits, squash bits, etc. If you paint an arrow on the shape, after the transformation the arrow will most likely be pointing a different direction.*\n\n        *But sometimes the transformation you apply conspires to make so that the arrow doesn't change direction. Maybe it gets stretched or squished, but it still points in the same direction. From the arrow's perspective, all you did was scale it up or down in size by some amount.*\n\n        *If that happens, the arrow is called an eigenvector of the transformation, and the amount it gets scaled by is its associated eigenvalue. They are properties of the transformation itself, not the shape you apply it to.\"*\n\n-   A matrix can have multiple eigenvalues BUT no more than its **number or rows/columns.**\n\n-   Each eigenvalue is associate with a specific eigenvector.\n\n-   You can get negative eigenvalues but they are not good for us in statistics.\n\n### Definite\n\n-   **eigenvalues are closely related to definiteness.**\n\n-   Why do we care about definiteness?\n\n    -   It is useful for establishing if a (multivariate) function has a maximum, minimum or neither at a critical point.\n\n        -   this is important for regression (OLS). We are trying to find the line that minimizes the squared difference.\n\n-   We want positive definiteness.\n\n    -   To have positive definiteness we need our matrix to satisfy the following:\n\n        -   symmetric\n\n        -   all eigenvalues are positive\n\n        -   all the subdeterminants are positive\n\n        -   you could also just calculate the quadratic form and check its positiveness\n\n    -   if the quadratic form is \\> 0 then it is positive definiteness.\n\n    -   we want a positive definite matrix because it is good for interpretation.\n\n## Variance Inflation Factor (VIF)\n\n-   metric that measures how much overlap we have between our independent variables.\n\n-   If we created a correlation matrix we can find the VIF easily.\n\nEND DAY 2!\n\n# Day 3\n\n## Differentiation\n\n### Derivative\n\nThe instantaneous rate of change. Finding the slope of a single point. The tangent line of the curve.\n\n### How do we write a derivative?\n\nDerivatives are represented by either:\n\n-   f'(x)\n\n-   $\\frac{\\textit{d}}{\\textit{dx}}f(x)$\n\nWe are saying to take the derivative of x\n\n### Differentiation formulas\n\n-   the derivative of a constant is 0\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}c=0$\n\n        -   example: $\\frac{\\textit{d}}{\\textit{du}}7$\n\n            -   if we take the derivative of just 7, we get zero.\n\n-   The derivative of a sum is the sum of the derivatives.\n\n    -   $\\frac{\\textit{d}}{\\textit{dt}}(t+4) = \\frac{\\textit{d}}{\\textit{dt}}(t)+\\frac{\\textit{d}}{\\textit{dt}}(4)=1+0=1$\n\n-   The derivative of u to a constant power (use this one a lot)\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}u^n=n*u^{n-1}du$\n    -   $\\frac{\\textit{d}}{\\textit{dx}}3x^3=3*3x^{2}=9x^2$\n\n-   The derivative of log:\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}log(u)=\\frac{1}{u}du$\n\n        -   $\\frac{\\textit{d}}{\\textit{dy}}3log(x)=3*\\frac{1}{x}*\\frac{\\textit{d}}{\\textit{dx}}x=\\frac{3}{x}$\n\n-   The derivative of e:\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}e^u=e^udu$\n\n        -   $\\frac{\\textit{d}}{\\textit{dy}}e^{4y}=e^{4y}*\\frac{\\textit{d}}{\\textit{dy}}4y=e^{4y}*4=4^{4y}$\n\n-   There is also the Product and Quotient rules\n\n    -   put those here TK.\n\n#### The Chain Rule\n\n-   The chain rule allows you to combine any of the differentiation rules we have already covered\n\n-   first do the derivative of the outside and then do the derivative of the inside.\n\n    -   $\\frac{\\textit{d}}{\\textit{du}}f(g(u))=f'(g(u))*g'(u)*du$\n\n#### Partial derivatives\n\nPartial derivatives are a way to derive functions that have more than one independent variable.\n\nPartial derivatives use a notation that is intentionally similar to that of regular derivatives. Their overall format is the same but the shorthand symbols, such as \"dx,\" are replaced by their stylized version of the letter d: $\\partial$. Partial derivatives should be labeled as $\\frac{\\partial}{\\partial x}$ or $\\frac{\\partial}{\\partial y}$, depending on the variable being derived.\n\nThe two derivatives should be read as \"The partial derivative, with respect to x,\" and \"The partial derivative, with respect to y.\"\n\n-   When taking a partial derivative with respect to a particular variable, treat all other variables as though they are constants.\n\n    -   When deriving with respect to x, treat y as a constant, and do not derive the y.\n\n##### Example:\n\n$f(x,y)=x^2y+2x^3$\n\n$f(x,y)=xy^2+x^3$\n\n-   $\\frac{\\partial}{\\partial x}=\\frac{\\partial}{\\partial x}(x)y^2+\\frac{\\partial}{\\partial x}(x^3)=y^2 +3x^2$\n\n    -   take the partial derivative, with respect to x.\n\n        -   hold y constant.\n\n        -   the $y^2+3x^2$ is the **rate of change** for the function.\n\n-   $\\frac{\\partial}{\\partial y}=\\frac{\\partial}{\\partial y}x(y^2)+\\frac{\\partial}{\\partial y}x^3=2xy+0=2xy$\n\n    -   take the partial derivative, with respect to y.\n\n        -   hold x constant. \\`\n\n        -   the $2xy$ is the **rate of change** for the function.\n\n## Integration\n\n-   The integral provides us with information about the area under the function.\n\n-   The indefinite integral of a function is a function, but the definite integral may be a number.\n\n    -   Example:\n\n        -   the integral of the rate of function is the distance function\n\n        -   integrating over a specified time tells us how far we've gone.\n\n-   Two forms of integration:\n\n    -   indefinite (anti-derivative)\n\n        -   $\\int f(x)dx$\n\n            -   integrate the whole thing.\n\n        -   When doing an indefinite integral, we must add a constant to our function\n\n    -   definite\n\n        -   $\\int_{a}^{b}f(x)dx$\n\n            -   notice the integral sign (the a and b). These serve as limits and tell us what values to integrate over.\n\n            -   the integration is going over a lower end to a positive end.\n\nTK TK TK TK MORE ON INTEGRALS HERE + FORMULAS\n\n## Calculus in Statistics\n\nWe use calculus in many ways, some include:\n\n-   continuous density functions\n\n-   finding the expected value (mean) of a distribution\n\n-   finding the variance (standard deviation) of a distribution\n\n-   finding the median of a distribution\n\n### Probability Density Function (pdf)\n\n-   The probability density function of a variable tells us the probability of a certain event when a continuum of events is possible.\n\n-   The pdf of x is usually noted by the lowercase, f, i.e. f(x).\n\n-   The probability of events at a particular point.\n\n-   The area under a probability density function is 1.\n\n    -   the integral of the pdf over **all** events is 1.\n\n        -   think about what this looks like.\n\n            -   if you were to shade in the pdf, you would color the whole thing!\n\n### Cumulative Density Function (cdf)\n\n-   The cdf measures the area under the pdf.\n\n    -   cdf is the integral of the pdf.\n\n-   denoted with a capital F, i.e. F(x)\n\nEND DAY 3!\n\n# Day 4\n\n## Continuous Probability Distributions\n\n-   Continuous random variables can take on an *infinite* number of possible values, corresponding to every value in an interval.\n\n-   values where the curve is high = higher probability of occurring.\n\n-   we model a continuous random variable with a curve f(x), called a probability density function (pdf).\n\n    -   f(x) represents the height of the curve at point x\n\n    -   for continuous random variables probabilities are *areas under the curve.*\n\n-   The probability that a random value of x is exactly equal to one specific value is ZERO\n\n    -   this is because the variable is continuous.\n\n    -   the value can be an infinitely small value.\n\n        -   so we use ranges. (Pa\\<X\\<b)\n\n            -   need to find area between an interval.\n\n-   Total area under the curve is always 1.\n\n-   What are continuous probability distributions?\n\n    -   they describe the probabilities of a continuous random variable's possible values.\n\n    -   There a couple distributions we should know:\n\n        -   Uniform, Normal (Gaussian), Standardized normal, Gamma,\n\n-   Probabilities and percentiles are found by integrating the probability density function.\n\n    -   Deriving the mean and variance also requires integration.\n\n## Uniform Distribution\n\n-   often called the rectangular distribution.\n\n-   Function of x; A, B\n\n    -   where X is a random variable\n\n    -   where A and B are the known values of the lower bound and upper bound\n\n-   Represented mathematically as:\n\n    -   $\\frac{1}{B-A}$\n\n        -   the pdf will be the function above. Elsewhere it will be 0.\n\n-   How the Uniform Distribution looks:\n\n    -   \n\n        ![Uniform Distribution (visual)](uniform_distribution.png)\n\n## Normal Distribution (Gaussian)\n\n-   Most important continuous probability distribution in the entire field of statistics\n\n-   Approximately describes many phenomena that occur in nature, industry, and research. For example, physical measurements in areas such as meteorological experiments, rainfall studies, and measurements of manufactured parts.\n\n-   This distribution is our assumption in regression\n\n    -   we assume normal distribution.\n\n-   Bell-shaped.\n\n-   The highest probability of events to occur will happen around the mean.\n\n    -   farther from the mean = less probability of event occurring.\n\n-   Mean = median = mode\n\n-   **The beauty of the normal curve:**\n\n    -   **No matter what** $\\mu$ and $\\sigma$ are, the area between $\\mu-\\sigma$ and $\\mu+\\sigma$ is about 68%; the area between $\\mu-2\\sigma$ and $u+2\\sigma$ is about 95%; and the area between $\\mu-3\\sigma$ and $\\mu+3\\sigma$ is about 99.7%. Almost all values fall within 3 standard deviations.\n\n        -   changing the variance simply moves away how far away that 99.7% is. Think: how spread affects the shape of the distribution and subsequently the area under the curve.\n\n### Standard Normal (Z):\n\n-   similar to the normal distribution\n\n    -   we are just standardizing.\n\n        -   normal distribution can have any mean and standard deviation\n\n        -   the standard normal distribution has a mean of 0 and a standard deviation to 1.\n\n            -   makes it easier to interpret and solve\n\n            -   Standard normal distribution simply translates the normal distribution using z-scores.\n\n                -   will give us the same area under the curve.\n\n-   z-scores represent the area under the curve from a value of negative infinity to the z-score.\n\n    -   if z-score is 0, then we get .5 = which means = .5 area under the curve.\n\n-   Below is an image that shows the difference between the normal distribution and the standard normal.\n\n    -   notice the mean and standard deviation values.\n\n![](normal_gaussian_comparison.png){fig-align=\"center\" width=\"401\"}\n\n## Gamma Distribution\n\n-   The gamma function is only for positive values.\n\n    -   we are using it for only values that can be positive.\n\n        -   use for something that cannot be negative (time).\n\n-   Basically can get us a bunch of other distributions.\n\n### Exponential Distribution\n\n-   Poisson distribution is used to compute the probability of specific numbers of events during a particular period of time or span of space.\n\n    -   looking across a time period or space.\n\n    -   count data.\n\n-   just need to know lambda to define shape of distribution.\n\n## Chi-squared Distribution\n\n-   A special case of the gamma distribution.\n\n-   all we need to know is the degrees of freedom.\n\nEND DAY 4.\n\n## Day 5\n\n### Discrete Probability Distributions\n\n-   Not continuous (duh).\n\n-   have to be a whole number!\n\n    -   we treat these different.\n\n    -   count data!\n\n-   We don't have to integrate\n\n-   instead of looking at a PDF, we look at a probability mass function. (PMF)\n\n-   We can get a probability for an exact value!\n\n-   summation instead of integration\n\n-   REMEMBER: X IS NOW COUNTABLE - MUST BE A WHOLE NUMBER TO USE THESE DISTRIBUTIONS!\n\n-   Discrete random variable:\n\n    -   values consitute a finite or countably infinite set\n\n### Bernoulli Random Variable\n\n-   any random variable whose only possible values are 0 and 1 is called a Bernoulli random variable.\n\n-   binary, yes or no, right or wrong, True or False.\n\n-   Each trial is independent.\n\n-   What we count is called a success\n\n    -   everything else is called a failure\n\n-   P(success) = p\n\n    -   P(failure) = 1 - p\n\n-   Let X = 1 if a success occurs, and X = 0 if a failure occurs.\n\n    -   Then X has a Bernoulli distribution\n\n        -   $P(X=x)=p^x(1-p)^{1-x}$\n\n            -   this is referred to the mass function of the Bernoulli distribution.\n\n-   Why is the Bernoulli important?\n\n    -   some other common discrete probability distributions are built on the assumption of independent Bernoulli trials.\n\n        -   Binomial, geometric, negative binomial\n\n### Geometric Random Variable\n\n-   **Distribution of the number of trials to get the first success in n Bernoulli trials.**\n\n-   Similar in the sense that it is also a specific value BUT we are interested in the count of how long it takes us to achieve a success.\n\n### Binomial Distribution\n\n-   **Distribution of the number of success in n independent Bernoulli Trials.**\n\n-   \n\n### Negative Binomial\n\n-   **The distribution of the number of trials to get the r*th* success in independent Bernoulli Trials.**\n\n### Poisson Distribution\n\n-   Poisson can either be continuous or discrete.\n\n-   this section focuses on discrete version of poisson distribution.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.554","editor":"visual","theme":"cosmo","title":"ICPSR - Math For Social Science (workshop)","author":"Stone Neilon"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}